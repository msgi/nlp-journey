{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 10\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = [128]  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    \n",
    "    outputSize = 128  # 从高维映射到低维的神经元个数\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "构建模型，模型的架构如下：\n",
    "1，利用Bi-LSTM获得上下文的信息\n",
    "2，将Bi-LSTM获得的隐层输出和词向量拼接[fwOutput;wordEmbedding;bwOutput]\n",
    "3，将2所得的词表示映射到低维\n",
    "4，hidden_size上每个位置的值都取时间步上最大的值，类似于max-pool\n",
    "5，softmax分类\n",
    "\"\"\"\n",
    "\n",
    "class RCNN(object):\n",
    "    \"\"\"\n",
    "    RCNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        \n",
    "        # 定义l2损失\n",
    "        l2Loss = tf.constant(0.0)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用预训练的词向量初始化词嵌入矩阵\n",
    "            self.W = tf.Variable(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\") ,name=\"W\")\n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(self.W, self.inputX)\n",
    "            # 复制一份embedding input\n",
    "            self.embeddedWords_ = self.embeddedWords\n",
    "            \n",
    "        # 定义两层双向LSTM的模型结构\n",
    "\n",
    "#         with tf.name_scope(\"Bi-LSTM\"):\n",
    "#             fwHiddenLayers = []\n",
    "#             bwHiddenLayers = []\n",
    "#             for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "\n",
    "#                 with tf.name_scope(\"Bi-LSTM-\" + str(idx)):\n",
    "#                     # 定义前向LSTM结构\n",
    "#                     lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "#                                                                  output_keep_prob=self.dropoutKeepProb)\n",
    "#                     # 定义反向LSTM结构\n",
    "#                     lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "#                                                                  output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "#                 fwHiddenLayers.append(lstmFwCell)\n",
    "#                 bwHiddenLayers.append(lstmBwCell)\n",
    "\n",
    "#             # 实现多层的LSTM结构， state_is_tuple=True，则状态会以元祖的形式组合(h, c)，否则列向拼接\n",
    "#             fwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=fwHiddenLayers, state_is_tuple=True)\n",
    "#             bwMultiLstm = tf.nn.rnn_cell.MultiRNNCell(cells=bwHiddenLayers, state_is_tuple=True)\n",
    "\n",
    "#             # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "#             # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "#             # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "#             outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(fwMultiLstm, bwMultiLstm, self.embeddedWords, dtype=tf.float32)\n",
    "#             fwOutput, bwOutput = outputs\n",
    "\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "            for idx, hiddenSize in enumerate(config.model.hiddenSizes):\n",
    "                with tf.name_scope(\"Bi-LSTM\" + str(idx)):\n",
    "                    # 定义前向LSTM结构\n",
    "                    lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "                    # 定义反向LSTM结构\n",
    "                    lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=hiddenSize, state_is_tuple=True),\n",
    "                                                                 output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "\n",
    "                    # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "                    # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "                    # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "                    outputs_, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
    "                                                                                  self.embeddedWords_, dtype=tf.float32,\n",
    "                                                                                  scope=\"bi-lstm\" + str(idx))\n",
    "        \n",
    "                    # 对outputs中的fw和bw的结果拼接 [batch_size, time_step, hidden_size * 2], 传入到下一层Bi-LSTM中\n",
    "                    self.embeddedWords_ = tf.concat(outputs_, 2)\n",
    "                \n",
    "        # 将最后一层Bi-LSTM输出的结果分割成前向和后向的输出\n",
    "        fwOutput, bwOutput = tf.split(self.embeddedWords_, 2, -1)\n",
    "            \n",
    "        with tf.name_scope(\"context\"):\n",
    "            shape = [tf.shape(fwOutput)[0], 1, tf.shape(fwOutput)[2]]\n",
    "            self.contextLeft = tf.concat([tf.zeros(shape), fwOutput[:, :-1]], axis=1, name=\"contextLeft\")\n",
    "            self.contextRight = tf.concat([bwOutput[:, 1:], tf.zeros(shape)], axis=1, name=\"contextRight\")\n",
    "            \n",
    "        # 将前向，后向的输出和最早的词向量拼接在一起得到最终的词表征\n",
    "        with tf.name_scope(\"wordRepresentation\"):\n",
    "            self.wordRepre = tf.concat([self.contextLeft, self.embeddedWords, self.contextRight], axis=2)\n",
    "            wordSize = config.model.hiddenSizes[-1] * 2 + config.model.embeddingSize \n",
    "        \n",
    "        with tf.name_scope(\"textRepresentation\"):\n",
    "            outputSize = config.model.outputSize\n",
    "            textW = tf.Variable(tf.random_uniform([wordSize, outputSize], -1.0, 1.0), name=\"W2\")\n",
    "            textB = tf.Variable(tf.constant(0.1, shape=[outputSize]), name=\"b2\")\n",
    "            \n",
    "            # tf.einsum可以指定维度的消除运算\n",
    "            self.textRepre = tf.tanh(tf.einsum('aij,jk->aik', self.wordRepre, textW) + textB)\n",
    "            \n",
    "        # 做max-pool的操作，将时间步的维度消失\n",
    "        output = tf.reduce_max(self.textRepre, axis=1)\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            l2Loss += tf.nn.l2_loss(outputW)\n",
    "            l2Loss += tf.nn.l2_loss(outputB)\n",
    "            self.predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "            self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "        \n",
    "        # 计算二元交叉熵损失\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            \n",
    "            losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "            self.loss = tf.reduce_mean(losses) + config.model.l2RegLambda * l2Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"context/contextLeft:0\", shape=(?, 200, 128), dtype=float32)\n",
      "Tensor(\"context/contextRight:0\", shape=(?, 200, 128), dtype=float32)\n",
      "Tensor(\"wordRepresentation/concat:0\", shape=(?, 200, 456), dtype=float32)\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/fw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/hist is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name bi-lstm0/bw/lstm_cell/bias:0/grad/sparsity is illegal; using bi-lstm0/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name textRepresentation/W2:0/grad/hist is illegal; using textRepresentation/W2_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name textRepresentation/W2:0/grad/sparsity is illegal; using textRepresentation/W2_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name textRepresentation/b2:0/grad/hist is illegal; using textRepresentation/b2_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name textRepresentation/b2:0/grad/sparsity is illegal; using textRepresentation/b2_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/hist is illegal; using outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name outputW:0/grad/sparsity is illegal; using outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/hist is illegal; using output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/outputB:0/grad/sparsity is illegal; using output/outputB_0/grad/sparsity instead.\n",
      "Writing to D:\\pythonRepo\\textClassifier\\RCNN\\summarys\n",
      "\n",
      "start training model\n",
      "2019-03-06T16:31:30.323978, step: 1, loss: 0.7492114305496216, acc: 0.4688, auc: 0.4145, precision: 0.4524, recall: 0.6333\n",
      "2019-03-06T16:31:30.767822, step: 2, loss: 0.6868022680282593, acc: 0.4609, auc: 0.561, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:31.157748, step: 3, loss: 0.6929646730422974, acc: 0.5234, auc: 0.565, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:31.533742, step: 4, loss: 0.71061110496521, acc: 0.4922, auc: 0.42, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:31.910735, step: 5, loss: 0.7061306834220886, acc: 0.4609, auc: 0.4559, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:32.298697, step: 6, loss: 0.6964321136474609, acc: 0.5312, auc: 0.4804, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:32.677770, step: 7, loss: 0.6941243410110474, acc: 0.4766, auc: 0.496, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:33.058780, step: 8, loss: 0.6939561367034912, acc: 0.5156, auc: 0.5205, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:33.465801, step: 9, loss: 0.6952872276306152, acc: 0.4297, auc: 0.4802, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:33.843792, step: 10, loss: 0.6933609247207642, acc: 0.5156, auc: 0.5239, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:34.226766, step: 11, loss: 0.700320839881897, acc: 0.5312, auc: 0.4402, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:34.612338, step: 12, loss: 0.6906006336212158, acc: 0.5703, auc: 0.5203, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:34.996311, step: 13, loss: 0.6973238587379456, acc: 0.4609, auc: 0.4989, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:35.372337, step: 14, loss: 0.6903687715530396, acc: 0.5547, auc: 0.4922, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:35.763874, step: 15, loss: 0.6950944662094116, acc: 0.5, auc: 0.4929, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:36.140837, step: 16, loss: 0.6946186423301697, acc: 0.5312, auc: 0.41, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:36.526993, step: 17, loss: 0.7036144733428955, acc: 0.4219, auc: 0.5128, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:36.907974, step: 18, loss: 0.7059215307235718, acc: 0.3906, auc: 0.5174, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:37.304914, step: 19, loss: 0.6930527091026306, acc: 0.5234, auc: 0.4683, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:37.685894, step: 20, loss: 0.695935845375061, acc: 0.5312, auc: 0.4488, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:38.069898, step: 21, loss: 0.6968294978141785, acc: 0.4609, auc: 0.383, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:38.450880, step: 22, loss: 0.6887440085411072, acc: 0.4297, auc: 0.5255, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:38.833959, step: 23, loss: 0.6985455751419067, acc: 0.5234, auc: 0.4659, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:39.215124, step: 24, loss: 0.6892092227935791, acc: 0.4531, auc: 0.5202, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:39.600092, step: 25, loss: 0.6857477426528931, acc: 0.4297, auc: 0.5278, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:39.974064, step: 26, loss: 0.6934006214141846, acc: 0.4688, auc: 0.448, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:40.364049, step: 27, loss: 0.6994789838790894, acc: 0.5, auc: 0.4922, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:40.745033, step: 28, loss: 0.6876206398010254, acc: 0.4609, auc: 0.6057, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:41.131967, step: 29, loss: 0.7122271060943604, acc: 0.5547, auc: 0.5088, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:41.516141, step: 30, loss: 0.7097133994102478, acc: 0.5625, auc: 0.5404, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:41.907095, step: 31, loss: 0.687859833240509, acc: 0.4531, auc: 0.5382, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:42.300175, step: 32, loss: 0.6954659819602966, acc: 0.5234, auc: 0.6345, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:42.682154, step: 33, loss: 0.7012239098548889, acc: 0.6094, auc: 0.5867, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:43.057151, step: 34, loss: 0.6929643750190735, acc: 0.5312, auc: 0.5456, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:43.452095, step: 35, loss: 0.6962538957595825, acc: 0.4453, auc: 0.4734, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:43.830084, step: 36, loss: 0.6888274550437927, acc: 0.5625, auc: 0.4566, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:44.210068, step: 37, loss: 0.6865533590316772, acc: 0.5625, auc: 0.4807, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:44.601729, step: 38, loss: 0.6971627473831177, acc: 0.5, auc: 0.541, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:44.990689, step: 39, loss: 0.703591525554657, acc: 0.4844, auc: 0.5105, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:45.369781, step: 40, loss: 0.7018455266952515, acc: 0.4922, auc: 0.5521, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:45.750051, step: 41, loss: 0.6924666166305542, acc: 0.5312, auc: 0.5289, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:46.124051, step: 42, loss: 0.7023823261260986, acc: 0.4922, auc: 0.4762, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:46.505001, step: 43, loss: 0.7124037742614746, acc: 0.4453, auc: 0.428, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:46.883987, step: 44, loss: 0.697925329208374, acc: 0.5, auc: 0.4543, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:47.259981, step: 45, loss: 0.6940183639526367, acc: 0.4844, auc: 0.5596, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:47.638107, step: 46, loss: 0.6938184499740601, acc: 0.5078, auc: 0.5179, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:48.014102, step: 47, loss: 0.6916506290435791, acc: 0.4922, auc: 0.526, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:48.392238, step: 48, loss: 0.6868472695350647, acc: 0.4609, auc: 0.576, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:48.768233, step: 49, loss: 0.6943910717964172, acc: 0.5, auc: 0.5112, precision: 0.0, recall: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:31:49.153232, step: 50, loss: 0.6995680928230286, acc: 0.5, auc: 0.5171, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:49.534184, step: 51, loss: 0.6941133737564087, acc: 0.4531, auc: 0.4042, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:49.915198, step: 52, loss: 0.7046807408332825, acc: 0.5312, auc: 0.5586, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:50.286173, step: 53, loss: 0.7045686841011047, acc: 0.5391, auc: 0.4949, precision: 1.0, recall: 0.0167\n",
      "2019-03-06T16:31:50.676238, step: 54, loss: 0.6949774026870728, acc: 0.4922, auc: 0.4857, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:51.057252, step: 55, loss: 0.697566032409668, acc: 0.5234, auc: 0.5143, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:51.443344, step: 56, loss: 0.6845508813858032, acc: 0.4297, auc: 0.6017, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:51.821303, step: 57, loss: 0.6892661452293396, acc: 0.4609, auc: 0.5652, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:52.205276, step: 58, loss: 0.6935436725616455, acc: 0.4844, auc: 0.4572, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:52.577281, step: 59, loss: 0.6873258352279663, acc: 0.4531, auc: 0.6027, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:52.957266, step: 60, loss: 0.6937398314476013, acc: 0.4688, auc: 0.4958, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:53.330268, step: 61, loss: 0.6881240010261536, acc: 0.4688, auc: 0.5949, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:53.701555, step: 62, loss: 0.6937810182571411, acc: 0.4922, auc: 0.5072, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:54.076553, step: 63, loss: 0.685562014579773, acc: 0.4219, auc: 0.6171, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:54.459164, step: 64, loss: 0.6978057622909546, acc: 0.4844, auc: 0.444, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:54.844132, step: 65, loss: 0.6886472702026367, acc: 0.4531, auc: 0.5416, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:55.233062, step: 66, loss: 0.6974568367004395, acc: 0.5078, auc: 0.4339, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:55.609088, step: 67, loss: 0.6858956813812256, acc: 0.4453, auc: 0.5303, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:55.987202, step: 68, loss: 0.6884101629257202, acc: 0.5, auc: 0.6311, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:56.362201, step: 69, loss: 0.7089169025421143, acc: 0.5625, auc: 0.5062, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:56.737481, step: 70, loss: 0.6950200796127319, acc: 0.5312, auc: 0.5414, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:57.116466, step: 71, loss: 0.6839803457260132, acc: 0.4531, auc: 0.6101, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:57.505536, step: 72, loss: 0.6879090070724487, acc: 0.4766, auc: 0.5845, precision: 0.5, recall: 0.0149\n",
      "2019-03-06T16:31:57.885519, step: 73, loss: 0.7032462358474731, acc: 0.5703, auc: 0.4864, precision: 0.5, recall: 0.0182\n",
      "2019-03-06T16:31:58.261547, step: 74, loss: 0.6984466910362244, acc: 0.4766, auc: 0.5053, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:58.647481, step: 75, loss: 0.6918675899505615, acc: 0.4453, auc: 0.5295, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:59.030489, step: 76, loss: 0.6919665932655334, acc: 0.5, auc: 0.5249, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:59.414460, step: 77, loss: 0.6905986070632935, acc: 0.5391, auc: 0.5296, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:31:59.806158, step: 78, loss: 0.6898547410964966, acc: 0.4453, auc: 0.5881, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:00.207313, step: 79, loss: 0.6932856440544128, acc: 0.5938, auc: 0.4489, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:00.598267, step: 80, loss: 0.6888923048973083, acc: 0.5156, auc: 0.5161, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:00.999195, step: 81, loss: 0.6908884048461914, acc: 0.5078, auc: 0.5389, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:01.381175, step: 82, loss: 0.7023749947547913, acc: 0.4844, auc: 0.4775, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:01.777123, step: 83, loss: 0.6999894380569458, acc: 0.3984, auc: 0.5381, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:02.163083, step: 84, loss: 0.6914833784103394, acc: 0.4844, auc: 0.5254, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:02.535192, step: 85, loss: 0.6952221393585205, acc: 0.3672, auc: 0.4389, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:02.913189, step: 86, loss: 0.6826052665710449, acc: 0.5, auc: 0.6252, precision: 1.0, recall: 0.0154\n",
      "2019-03-06T16:32:03.292284, step: 87, loss: 0.7315849661827087, acc: 0.5625, auc: 0.4892, precision: 0.2727, recall: 0.0588\n",
      "2019-03-06T16:32:03.683237, step: 88, loss: 0.6974555253982544, acc: 0.4688, auc: 0.4854, precision: 0.5455, recall: 0.087\n",
      "2019-03-06T16:32:04.068205, step: 89, loss: 0.7143362164497375, acc: 0.4844, auc: 0.4724, precision: 0.1667, recall: 0.0161\n",
      "2019-03-06T16:32:04.453716, step: 90, loss: 0.70534348487854, acc: 0.5391, auc: 0.5212, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:04.833700, step: 91, loss: 0.6971772313117981, acc: 0.5, auc: 0.5125, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:05.208698, step: 92, loss: 0.6889302730560303, acc: 0.3906, auc: 0.5438, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:05.580859, step: 93, loss: 0.6872773170471191, acc: 0.5078, auc: 0.5849, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:05.950963, step: 94, loss: 0.6953616738319397, acc: 0.4688, auc: 0.51, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:06.326119, step: 95, loss: 0.6940985321998596, acc: 0.4766, auc: 0.5146, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:06.695133, step: 96, loss: 0.6892185211181641, acc: 0.4375, auc: 0.5722, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:07.077110, step: 97, loss: 0.6858506798744202, acc: 0.5, auc: 0.5947, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:07.461083, step: 98, loss: 0.6952126026153564, acc: 0.5625, auc: 0.551, precision: 1.0, recall: 0.0175\n",
      "2019-03-06T16:32:07.838268, step: 99, loss: 0.6961159706115723, acc: 0.5781, auc: 0.5611, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:08.214231, step: 100, loss: 0.6850872039794922, acc: 0.4609, auc: 0.591, precision: 0.0, recall: 0.0\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T16:32:23.428237, step: 100, loss: 0.740526000658671, acc: 0.5128230769230769, auc: 0.5153410256410256, precision: 0.5137564102564104, recall: 0.6948128205128205\n",
      "2019-03-06T16:32:23.820418, step: 101, loss: 0.6957229375839233, acc: 0.5703, auc: 0.4857, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:24.206535, step: 102, loss: 0.6958796977996826, acc: 0.4922, auc: 0.5055, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:24.586988, step: 103, loss: 0.6898125410079956, acc: 0.5, auc: 0.5679, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:24.969962, step: 104, loss: 0.6897759437561035, acc: 0.5078, auc: 0.5455, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:25.356960, step: 105, loss: 0.6879129409790039, acc: 0.5312, auc: 0.5466, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:25.835771, step: 106, loss: 0.6839789152145386, acc: 0.5, auc: 0.6118, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:26.212794, step: 107, loss: 0.678989052772522, acc: 0.5391, auc: 0.6249, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:26.600863, step: 108, loss: 0.6828938722610474, acc: 0.5469, auc: 0.5862, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:26.988863, step: 109, loss: 0.6766053438186646, acc: 0.5469, auc: 0.6246, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:27.459790, step: 110, loss: 0.681037425994873, acc: 0.5234, auc: 0.6083, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:27.846846, step: 111, loss: 0.6785513758659363, acc: 0.5625, auc: 0.5967, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:28.317592, step: 112, loss: 0.6851588487625122, acc: 0.5469, auc: 0.5557, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:28.698541, step: 113, loss: 0.6846354007720947, acc: 0.5547, auc: 0.5705, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:29.132381, step: 114, loss: 0.6982899904251099, acc: 0.4766, auc: 0.6115, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:29.553392, step: 115, loss: 0.6744763255119324, acc: 0.5625, auc: 0.6002, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:29.928961, step: 116, loss: 0.6700453758239746, acc: 0.6016, auc: 0.6007, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:30.312933, step: 117, loss: 0.7019822001457214, acc: 0.4766, auc: 0.5889, precision: 0.0, recall: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:32:30.690924, step: 118, loss: 0.6970404386520386, acc: 0.5078, auc: 0.5414, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:31.086834, step: 119, loss: 0.682064414024353, acc: 0.5391, auc: 0.5905, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:31.475825, step: 120, loss: 0.6814188957214355, acc: 0.4844, auc: 0.6227, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:31.870769, step: 121, loss: 0.6842150688171387, acc: 0.5547, auc: 0.592, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:32.265683, step: 122, loss: 0.6854759454727173, acc: 0.4766, auc: 0.5696, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:32.658835, step: 123, loss: 0.6752969622612, acc: 0.4297, auc: 0.6169, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:33.040816, step: 124, loss: 0.6952329874038696, acc: 0.5859, auc: 0.5907, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:33.449867, step: 125, loss: 0.6755962371826172, acc: 0.4922, auc: 0.6474, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:33.826860, step: 126, loss: 0.6734633445739746, acc: 0.5312, auc: 0.6599, precision: 1.0, recall: 0.0164\n",
      "2019-03-06T16:32:34.211830, step: 127, loss: 0.6841596364974976, acc: 0.5, auc: 0.5911, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:34.607321, step: 128, loss: 0.681281566619873, acc: 0.4844, auc: 0.6227, precision: 1.0, recall: 0.0149\n",
      "2019-03-06T16:32:35.005257, step: 129, loss: 0.681159496307373, acc: 0.5234, auc: 0.5943, precision: 1.0, recall: 0.0161\n",
      "2019-03-06T16:32:35.391224, step: 130, loss: 0.6761500835418701, acc: 0.5, auc: 0.6499, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:35.782647, step: 131, loss: 0.6860517263412476, acc: 0.3906, auc: 0.5913, precision: 1.0, recall: 0.0127\n",
      "2019-03-06T16:32:36.182776, step: 132, loss: 0.6966404914855957, acc: 0.5312, auc: 0.5034, precision: 1.0, recall: 0.0323\n",
      "2019-03-06T16:32:36.571737, step: 133, loss: 0.6689283847808838, acc: 0.5938, auc: 0.7177, precision: 0.8, recall: 0.1379\n",
      "2019-03-06T16:32:36.962690, step: 134, loss: 0.6609585285186768, acc: 0.5234, auc: 0.6782, precision: 0.8182, recall: 0.1324\n",
      "2019-03-06T16:32:37.368637, step: 135, loss: 0.6765618324279785, acc: 0.5938, auc: 0.6626, precision: 0.8, recall: 0.1967\n",
      "2019-03-06T16:32:37.751610, step: 136, loss: 0.6884360313415527, acc: 0.5938, auc: 0.6303, precision: 0.8, recall: 0.1379\n",
      "2019-03-06T16:32:38.138577, step: 137, loss: 0.6764687895774841, acc: 0.4844, auc: 0.6115, precision: 0.6667, recall: 0.0299\n",
      "2019-03-06T16:32:38.521731, step: 138, loss: 0.6764307022094727, acc: 0.5469, auc: 0.6229, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:38.918672, step: 139, loss: 0.6852887868881226, acc: 0.5, auc: 0.605, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:39.301648, step: 140, loss: 0.6839184165000916, acc: 0.4688, auc: 0.6615, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:39.681631, step: 141, loss: 0.6788831353187561, acc: 0.4844, auc: 0.6694, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:40.120427, step: 142, loss: 0.6466813087463379, acc: 0.5156, auc: 0.7889, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:40.532362, step: 143, loss: 0.6582205295562744, acc: 0.5078, auc: 0.7184, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:40.949240, step: 144, loss: 0.6779624223709106, acc: 0.4844, auc: 0.6212, precision: 0.0, recall: 0.0\n",
      "2019-03-06T16:32:41.326231, step: 145, loss: 0.6381077766418457, acc: 0.5312, auc: 0.8255, precision: 0.5, recall: 0.0167\n",
      "2019-03-06T16:32:41.706437, step: 146, loss: 0.6321185827255249, acc: 0.5312, auc: 0.8174, precision: 1.0, recall: 0.0323\n",
      "2019-03-06T16:32:42.092375, step: 147, loss: 0.6355268955230713, acc: 0.5859, auc: 0.7714, precision: 1.0, recall: 0.0364\n",
      "2019-03-06T16:32:42.471557, step: 148, loss: 0.6268967390060425, acc: 0.5625, auc: 0.8047, precision: 0.6316, recall: 0.1967\n",
      "2019-03-06T16:32:42.854500, step: 149, loss: 0.6213699579238892, acc: 0.7031, auc: 0.8182, precision: 0.7838, recall: 0.4915\n",
      "2019-03-06T16:32:43.230523, step: 150, loss: 0.6266287565231323, acc: 0.5781, auc: 0.7683, precision: 0.7619, recall: 0.2462\n",
      "2019-03-06T16:32:43.610510, step: 151, loss: 0.6408931612968445, acc: 0.5859, auc: 0.7217, precision: 0.8571, recall: 0.1034\n",
      "2019-03-06T16:32:43.994483, step: 152, loss: 0.6608643531799316, acc: 0.5547, auc: 0.6531, precision: 1.0, recall: 0.1094\n",
      "2019-03-06T16:32:44.391906, step: 153, loss: 0.5795702338218689, acc: 0.6953, auc: 0.8405, precision: 0.9474, recall: 0.3214\n",
      "2019-03-06T16:32:44.789106, step: 154, loss: 0.6250942945480347, acc: 0.6797, auc: 0.7373, precision: 0.7288, recall: 0.6324\n",
      "2019-03-06T16:32:45.166307, step: 155, loss: 0.6330420970916748, acc: 0.7188, auc: 0.7772, precision: 0.7286, recall: 0.75\n",
      "2019-03-06T16:32:45.544263, step: 156, loss: 0.670082688331604, acc: 0.625, auc: 0.7194, precision: 0.6032, recall: 0.623\n",
      "start training model\n",
      "2019-03-06T16:32:45.950177, step: 157, loss: 0.6113190650939941, acc: 0.6328, auc: 0.783, precision: 0.7941, recall: 0.403\n",
      "2019-03-06T16:32:46.332186, step: 158, loss: 0.6400335431098938, acc: 0.5234, auc: 0.7354, precision: 0.8, recall: 0.0625\n",
      "2019-03-06T16:32:46.720148, step: 159, loss: 0.6509706974029541, acc: 0.4922, auc: 0.8397, precision: 0.75, recall: 0.0448\n",
      "2019-03-06T16:32:47.107111, step: 160, loss: 0.6011378765106201, acc: 0.6016, auc: 0.8172, precision: 1.0, recall: 0.1207\n",
      "2019-03-06T16:32:47.490367, step: 161, loss: 0.5325286388397217, acc: 0.6562, auc: 0.9067, precision: 0.85, recall: 0.2931\n",
      "2019-03-06T16:32:47.881440, step: 162, loss: 0.5841705799102783, acc: 0.6641, auc: 0.8096, precision: 0.7609, recall: 0.5224\n",
      "2019-03-06T16:32:48.260560, step: 163, loss: 0.5686460137367249, acc: 0.75, auc: 0.8277, precision: 0.8302, recall: 0.6567\n",
      "2019-03-06T16:32:48.651515, step: 164, loss: 0.5289522409439087, acc: 0.75, auc: 0.8789, precision: 0.8511, recall: 0.6154\n",
      "2019-03-06T16:32:49.041472, step: 165, loss: 0.6026167869567871, acc: 0.6172, auc: 0.8107, precision: 0.9, recall: 0.3699\n",
      "2019-03-06T16:32:49.513182, step: 166, loss: 0.5280007719993591, acc: 0.75, auc: 0.8794, precision: 0.9143, recall: 0.5246\n",
      "2019-03-06T16:32:49.942035, step: 167, loss: 0.5543809533119202, acc: 0.7422, auc: 0.8038, precision: 0.8475, recall: 0.6757\n",
      "2019-03-06T16:32:50.337004, step: 168, loss: 0.5524715781211853, acc: 0.7891, auc: 0.8239, precision: 0.7727, recall: 0.9067\n",
      "2019-03-06T16:32:50.725040, step: 169, loss: 0.6913479566574097, acc: 0.6328, auc: 0.73, precision: 0.581, recall: 0.9531\n",
      "2019-03-06T16:32:51.112036, step: 170, loss: 0.641539454460144, acc: 0.7344, auc: 0.7167, precision: 0.6566, recall: 1.0\n",
      "2019-03-06T16:32:51.493116, step: 171, loss: 0.5439646244049072, acc: 0.7578, auc: 0.8541, precision: 0.7091, recall: 0.7222\n",
      "2019-03-06T16:32:51.868309, step: 172, loss: 0.6062276363372803, acc: 0.6172, auc: 0.8003, precision: 0.8, recall: 0.3125\n",
      "2019-03-06T16:32:52.256242, step: 173, loss: 0.6547605991363525, acc: 0.5312, auc: 0.8481, precision: 0.913, recall: 0.2658\n",
      "2019-03-06T16:32:52.635260, step: 174, loss: 0.5135126709938049, acc: 0.7812, auc: 0.8766, precision: 0.8696, recall: 0.6452\n",
      "2019-03-06T16:32:53.016210, step: 175, loss: 0.5000877976417542, acc: 0.8047, auc: 0.8853, precision: 0.8889, recall: 0.6667\n",
      "2019-03-06T16:32:53.390211, step: 176, loss: 0.52633136510849, acc: 0.7812, auc: 0.8532, precision: 0.9057, recall: 0.6761\n",
      "2019-03-06T16:32:53.759491, step: 177, loss: 0.508155345916748, acc: 0.8438, auc: 0.8948, precision: 0.8548, recall: 0.8281\n",
      "2019-03-06T16:32:54.149487, step: 178, loss: 0.5093284845352173, acc: 0.7656, auc: 0.8553, precision: 0.8209, recall: 0.7534\n",
      "2019-03-06T16:32:54.528942, step: 179, loss: 0.5645921230316162, acc: 0.7422, auc: 0.8093, precision: 0.7143, recall: 0.75\n",
      "2019-03-06T16:32:54.909921, step: 180, loss: 0.49481824040412903, acc: 0.7891, auc: 0.8685, precision: 0.8421, recall: 0.7273\n",
      "2019-03-06T16:32:55.297853, step: 181, loss: 0.52265864610672, acc: 0.7734, auc: 0.8475, precision: 0.8039, recall: 0.6833\n",
      "2019-03-06T16:32:55.692833, step: 182, loss: 0.5186076760292053, acc: 0.75, auc: 0.8421, precision: 0.8824, recall: 0.6338\n",
      "2019-03-06T16:32:56.090975, step: 183, loss: 0.5395684838294983, acc: 0.7422, auc: 0.8389, precision: 0.8913, recall: 0.5942\n",
      "2019-03-06T16:32:56.477243, step: 184, loss: 0.5333790183067322, acc: 0.7578, auc: 0.8446, precision: 0.86, recall: 0.6418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:32:56.871191, step: 185, loss: 0.47419634461402893, acc: 0.8203, auc: 0.8789, precision: 0.85, recall: 0.7846\n",
      "2019-03-06T16:32:57.265278, step: 186, loss: 0.45825594663619995, acc: 0.8281, auc: 0.9064, precision: 0.8136, recall: 0.8136\n",
      "2019-03-06T16:32:57.640295, step: 187, loss: 0.509143054485321, acc: 0.7734, auc: 0.8389, precision: 0.7941, recall: 0.7826\n",
      "2019-03-06T16:32:58.020258, step: 188, loss: 0.5246121287345886, acc: 0.7422, auc: 0.8365, precision: 0.7458, recall: 0.7097\n",
      "2019-03-06T16:32:58.409217, step: 189, loss: 0.45105502009391785, acc: 0.8516, auc: 0.8614, precision: 0.8772, recall: 0.8065\n",
      "2019-03-06T16:32:58.787207, step: 190, loss: 0.4695312976837158, acc: 0.7734, auc: 0.8604, precision: 0.871, recall: 0.72\n",
      "2019-03-06T16:32:59.162236, step: 191, loss: 0.48737162351608276, acc: 0.7891, auc: 0.8532, precision: 0.8462, recall: 0.7639\n",
      "2019-03-06T16:32:59.560257, step: 192, loss: 0.46560007333755493, acc: 0.8438, auc: 0.8851, precision: 0.8269, recall: 0.7963\n",
      "2019-03-06T16:32:59.935517, step: 193, loss: 0.5434787273406982, acc: 0.7578, auc: 0.8277, precision: 0.7313, recall: 0.7903\n",
      "2019-03-06T16:33:00.332820, step: 194, loss: 0.47781896591186523, acc: 0.7812, auc: 0.8749, precision: 0.8571, recall: 0.6207\n",
      "2019-03-06T16:33:00.717761, step: 195, loss: 0.5115500092506409, acc: 0.7578, auc: 0.8358, precision: 0.8936, recall: 0.6176\n",
      "2019-03-06T16:33:01.091789, step: 196, loss: 0.4654291272163391, acc: 0.7891, auc: 0.8734, precision: 0.907, recall: 0.629\n",
      "2019-03-06T16:33:01.468752, step: 197, loss: 0.4436021149158478, acc: 0.7969, auc: 0.9111, precision: 0.9273, recall: 0.6986\n",
      "2019-03-06T16:33:01.854751, step: 198, loss: 0.4898689389228821, acc: 0.7891, auc: 0.8652, precision: 0.7564, recall: 0.8806\n",
      "2019-03-06T16:33:02.246672, step: 199, loss: 0.40708112716674805, acc: 0.8594, auc: 0.9172, precision: 0.8611, recall: 0.8857\n",
      "2019-03-06T16:33:02.631674, step: 200, loss: 0.4062068462371826, acc: 0.875, auc: 0.9075, precision: 0.84, recall: 0.9403\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T16:33:18.667360, step: 200, loss: 0.45875957455390537, acc: 0.8233205128205129, auc: 0.8851615384615383, precision: 0.8058820512820514, recall: 0.8578743589743589\n",
      "2019-03-06T16:33:19.048340, step: 201, loss: 0.40657442808151245, acc: 0.8438, auc: 0.9138, precision: 0.94, recall: 0.7344\n",
      "2019-03-06T16:33:19.427325, step: 202, loss: 0.3668785095214844, acc: 0.875, auc: 0.9381, precision: 0.8776, recall: 0.8113\n",
      "2019-03-06T16:33:19.820275, step: 203, loss: 0.4181711673736572, acc: 0.8438, auc: 0.8906, precision: 0.8413, recall: 0.8413\n",
      "2019-03-06T16:33:20.201255, step: 204, loss: 0.44102877378463745, acc: 0.8125, auc: 0.8848, precision: 0.7937, recall: 0.8197\n",
      "2019-03-06T16:33:20.591406, step: 205, loss: 0.4746937155723572, acc: 0.7734, auc: 0.8784, precision: 0.7349, recall: 0.8971\n",
      "2019-03-06T16:33:20.969394, step: 206, loss: 0.530835747718811, acc: 0.7734, auc: 0.8748, precision: 0.6966, recall: 0.9688\n",
      "2019-03-06T16:33:21.346508, step: 207, loss: 0.37180936336517334, acc: 0.875, auc: 0.9103, precision: 0.8553, recall: 0.9286\n",
      "2019-03-06T16:33:21.727752, step: 208, loss: 0.3498087227344513, acc: 0.8516, auc: 0.9492, precision: 0.9273, recall: 0.7727\n",
      "2019-03-06T16:33:22.111726, step: 209, loss: 0.4131668508052826, acc: 0.8203, auc: 0.9429, precision: 0.9592, recall: 0.6912\n",
      "2019-03-06T16:33:22.487717, step: 210, loss: 0.4582502841949463, acc: 0.8047, auc: 0.8792, precision: 0.8421, recall: 0.75\n",
      "2019-03-06T16:33:22.868701, step: 211, loss: 0.38267213106155396, acc: 0.8672, auc: 0.8986, precision: 0.8904, recall: 0.8784\n",
      "2019-03-06T16:33:23.253640, step: 212, loss: 0.422823429107666, acc: 0.8516, auc: 0.8997, precision: 0.7917, recall: 0.9344\n",
      "2019-03-06T16:33:23.633820, step: 213, loss: 0.4248204529285431, acc: 0.7891, auc: 0.8889, precision: 0.7778, recall: 0.8358\n",
      "2019-03-06T16:33:24.013804, step: 214, loss: 0.4914271831512451, acc: 0.7969, auc: 0.8266, precision: 0.7419, recall: 0.8214\n",
      "2019-03-06T16:33:24.386451, step: 215, loss: 0.3709862530231476, acc: 0.8359, auc: 0.9308, precision: 0.8776, recall: 0.7414\n",
      "2019-03-06T16:33:24.805390, step: 216, loss: 0.5319401025772095, acc: 0.7656, auc: 0.8571, precision: 0.9231, recall: 0.5714\n",
      "2019-03-06T16:33:25.180386, step: 217, loss: 0.4961055517196655, acc: 0.75, auc: 0.8725, precision: 0.907, recall: 0.5821\n",
      "2019-03-06T16:33:25.560340, step: 218, loss: 0.4275933504104614, acc: 0.8281, auc: 0.8925, precision: 0.7647, recall: 0.7959\n",
      "2019-03-06T16:33:25.940416, step: 219, loss: 0.423713356256485, acc: 0.8281, auc: 0.9081, precision: 0.7568, recall: 0.9333\n",
      "2019-03-06T16:33:26.317437, step: 220, loss: 0.3543253242969513, acc: 0.875, auc: 0.9365, precision: 0.8226, recall: 0.9107\n",
      "2019-03-06T16:33:26.698576, step: 221, loss: 0.36625930666923523, acc: 0.8516, auc: 0.9192, precision: 0.8689, recall: 0.8281\n",
      "2019-03-06T16:33:27.072576, step: 222, loss: 0.41668564081192017, acc: 0.8281, auc: 0.9192, precision: 0.95, recall: 0.6552\n",
      "2019-03-06T16:33:27.443721, step: 223, loss: 0.3135538697242737, acc: 0.8984, auc: 0.9516, precision: 0.8966, recall: 0.8814\n",
      "2019-03-06T16:33:27.826846, step: 224, loss: 0.3784506618976593, acc: 0.8359, auc: 0.9128, precision: 0.8644, recall: 0.7969\n",
      "2019-03-06T16:33:28.208795, step: 225, loss: 0.32752150297164917, acc: 0.8828, auc: 0.9434, precision: 0.8571, recall: 0.9\n",
      "2019-03-06T16:33:28.587783, step: 226, loss: 0.33861076831817627, acc: 0.8906, auc: 0.9493, precision: 0.8286, recall: 0.9667\n",
      "2019-03-06T16:33:29.078471, step: 227, loss: 0.445237398147583, acc: 0.8047, auc: 0.868, precision: 0.8246, recall: 0.7581\n",
      "2019-03-06T16:33:29.499502, step: 228, loss: 0.4167872667312622, acc: 0.8281, auc: 0.8999, precision: 0.8689, recall: 0.791\n",
      "2019-03-06T16:33:29.890427, step: 229, loss: 0.5282409191131592, acc: 0.75, auc: 0.8539, precision: 0.8824, recall: 0.5172\n",
      "2019-03-06T16:33:30.262553, step: 230, loss: 0.4775388836860657, acc: 0.7656, auc: 0.8961, precision: 0.9286, recall: 0.5909\n",
      "2019-03-06T16:33:30.643807, step: 231, loss: 0.3221414089202881, acc: 0.875, auc: 0.9557, precision: 0.8947, recall: 0.7391\n",
      "2019-03-06T16:33:31.024787, step: 232, loss: 0.38678890466690063, acc: 0.8516, auc: 0.9289, precision: 0.8571, recall: 0.7778\n",
      "2019-03-06T16:33:31.405800, step: 233, loss: 0.36977607011795044, acc: 0.875, auc: 0.9396, precision: 0.8571, recall: 0.9091\n",
      "2019-03-06T16:33:31.782790, step: 234, loss: 0.39976221323013306, acc: 0.8438, auc: 0.9162, precision: 0.8478, recall: 0.75\n",
      "2019-03-06T16:33:32.167731, step: 235, loss: 0.40254226326942444, acc: 0.8281, auc: 0.917, precision: 0.8621, recall: 0.7812\n",
      "2019-03-06T16:33:32.546916, step: 236, loss: 0.45445266366004944, acc: 0.7812, auc: 0.8728, precision: 0.8596, recall: 0.7101\n",
      "2019-03-06T16:33:32.917895, step: 237, loss: 0.37097078561782837, acc: 0.8516, auc: 0.9392, precision: 0.9583, recall: 0.7302\n",
      "2019-03-06T16:33:33.304888, step: 238, loss: 0.3852158784866333, acc: 0.8203, auc: 0.9209, precision: 0.9123, recall: 0.7429\n",
      "2019-03-06T16:33:33.687836, step: 239, loss: 0.3437134027481079, acc: 0.8594, auc: 0.9311, precision: 0.9, recall: 0.8182\n",
      "2019-03-06T16:33:34.064828, step: 240, loss: 0.3494287133216858, acc: 0.875, auc: 0.9281, precision: 0.8947, recall: 0.8361\n",
      "2019-03-06T16:33:34.444236, step: 241, loss: 0.3990422785282135, acc: 0.8359, auc: 0.8973, precision: 0.8507, recall: 0.8382\n",
      "2019-03-06T16:33:34.829206, step: 242, loss: 0.33448144793510437, acc: 0.875, auc: 0.9429, precision: 0.8281, recall: 0.9138\n",
      "2019-03-06T16:33:35.215174, step: 243, loss: 0.3889973759651184, acc: 0.8516, auc: 0.91, precision: 0.8776, recall: 0.7679\n",
      "2019-03-06T16:33:35.585335, step: 244, loss: 0.41092002391815186, acc: 0.8047, auc: 0.9059, precision: 0.902, recall: 0.697\n",
      "2019-03-06T16:33:35.972429, step: 245, loss: 0.36798930168151855, acc: 0.8359, auc: 0.9167, precision: 0.9524, recall: 0.678\n",
      "2019-03-06T16:33:36.367579, step: 246, loss: 0.3885345458984375, acc: 0.8594, auc: 0.8969, precision: 0.9138, recall: 0.803\n",
      "2019-03-06T16:33:36.756540, step: 247, loss: 0.32738378643989563, acc: 0.8906, auc: 0.9347, precision: 0.8667, recall: 0.8966\n",
      "2019-03-06T16:33:37.136524, step: 248, loss: 0.4173538088798523, acc: 0.8203, auc: 0.8947, precision: 0.8281, recall: 0.8154\n",
      "2019-03-06T16:33:37.521494, step: 249, loss: 0.32105547189712524, acc: 0.8906, auc: 0.9476, precision: 0.8875, recall: 0.9342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:33:37.912479, step: 250, loss: 0.38814878463745117, acc: 0.8438, auc: 0.9104, precision: 0.8475, recall: 0.8197\n",
      "2019-03-06T16:33:38.291435, step: 251, loss: 0.4007378816604614, acc: 0.8281, auc: 0.8997, precision: 0.9375, recall: 0.7031\n",
      "2019-03-06T16:33:38.664540, step: 252, loss: 0.3800564706325531, acc: 0.8203, auc: 0.9163, precision: 0.9184, recall: 0.7031\n",
      "2019-03-06T16:33:39.044549, step: 253, loss: 0.3425327241420746, acc: 0.8281, auc: 0.9336, precision: 0.8966, recall: 0.7647\n",
      "2019-03-06T16:33:39.424659, step: 254, loss: 0.3951859772205353, acc: 0.8047, auc: 0.9076, precision: 0.8475, recall: 0.7576\n",
      "2019-03-06T16:33:39.806668, step: 255, loss: 0.36047887802124023, acc: 0.875, auc: 0.9316, precision: 0.8871, recall: 0.8594\n",
      "2019-03-06T16:33:40.182632, step: 256, loss: 0.39444535970687866, acc: 0.8594, auc: 0.9224, precision: 0.7971, recall: 0.9322\n",
      "2019-03-06T16:33:40.564611, step: 257, loss: 0.3307536244392395, acc: 0.8984, auc: 0.9375, precision: 0.8793, recall: 0.8947\n",
      "2019-03-06T16:33:40.940606, step: 258, loss: 0.4275994896888733, acc: 0.8047, auc: 0.8923, precision: 0.8197, recall: 0.7812\n",
      "2019-03-06T16:33:41.314636, step: 259, loss: 0.40910977125167847, acc: 0.8047, auc: 0.8924, precision: 0.8696, recall: 0.678\n",
      "2019-03-06T16:33:41.696708, step: 260, loss: 0.36219173669815063, acc: 0.8359, auc: 0.933, precision: 0.9091, recall: 0.7576\n",
      "2019-03-06T16:33:42.064755, step: 261, loss: 0.3861290514469147, acc: 0.8281, auc: 0.9387, precision: 0.9286, recall: 0.7429\n",
      "2019-03-06T16:33:42.436918, step: 262, loss: 0.38968753814697266, acc: 0.8203, auc: 0.9187, precision: 0.7895, recall: 0.8036\n",
      "2019-03-06T16:33:42.814912, step: 263, loss: 0.40004783868789673, acc: 0.875, auc: 0.9161, precision: 0.8169, recall: 0.9508\n",
      "2019-03-06T16:33:43.185886, step: 264, loss: 0.44806331396102905, acc: 0.8594, auc: 0.9184, precision: 0.7922, recall: 0.9683\n",
      "2019-03-06T16:33:43.566897, step: 265, loss: 0.38675177097320557, acc: 0.8359, auc: 0.9206, precision: 0.8169, recall: 0.8788\n",
      "2019-03-06T16:33:43.947849, step: 266, loss: 0.37662699818611145, acc: 0.8516, auc: 0.9153, precision: 0.8621, recall: 0.8197\n",
      "2019-03-06T16:33:44.327867, step: 267, loss: 0.34653303027153015, acc: 0.8359, auc: 0.9289, precision: 0.8689, recall: 0.803\n",
      "2019-03-06T16:33:44.711564, step: 268, loss: 0.4712798595428467, acc: 0.7734, auc: 0.9078, precision: 0.9565, recall: 0.6197\n",
      "2019-03-06T16:33:45.087527, step: 269, loss: 0.3424058258533478, acc: 0.8438, auc: 0.9403, precision: 0.9123, recall: 0.7761\n",
      "2019-03-06T16:33:45.458646, step: 270, loss: 0.4021539092063904, acc: 0.8203, auc: 0.9306, precision: 0.9623, recall: 0.7083\n",
      "2019-03-06T16:33:45.851747, step: 271, loss: 0.3269970417022705, acc: 0.875, auc: 0.9509, precision: 0.8676, recall: 0.8939\n",
      "2019-03-06T16:33:46.235685, step: 272, loss: 0.4215642809867859, acc: 0.8359, auc: 0.9025, precision: 0.7662, recall: 0.9516\n",
      "2019-03-06T16:33:46.618688, step: 273, loss: 0.43517404794692993, acc: 0.8203, auc: 0.9414, precision: 0.7342, recall: 0.9667\n",
      "2019-03-06T16:33:46.993657, step: 274, loss: 0.42780929803848267, acc: 0.8281, auc: 0.9382, precision: 0.759, recall: 0.9692\n",
      "2019-03-06T16:33:47.390596, step: 275, loss: 0.3783387541770935, acc: 0.8438, auc: 0.9228, precision: 0.8209, recall: 0.873\n",
      "2019-03-06T16:33:47.786982, step: 276, loss: 0.36450719833374023, acc: 0.8359, auc: 0.916, precision: 0.8281, recall: 0.8413\n",
      "2019-03-06T16:33:48.160216, step: 277, loss: 0.4208003580570221, acc: 0.7891, auc: 0.9276, precision: 0.9388, recall: 0.6571\n",
      "2019-03-06T16:33:48.533187, step: 278, loss: 0.42806464433670044, acc: 0.7656, auc: 0.95, precision: 0.9762, recall: 0.5857\n",
      "2019-03-06T16:33:48.929158, step: 279, loss: 0.38780197501182556, acc: 0.8047, auc: 0.9294, precision: 0.8889, recall: 0.6667\n",
      "2019-03-06T16:33:49.302130, step: 280, loss: 0.35532596707344055, acc: 0.875, auc: 0.9453, precision: 0.9464, recall: 0.803\n",
      "2019-03-06T16:33:49.695108, step: 281, loss: 0.39017602801322937, acc: 0.8125, auc: 0.8874, precision: 0.8475, recall: 0.7692\n",
      "2019-03-06T16:33:50.076091, step: 282, loss: 0.345561146736145, acc: 0.8359, auc: 0.9407, precision: 0.8472, recall: 0.8592\n",
      "2019-03-06T16:33:50.444107, step: 283, loss: 0.3402565121650696, acc: 0.8984, auc: 0.9397, precision: 0.8806, recall: 0.9219\n",
      "2019-03-06T16:33:50.823290, step: 284, loss: 0.41066259145736694, acc: 0.8516, auc: 0.9096, precision: 0.8228, recall: 0.9286\n",
      "2019-03-06T16:33:51.202440, step: 285, loss: 0.4040602147579193, acc: 0.8359, auc: 0.9293, precision: 0.7568, recall: 0.9492\n",
      "2019-03-06T16:33:51.586442, step: 286, loss: 0.3237628936767578, acc: 0.8984, auc: 0.9409, precision: 0.8462, recall: 0.9483\n",
      "2019-03-06T16:33:51.970625, step: 287, loss: 0.33327674865722656, acc: 0.875, auc: 0.9313, precision: 0.8846, recall: 0.8214\n",
      "2019-03-06T16:33:52.354632, step: 288, loss: 0.40049129724502563, acc: 0.8203, auc: 0.9116, precision: 0.875, recall: 0.7119\n",
      "2019-03-06T16:33:52.732619, step: 289, loss: 0.3900916278362274, acc: 0.8047, auc: 0.9227, precision: 0.8913, recall: 0.6721\n",
      "2019-03-06T16:33:53.111606, step: 290, loss: 0.3641934096813202, acc: 0.8516, auc: 0.9409, precision: 0.9756, recall: 0.6897\n",
      "2019-03-06T16:33:53.496720, step: 291, loss: 0.47750020027160645, acc: 0.7656, auc: 0.9, precision: 0.881, recall: 0.5968\n",
      "2019-03-06T16:33:53.872685, step: 292, loss: 0.25935059785842896, acc: 0.8828, auc: 0.9671, precision: 0.875, recall: 0.8596\n",
      "2019-03-06T16:33:54.250829, step: 293, loss: 0.3547302484512329, acc: 0.8672, auc: 0.9262, precision: 0.8772, recall: 0.8333\n",
      "2019-03-06T16:33:54.623459, step: 294, loss: 0.38466012477874756, acc: 0.8359, auc: 0.9265, precision: 0.7667, recall: 0.8679\n",
      "2019-03-06T16:33:55.001449, step: 295, loss: 0.3405391573905945, acc: 0.8672, auc: 0.9326, precision: 0.9273, recall: 0.7969\n",
      "2019-03-06T16:33:55.375449, step: 296, loss: 0.33626866340637207, acc: 0.8828, auc: 0.9421, precision: 0.8333, recall: 0.9524\n",
      "2019-03-06T16:33:55.751414, step: 297, loss: 0.3609199821949005, acc: 0.8438, auc: 0.926, precision: 0.9216, recall: 0.746\n",
      "2019-03-06T16:33:56.132572, step: 298, loss: 0.3992322087287903, acc: 0.8047, auc: 0.9058, precision: 0.8462, recall: 0.7213\n",
      "2019-03-06T16:33:56.510561, step: 299, loss: 0.3752102851867676, acc: 0.8047, auc: 0.9256, precision: 0.8793, recall: 0.7391\n",
      "2019-03-06T16:33:56.885558, step: 300, loss: 0.31173646450042725, acc: 0.875, auc: 0.9492, precision: 0.9344, recall: 0.8261\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T16:34:11.901868, step: 300, loss: 0.39167289244822967, acc: 0.8455564102564103, auc: 0.9175230769230766, precision: 0.8099615384615384, recall: 0.9082820512820515\n",
      "2019-03-06T16:34:12.313767, step: 301, loss: 0.3956667184829712, acc: 0.7969, auc: 0.9137, precision: 0.8429, recall: 0.7973\n",
      "2019-03-06T16:34:12.691756, step: 302, loss: 0.38728171586990356, acc: 0.8438, auc: 0.9138, precision: 0.8246, recall: 0.8246\n",
      "2019-03-06T16:34:13.068786, step: 303, loss: 0.36504751443862915, acc: 0.8516, auc: 0.9328, precision: 0.7969, recall: 0.8947\n",
      "2019-03-06T16:34:13.453756, step: 304, loss: 0.2977568507194519, acc: 0.8672, auc: 0.9502, precision: 0.9041, recall: 0.8684\n",
      "2019-03-06T16:34:13.843682, step: 305, loss: 0.3565840423107147, acc: 0.8594, auc: 0.9293, precision: 0.9333, recall: 0.8\n",
      "2019-03-06T16:34:14.228682, step: 306, loss: 0.29943662881851196, acc: 0.8984, auc: 0.9499, precision: 0.8939, recall: 0.9077\n",
      "2019-03-06T16:34:14.637104, step: 307, loss: 0.36667191982269287, acc: 0.8438, auc: 0.916, precision: 0.8929, recall: 0.7812\n",
      "2019-03-06T16:34:15.020111, step: 308, loss: 0.3181260824203491, acc: 0.8359, auc: 0.9555, precision: 0.9767, recall: 0.6774\n",
      "2019-03-06T16:34:15.402191, step: 309, loss: 0.375858336687088, acc: 0.8281, auc: 0.9236, precision: 0.8772, recall: 0.7692\n",
      "2019-03-06T16:34:15.775191, step: 310, loss: 0.30802538990974426, acc: 0.8984, auc: 0.921, precision: 0.8857, recall: 0.9254\n",
      "2019-03-06T16:34:16.148332, step: 311, loss: 0.46198341250419617, acc: 0.8047, auc: 0.8894, precision: 0.7344, recall: 0.8545\n",
      "2019-03-06T16:34:16.524327, step: 312, loss: 0.3475276827812195, acc: 0.8672, auc: 0.9189, precision: 0.84, recall: 0.9265\n",
      "start training model\n",
      "2019-03-06T16:34:16.930241, step: 313, loss: 0.3722709119319916, acc: 0.8516, auc: 0.9197, precision: 0.8514, recall: 0.8873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:34:17.304241, step: 314, loss: 0.2715761959552765, acc: 0.8984, auc: 0.9672, precision: 0.8475, recall: 0.9259\n",
      "2019-03-06T16:34:17.677442, step: 315, loss: 0.229652538895607, acc: 0.9375, auc: 0.9669, precision: 0.9333, recall: 0.9333\n",
      "2019-03-06T16:34:18.061386, step: 316, loss: 0.32398009300231934, acc: 0.8906, auc: 0.9384, precision: 0.9583, recall: 0.7931\n",
      "2019-03-06T16:34:18.445518, step: 317, loss: 0.3671256899833679, acc: 0.8359, auc: 0.9658, precision: 1.0, recall: 0.7237\n",
      "2019-03-06T16:34:18.828522, step: 318, loss: 0.27849921584129333, acc: 0.8828, auc: 0.9609, precision: 0.9792, recall: 0.7705\n",
      "2019-03-06T16:34:19.206512, step: 319, loss: 0.3171914219856262, acc: 0.8594, auc: 0.9595, precision: 0.9667, recall: 0.7838\n",
      "2019-03-06T16:34:19.576525, step: 320, loss: 0.237511545419693, acc: 0.9297, auc: 0.9568, precision: 0.9559, recall: 0.9155\n",
      "2019-03-06T16:34:19.961465, step: 321, loss: 0.25509148836135864, acc: 0.9297, auc: 0.9763, precision: 0.8857, recall: 0.9841\n",
      "2019-03-06T16:34:20.341449, step: 322, loss: 0.28012463450431824, acc: 0.8984, auc: 0.9866, precision: 0.8451, recall: 0.9677\n",
      "2019-03-06T16:34:20.723538, step: 323, loss: 0.3785412311553955, acc: 0.8672, auc: 0.9315, precision: 0.791, recall: 0.9464\n",
      "2019-03-06T16:34:21.103553, step: 324, loss: 0.22963614761829376, acc: 0.9141, auc: 0.9682, precision: 0.8986, recall: 0.9394\n",
      "2019-03-06T16:34:21.480636, step: 325, loss: 0.25095176696777344, acc: 0.8906, auc: 0.967, precision: 0.9483, recall: 0.8333\n",
      "2019-03-06T16:34:21.862846, step: 326, loss: 0.29794296622276306, acc: 0.875, auc: 0.9571, precision: 0.9038, recall: 0.8103\n",
      "2019-03-06T16:34:22.235853, step: 327, loss: 0.28050002455711365, acc: 0.875, auc: 0.9407, precision: 0.9623, recall: 0.7846\n",
      "2019-03-06T16:34:22.611844, step: 328, loss: 0.31611406803131104, acc: 0.8672, auc: 0.9622, precision: 0.9655, recall: 0.7887\n",
      "2019-03-06T16:34:22.986841, step: 329, loss: 0.17691154778003693, acc: 0.9531, auc: 0.9787, precision: 1.0, recall: 0.8929\n",
      "2019-03-06T16:34:23.366796, step: 330, loss: 0.2902161478996277, acc: 0.875, auc: 0.9512, precision: 0.9273, recall: 0.8095\n",
      "2019-03-06T16:34:23.747142, step: 331, loss: 0.24503223598003387, acc: 0.9141, auc: 0.9596, precision: 0.9434, recall: 0.8621\n",
      "2019-03-06T16:34:24.125129, step: 332, loss: 0.25622376799583435, acc: 0.9062, auc: 0.9642, precision: 0.9118, recall: 0.9118\n",
      "2019-03-06T16:34:24.509223, step: 333, loss: 0.2808770537376404, acc: 0.8984, auc: 0.9624, precision: 0.8696, recall: 0.9375\n",
      "2019-03-06T16:34:24.890236, step: 334, loss: 0.2776467800140381, acc: 0.8984, auc: 0.9764, precision: 0.8261, recall: 0.9828\n",
      "2019-03-06T16:34:25.263238, step: 335, loss: 0.31258201599121094, acc: 0.8984, auc: 0.9335, precision: 0.88, recall: 0.9429\n",
      "2019-03-06T16:34:25.632220, step: 336, loss: 0.27190953493118286, acc: 0.8984, auc: 0.9552, precision: 0.9429, recall: 0.88\n",
      "2019-03-06T16:34:26.012251, step: 337, loss: 0.2969376742839813, acc: 0.8594, auc: 0.9509, precision: 0.9434, recall: 0.7692\n",
      "2019-03-06T16:34:26.394213, step: 338, loss: 0.2911173403263092, acc: 0.8672, auc: 0.9683, precision: 0.9375, recall: 0.7627\n",
      "2019-03-06T16:34:26.778241, step: 339, loss: 0.18015027046203613, acc: 0.9609, auc: 0.9821, precision: 1.0, recall: 0.9315\n",
      "2019-03-06T16:34:27.159373, step: 340, loss: 0.272580087184906, acc: 0.9219, auc: 0.9464, precision: 0.8704, recall: 0.94\n",
      "2019-03-06T16:34:27.536366, step: 341, loss: 0.2435651570558548, acc: 0.9062, auc: 0.9662, precision: 0.9365, recall: 0.8806\n",
      "2019-03-06T16:34:27.914561, step: 342, loss: 0.18817000091075897, acc: 0.9531, auc: 0.9802, precision: 0.9552, recall: 0.9552\n",
      "2019-03-06T16:34:28.292554, step: 343, loss: 0.236561119556427, acc: 0.9141, auc: 0.9634, precision: 0.9219, recall: 0.9077\n",
      "2019-03-06T16:34:28.672504, step: 344, loss: 0.23550136387348175, acc: 0.9375, auc: 0.9667, precision: 0.9206, recall: 0.9508\n",
      "2019-03-06T16:34:29.046535, step: 345, loss: 0.2176128476858139, acc: 0.9219, auc: 0.979, precision: 0.9062, recall: 0.9355\n",
      "2019-03-06T16:34:29.424523, step: 346, loss: 0.24465438723564148, acc: 0.9062, auc: 0.9695, precision: 0.931, recall: 0.871\n",
      "2019-03-06T16:34:29.801614, step: 347, loss: 0.18666218221187592, acc: 0.9375, auc: 0.9795, precision: 0.9038, recall: 0.94\n",
      "2019-03-06T16:34:30.179843, step: 348, loss: 0.3081151247024536, acc: 0.875, auc: 0.945, precision: 0.9032, recall: 0.8485\n",
      "2019-03-06T16:34:30.593116, step: 349, loss: 0.2681075930595398, acc: 0.9062, auc: 0.9484, precision: 0.931, recall: 0.871\n",
      "2019-03-06T16:34:30.962548, step: 350, loss: 0.25288546085357666, acc: 0.9062, auc: 0.9641, precision: 0.9057, recall: 0.8727\n",
      "2019-03-06T16:34:31.370456, step: 351, loss: 0.22401556372642517, acc: 0.8906, auc: 0.9773, precision: 0.918, recall: 0.8615\n",
      "2019-03-06T16:34:31.750477, step: 352, loss: 0.2931288182735443, acc: 0.875, auc: 0.9595, precision: 0.9231, recall: 0.8451\n",
      "2019-03-06T16:34:32.182294, step: 353, loss: 0.26461249589920044, acc: 0.9062, auc: 0.947, precision: 0.918, recall: 0.8889\n",
      "2019-03-06T16:34:32.603284, step: 354, loss: 0.21638089418411255, acc: 0.9219, auc: 0.9675, precision: 0.9219, recall: 0.9219\n",
      "2019-03-06T16:34:32.986302, step: 355, loss: 0.2898732125759125, acc: 0.8828, auc: 0.9631, precision: 0.8667, recall: 0.9286\n",
      "2019-03-06T16:34:33.367419, step: 356, loss: 0.2669920325279236, acc: 0.8984, auc: 0.9671, precision: 0.8621, recall: 0.9091\n",
      "2019-03-06T16:34:33.746403, step: 357, loss: 0.3343035578727722, acc: 0.8672, auc: 0.9155, precision: 0.875, recall: 0.8305\n",
      "2019-03-06T16:34:34.127356, step: 358, loss: 0.29495513439178467, acc: 0.8984, auc: 0.9473, precision: 0.9273, recall: 0.85\n",
      "2019-03-06T16:34:34.513355, step: 359, loss: 0.3197019398212433, acc: 0.8516, auc: 0.9621, precision: 0.9388, recall: 0.7419\n",
      "2019-03-06T16:34:34.898325, step: 360, loss: 0.23834741115570068, acc: 0.9219, auc: 0.9706, precision: 0.9464, recall: 0.8833\n",
      "2019-03-06T16:34:35.276313, step: 361, loss: 0.2621210217475891, acc: 0.9062, auc: 0.9676, precision: 0.9672, recall: 0.8551\n",
      "2019-03-06T16:34:35.651458, step: 362, loss: 0.20981654524803162, acc: 0.9141, auc: 0.9759, precision: 0.9552, recall: 0.8889\n",
      "2019-03-06T16:34:36.032440, step: 363, loss: 0.2920164465904236, acc: 0.8828, auc: 0.9482, precision: 0.8657, recall: 0.9062\n",
      "2019-03-06T16:34:36.405655, step: 364, loss: 0.27303972840309143, acc: 0.9219, auc: 0.9748, precision: 0.8714, recall: 0.9839\n",
      "2019-03-06T16:34:36.785670, step: 365, loss: 0.21450281143188477, acc: 0.9453, auc: 0.9809, precision: 0.9577, recall: 0.9444\n",
      "2019-03-06T16:34:37.164625, step: 366, loss: 0.29823023080825806, acc: 0.8906, auc: 0.9594, precision: 0.859, recall: 0.9571\n",
      "2019-03-06T16:34:37.562592, step: 367, loss: 0.24039366841316223, acc: 0.9219, auc: 0.9702, precision: 0.9661, recall: 0.8769\n",
      "2019-03-06T16:34:37.949556, step: 368, loss: 0.3330812156200409, acc: 0.875, auc: 0.9365, precision: 0.8852, recall: 0.8571\n",
      "2019-03-06T16:34:38.329510, step: 369, loss: 0.17874157428741455, acc: 0.9375, auc: 0.99, precision: 0.9815, recall: 0.8833\n",
      "2019-03-06T16:34:38.714688, step: 370, loss: 0.2589605748653412, acc: 0.9062, auc: 0.9547, precision: 0.9388, recall: 0.8364\n",
      "2019-03-06T16:34:39.087689, step: 371, loss: 0.25022268295288086, acc: 0.8672, auc: 0.9848, precision: 1.0, recall: 0.7167\n",
      "2019-03-06T16:34:39.473756, step: 372, loss: 0.33192211389541626, acc: 0.8203, auc: 0.9374, precision: 0.9057, recall: 0.7273\n",
      "2019-03-06T16:34:39.849750, step: 373, loss: 0.30549633502960205, acc: 0.8828, auc: 0.9366, precision: 0.9219, recall: 0.8551\n",
      "2019-03-06T16:34:40.227739, step: 374, loss: 0.20591774582862854, acc: 0.9219, auc: 0.9702, precision: 0.9242, recall: 0.9242\n",
      "2019-03-06T16:34:40.613741, step: 375, loss: 0.2671057879924774, acc: 0.9062, auc: 0.9686, precision: 0.8615, recall: 0.9492\n",
      "2019-03-06T16:34:40.991697, step: 376, loss: 0.36968472599983215, acc: 0.8828, auc: 0.9127, precision: 0.8254, recall: 0.9286\n",
      "2019-03-06T16:34:41.382651, step: 377, loss: 0.23825955390930176, acc: 0.9219, auc: 0.9495, precision: 0.9545, recall: 0.9\n",
      "2019-03-06T16:34:41.765869, step: 378, loss: 0.249824658036232, acc: 0.9062, auc: 0.9635, precision: 0.92, recall: 0.92\n",
      "2019-03-06T16:34:42.146109, step: 379, loss: 0.3572142720222473, acc: 0.8516, auc: 0.9209, precision: 0.8438, recall: 0.8571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:34:42.530083, step: 380, loss: 0.2478216290473938, acc: 0.9062, auc: 0.9645, precision: 0.9153, recall: 0.8852\n",
      "2019-03-06T16:34:42.912064, step: 381, loss: 0.2936312258243561, acc: 0.8828, auc: 0.9443, precision: 0.9455, recall: 0.8125\n",
      "2019-03-06T16:34:43.291019, step: 382, loss: 0.2795281410217285, acc: 0.8672, auc: 0.9689, precision: 0.9322, recall: 0.8088\n",
      "2019-03-06T16:34:43.670034, step: 383, loss: 0.2396518588066101, acc: 0.8906, auc: 0.9678, precision: 0.9245, recall: 0.8305\n",
      "2019-03-06T16:34:44.052982, step: 384, loss: 0.34391096234321594, acc: 0.8516, auc: 0.9293, precision: 0.9138, recall: 0.791\n",
      "2019-03-06T16:34:44.431461, step: 385, loss: 0.2761378586292267, acc: 0.875, auc: 0.9449, precision: 0.9104, recall: 0.8592\n",
      "2019-03-06T16:34:44.811592, step: 386, loss: 0.2139039933681488, acc: 0.9297, auc: 0.9816, precision: 0.9054, recall: 0.971\n",
      "2019-03-06T16:34:45.193759, step: 387, loss: 0.2685379683971405, acc: 0.9219, auc: 0.9517, precision: 0.9032, recall: 0.9333\n",
      "2019-03-06T16:34:45.577731, step: 388, loss: 0.2875213027000427, acc: 0.875, auc: 0.9672, precision: 0.8548, recall: 0.8833\n",
      "2019-03-06T16:34:45.954880, step: 389, loss: 0.21587467193603516, acc: 0.9453, auc: 0.9682, precision: 0.9434, recall: 0.9259\n",
      "2019-03-06T16:34:46.323893, step: 390, loss: 0.2395499348640442, acc: 0.9141, auc: 0.9655, precision: 0.9375, recall: 0.8955\n",
      "2019-03-06T16:34:46.698921, step: 391, loss: 0.22893303632736206, acc: 0.8984, auc: 0.9819, precision: 0.98, recall: 0.8033\n",
      "2019-03-06T16:34:47.085855, step: 392, loss: 0.16899485886096954, acc: 0.9453, auc: 0.9828, precision: 0.9615, recall: 0.9091\n",
      "2019-03-06T16:34:47.524852, step: 393, loss: 0.332746684551239, acc: 0.8359, auc: 0.9342, precision: 0.8936, recall: 0.7241\n",
      "2019-03-06T16:34:47.920932, step: 394, loss: 0.23393648862838745, acc: 0.9141, auc: 0.9642, precision: 0.9688, recall: 0.8732\n",
      "2019-03-06T16:34:48.321983, step: 395, loss: 0.22870796918869019, acc: 0.9375, auc: 0.9721, precision: 0.9362, recall: 0.898\n",
      "2019-03-06T16:34:48.707979, step: 396, loss: 0.38423606753349304, acc: 0.8438, auc: 0.9089, precision: 0.8657, recall: 0.8406\n",
      "2019-03-06T16:34:49.210606, step: 397, loss: 0.27180761098861694, acc: 0.8906, auc: 0.9554, precision: 0.8548, recall: 0.9138\n",
      "2019-03-06T16:34:49.605578, step: 398, loss: 0.26893746852874756, acc: 0.8906, auc: 0.9474, precision: 0.9538, recall: 0.8493\n",
      "2019-03-06T16:34:49.989524, step: 399, loss: 0.2853041887283325, acc: 0.9062, auc: 0.9504, precision: 0.8939, recall: 0.9219\n",
      "2019-03-06T16:34:50.371531, step: 400, loss: 0.2642955482006073, acc: 0.9141, auc: 0.9551, precision: 0.9692, recall: 0.875\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T16:35:05.072144, step: 400, loss: 0.3701775456086183, acc: 0.8617820512820512, auc: 0.9245410256410256, precision: 0.8285205128205129, recall: 0.9155948717948715\n",
      "2019-03-06T16:35:05.452099, step: 401, loss: 0.2665356695652008, acc: 0.8828, auc: 0.9609, precision: 0.9167, recall: 0.8462\n",
      "2019-03-06T16:35:05.829736, step: 402, loss: 0.22979362308979034, acc: 0.9219, auc: 0.9683, precision: 0.9153, recall: 0.9153\n",
      "2019-03-06T16:35:06.206117, step: 403, loss: 0.13345083594322205, acc: 0.9375, auc: 0.9968, precision: 1.0, recall: 0.8667\n",
      "2019-03-06T16:35:06.581116, step: 404, loss: 0.1520916074514389, acc: 0.9609, auc: 0.988, precision: 0.9839, recall: 0.9385\n",
      "2019-03-06T16:35:06.966084, step: 405, loss: 0.33598896861076355, acc: 0.8672, auc: 0.9148, precision: 0.8857, recall: 0.8732\n",
      "2019-03-06T16:35:07.347037, step: 406, loss: 0.227483332157135, acc: 0.9375, auc: 0.9554, precision: 0.9032, recall: 0.9655\n",
      "2019-03-06T16:35:07.726182, step: 407, loss: 0.16426636278629303, acc: 0.9297, auc: 0.991, precision: 0.9667, recall: 0.8923\n",
      "2019-03-06T16:35:08.106163, step: 408, loss: 0.25108033418655396, acc: 0.8906, auc: 0.9685, precision: 0.9016, recall: 0.873\n",
      "2019-03-06T16:35:08.477416, step: 409, loss: 0.26717495918273926, acc: 0.8828, auc: 0.9501, precision: 0.9333, recall: 0.8358\n",
      "2019-03-06T16:35:08.855407, step: 410, loss: 0.2366161346435547, acc: 0.9219, auc: 0.9674, precision: 0.9833, recall: 0.8676\n",
      "2019-03-06T16:35:09.228658, step: 411, loss: 0.24179649353027344, acc: 0.8984, auc: 0.9676, precision: 0.9231, recall: 0.8421\n",
      "2019-03-06T16:35:09.614595, step: 412, loss: 0.17373275756835938, acc: 0.9453, auc: 0.9816, precision: 0.9559, recall: 0.942\n",
      "2019-03-06T16:35:10.004564, step: 413, loss: 0.18942776322364807, acc: 0.9219, auc: 0.9803, precision: 0.9455, recall: 0.8814\n",
      "2019-03-06T16:35:10.392514, step: 414, loss: 0.32133007049560547, acc: 0.8672, auc: 0.9312, precision: 0.9286, recall: 0.8442\n",
      "2019-03-06T16:35:10.781474, step: 415, loss: 0.2579411566257477, acc: 0.8906, auc: 0.959, precision: 0.9385, recall: 0.8592\n",
      "2019-03-06T16:35:11.159499, step: 416, loss: 0.2972790598869324, acc: 0.8828, auc: 0.9552, precision: 0.8873, recall: 0.9\n",
      "2019-03-06T16:35:11.534671, step: 417, loss: 0.3035309910774231, acc: 0.8828, auc: 0.9454, precision: 0.875, recall: 0.9333\n",
      "2019-03-06T16:35:11.911664, step: 418, loss: 0.22310474514961243, acc: 0.9375, auc: 0.9656, precision: 0.913, recall: 0.9692\n",
      "2019-03-06T16:35:12.293720, step: 419, loss: 0.16970530152320862, acc: 0.9375, auc: 0.9822, precision: 0.9516, recall: 0.9219\n",
      "2019-03-06T16:35:12.664728, step: 420, loss: 0.28735584020614624, acc: 0.8828, auc: 0.9484, precision: 0.8793, recall: 0.8644\n",
      "2019-03-06T16:35:13.037759, step: 421, loss: 0.2663118839263916, acc: 0.9141, auc: 0.959, precision: 0.9531, recall: 0.8841\n",
      "2019-03-06T16:35:13.408767, step: 422, loss: 0.23635703325271606, acc: 0.9219, auc: 0.9619, precision: 0.9333, recall: 0.9032\n",
      "2019-03-06T16:35:13.780743, step: 423, loss: 0.26221227645874023, acc: 0.9141, auc: 0.9578, precision: 0.9077, recall: 0.9219\n",
      "2019-03-06T16:35:14.145797, step: 424, loss: 0.21384896337985992, acc: 0.9062, auc: 0.9732, precision: 0.9677, recall: 0.8571\n",
      "2019-03-06T16:35:14.521992, step: 425, loss: 0.2481617033481598, acc: 0.9141, auc: 0.9558, precision: 0.9344, recall: 0.8906\n",
      "2019-03-06T16:35:14.901007, step: 426, loss: 0.2535943388938904, acc: 0.9219, auc: 0.9669, precision: 0.9032, recall: 0.9333\n",
      "2019-03-06T16:35:15.280106, step: 427, loss: 0.20323611795902252, acc: 0.9297, auc: 0.9748, precision: 0.9355, recall: 0.9206\n",
      "2019-03-06T16:35:15.663080, step: 428, loss: 0.30809706449508667, acc: 0.8906, auc: 0.9382, precision: 0.9014, recall: 0.9014\n",
      "2019-03-06T16:35:16.053229, step: 429, loss: 0.28569233417510986, acc: 0.8906, auc: 0.9523, precision: 0.9385, recall: 0.8592\n",
      "2019-03-06T16:35:16.444184, step: 430, loss: 0.31022047996520996, acc: 0.9062, auc: 0.9328, precision: 0.8831, recall: 0.9577\n",
      "2019-03-06T16:35:16.841127, step: 431, loss: 0.2219420224428177, acc: 0.8984, auc: 0.9807, precision: 0.9028, recall: 0.9155\n",
      "2019-03-06T16:35:17.220108, step: 432, loss: 0.20686392486095428, acc: 0.9453, auc: 0.9735, precision: 0.9861, recall: 0.9221\n",
      "2019-03-06T16:35:17.591355, step: 433, loss: 0.20284825563430786, acc: 0.9141, auc: 0.9765, precision: 0.9667, recall: 0.8657\n",
      "2019-03-06T16:35:17.967318, step: 434, loss: 0.2813631296157837, acc: 0.9062, auc: 0.9384, precision: 0.8833, recall: 0.9138\n",
      "2019-03-06T16:35:18.353473, step: 435, loss: 0.30821067094802856, acc: 0.875, auc: 0.9527, precision: 0.9333, recall: 0.8235\n",
      "2019-03-06T16:35:18.768363, step: 436, loss: 0.21987615525722504, acc: 0.9219, auc: 0.9643, precision: 0.9, recall: 0.931\n",
      "2019-03-06T16:35:19.198182, step: 437, loss: 0.17395123839378357, acc: 0.9297, auc: 0.9824, precision: 0.9672, recall: 0.8939\n",
      "2019-03-06T16:35:19.584188, step: 438, loss: 0.3266684412956238, acc: 0.8828, auc: 0.9394, precision: 0.8679, recall: 0.8519\n",
      "2019-03-06T16:35:19.971114, step: 439, loss: 0.2680075466632843, acc: 0.9219, auc: 0.9584, precision: 0.8571, recall: 0.9818\n",
      "2019-03-06T16:35:20.357113, step: 440, loss: 0.2304430603981018, acc: 0.9297, auc: 0.9537, precision: 0.8889, recall: 0.9412\n",
      "2019-03-06T16:35:20.742054, step: 441, loss: 0.18562693893909454, acc: 0.9375, auc: 0.977, precision: 0.9815, recall: 0.8833\n",
      "2019-03-06T16:35:21.131014, step: 442, loss: 0.18922963738441467, acc: 0.9297, auc: 0.9849, precision: 0.9306, recall: 0.9437\n",
      "2019-03-06T16:35:21.522966, step: 443, loss: 0.23723968863487244, acc: 0.9062, auc: 0.956, precision: 0.9412, recall: 0.8421\n",
      "2019-03-06T16:35:21.917141, step: 444, loss: 0.18819357454776764, acc: 0.9062, auc: 0.9892, precision: 1.0, recall: 0.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:35:22.302113, step: 445, loss: 0.2682870924472809, acc: 0.9062, auc: 0.9583, precision: 0.9333, recall: 0.875\n",
      "2019-03-06T16:35:22.683064, step: 446, loss: 0.2710874676704407, acc: 0.9141, auc: 0.9681, precision: 0.871, recall: 0.9474\n",
      "2019-03-06T16:35:23.068034, step: 447, loss: 0.2656382918357849, acc: 0.9219, auc: 0.9614, precision: 0.9219, recall: 0.9219\n",
      "2019-03-06T16:35:23.461981, step: 448, loss: 0.1722315400838852, acc: 0.9375, auc: 0.9875, precision: 0.8971, recall: 0.9839\n",
      "2019-03-06T16:35:23.851047, step: 449, loss: 0.3152487576007843, acc: 0.9062, auc: 0.9215, precision: 0.8769, recall: 0.9344\n",
      "2019-03-06T16:35:24.242031, step: 450, loss: 0.26116740703582764, acc: 0.9141, auc: 0.9477, precision: 0.95, recall: 0.8769\n",
      "2019-03-06T16:35:24.615463, step: 451, loss: 0.3238331079483032, acc: 0.8672, auc: 0.936, precision: 0.8636, recall: 0.8769\n",
      "2019-03-06T16:35:25.008441, step: 452, loss: 0.2319168597459793, acc: 0.9219, auc: 0.9651, precision: 0.95, recall: 0.8906\n",
      "2019-03-06T16:35:25.382412, step: 453, loss: 0.31370580196380615, acc: 0.8516, auc: 0.957, precision: 0.9808, recall: 0.7391\n",
      "2019-03-06T16:35:25.777355, step: 454, loss: 0.2862914800643921, acc: 0.8672, auc: 0.9539, precision: 0.9275, recall: 0.8421\n",
      "2019-03-06T16:35:26.154487, step: 455, loss: 0.29801857471466064, acc: 0.8828, auc: 0.9588, precision: 0.8361, recall: 0.9107\n",
      "2019-03-06T16:35:26.539427, step: 456, loss: 0.3736622929573059, acc: 0.8672, auc: 0.9494, precision: 0.7812, recall: 0.9434\n",
      "2019-03-06T16:35:26.928386, step: 457, loss: 0.351656436920166, acc: 0.875, auc: 0.9217, precision: 0.8261, recall: 0.9344\n",
      "2019-03-06T16:35:27.314384, step: 458, loss: 0.2211845964193344, acc: 0.9297, auc: 0.9771, precision: 0.9074, recall: 0.9245\n",
      "2019-03-06T16:35:27.695336, step: 459, loss: 0.2597586512565613, acc: 0.875, auc: 0.9617, precision: 0.9455, recall: 0.8\n",
      "2019-03-06T16:35:28.081328, step: 460, loss: 0.328044593334198, acc: 0.8281, auc: 0.964, precision: 0.9444, recall: 0.7286\n",
      "2019-03-06T16:35:28.468303, step: 461, loss: 0.2536773681640625, acc: 0.8828, auc: 0.9695, precision: 0.9333, recall: 0.7778\n",
      "2019-03-06T16:35:28.855234, step: 462, loss: 0.26255279779434204, acc: 0.8906, auc: 0.9653, precision: 1.0, recall: 0.75\n",
      "2019-03-06T16:35:29.235249, step: 463, loss: 0.18090668320655823, acc: 0.9375, auc: 0.9851, precision: 0.9375, recall: 0.9375\n",
      "2019-03-06T16:35:29.615202, step: 464, loss: 0.29042407870292664, acc: 0.9141, auc: 0.9568, precision: 0.8833, recall: 0.9298\n",
      "2019-03-06T16:35:29.995217, step: 465, loss: 0.2473299354314804, acc: 0.9219, auc: 0.9772, precision: 0.8592, recall: 1.0\n",
      "2019-03-06T16:35:30.372300, step: 466, loss: 0.2522570788860321, acc: 0.8984, auc: 0.9743, precision: 0.8551, recall: 0.9516\n",
      "2019-03-06T16:35:30.752321, step: 467, loss: 0.26847586035728455, acc: 0.8984, auc: 0.9569, precision: 0.9524, recall: 0.8571\n",
      "2019-03-06T16:35:31.140596, step: 468, loss: 0.2762450575828552, acc: 0.8672, auc: 0.9602, precision: 0.8852, recall: 0.8438\n",
      "start training model\n",
      "2019-03-06T16:35:31.534511, step: 469, loss: 0.1895081251859665, acc: 0.9141, auc: 0.9875, precision: 0.9636, recall: 0.8548\n",
      "2019-03-06T16:35:31.911503, step: 470, loss: 0.21132227778434753, acc: 0.9062, auc: 0.9871, precision: 0.9818, recall: 0.8308\n",
      "2019-03-06T16:35:32.285534, step: 471, loss: 0.26413246989250183, acc: 0.9219, auc: 0.9407, precision: 0.9464, recall: 0.8833\n",
      "2019-03-06T16:35:32.671499, step: 472, loss: 0.15027862787246704, acc: 0.9609, auc: 0.9832, precision: 0.9683, recall: 0.9531\n",
      "2019-03-06T16:35:33.048491, step: 473, loss: 0.23514795303344727, acc: 0.9375, auc: 0.9621, precision: 0.8906, recall: 0.9828\n",
      "2019-03-06T16:35:33.422616, step: 474, loss: 0.18443536758422852, acc: 0.9609, auc: 0.9618, precision: 0.9865, recall: 0.9481\n",
      "2019-03-06T16:35:33.812572, step: 475, loss: 0.2351962774991989, acc: 0.9219, auc: 0.9632, precision: 0.9552, recall: 0.9014\n",
      "2019-03-06T16:35:34.202530, step: 476, loss: 0.1561954915523529, acc: 0.9453, auc: 0.9822, precision: 0.9464, recall: 0.9298\n",
      "2019-03-06T16:35:34.593516, step: 477, loss: 0.1150314062833786, acc: 0.9766, auc: 0.9985, precision: 0.9524, recall: 1.0\n",
      "2019-03-06T16:35:34.990453, step: 478, loss: 0.22931696474552155, acc: 0.9297, auc: 0.9532, precision: 0.9296, recall: 0.9429\n",
      "2019-03-06T16:35:35.366418, step: 479, loss: 0.19249258935451508, acc: 0.9375, auc: 0.972, precision: 0.9322, recall: 0.9322\n",
      "2019-03-06T16:35:35.758403, step: 480, loss: 0.20692262053489685, acc: 0.9453, auc: 0.9729, precision: 0.9231, recall: 0.9677\n",
      "2019-03-06T16:35:36.145335, step: 481, loss: 0.15700191259384155, acc: 0.9453, auc: 0.9877, precision: 0.9643, recall: 0.9153\n",
      "2019-03-06T16:35:36.544268, step: 482, loss: 0.14606159925460815, acc: 0.9453, auc: 0.9848, precision: 0.9683, recall: 0.9242\n",
      "2019-03-06T16:35:36.959159, step: 483, loss: 0.12614405155181885, acc: 0.9453, auc: 0.9978, precision: 1.0, recall: 0.8852\n",
      "2019-03-06T16:35:37.356097, step: 484, loss: 0.18341147899627686, acc: 0.9453, auc: 0.9741, precision: 0.9683, recall: 0.9242\n",
      "2019-03-06T16:35:37.747052, step: 485, loss: 0.09207605570554733, acc: 0.9766, auc: 0.9983, precision: 0.9692, recall: 0.9844\n",
      "2019-03-06T16:35:38.128065, step: 486, loss: 0.1397821605205536, acc: 0.9531, auc: 0.991, precision: 0.9833, recall: 0.9219\n",
      "2019-03-06T16:35:38.514032, step: 487, loss: 0.14283862709999084, acc: 0.9766, auc: 0.99, precision: 0.9552, recall: 1.0\n",
      "2019-03-06T16:35:38.894983, step: 488, loss: 0.16113264858722687, acc: 0.9609, auc: 0.9789, precision: 0.9467, recall: 0.9861\n",
      "2019-03-06T16:35:39.293945, step: 489, loss: 0.29156652092933655, acc: 0.8984, auc: 0.9646, precision: 0.8696, recall: 0.9375\n",
      "2019-03-06T16:35:39.682903, step: 490, loss: 0.1567063331604004, acc: 0.9453, auc: 0.989, precision: 0.9524, recall: 0.9375\n",
      "2019-03-06T16:35:40.073831, step: 491, loss: 0.18641194701194763, acc: 0.9297, auc: 0.9866, precision: 0.9701, recall: 0.9028\n",
      "2019-03-06T16:35:40.459799, step: 492, loss: 0.17726005613803864, acc: 0.9453, auc: 0.9844, precision: 0.9848, recall: 0.9155\n",
      "2019-03-06T16:35:40.841778, step: 493, loss: 0.10857555270195007, acc: 0.9609, auc: 0.9958, precision: 0.9818, recall: 0.931\n",
      "2019-03-06T16:35:41.233760, step: 494, loss: 0.22696207463741302, acc: 0.9062, auc: 0.9618, precision: 0.9412, recall: 0.8889\n",
      "2019-03-06T16:35:41.634686, step: 495, loss: 0.12251472473144531, acc: 0.9688, auc: 0.9944, precision: 0.9661, recall: 0.9661\n",
      "2019-03-06T16:35:42.032623, step: 496, loss: 0.1570669263601303, acc: 0.9375, auc: 0.9866, precision: 0.9508, recall: 0.9206\n",
      "2019-03-06T16:35:42.415568, step: 497, loss: 0.2193610817193985, acc: 0.9297, auc: 0.9491, precision: 0.9487, recall: 0.9367\n",
      "2019-03-06T16:35:42.799542, step: 498, loss: 0.2071155309677124, acc: 0.9453, auc: 0.96, precision: 0.9516, recall: 0.9365\n",
      "2019-03-06T16:35:43.195484, step: 499, loss: 0.23468053340911865, acc: 0.9297, auc: 0.9724, precision: 0.8939, recall: 0.9672\n",
      "2019-03-06T16:35:43.584443, step: 500, loss: 0.13886059820652008, acc: 0.9688, auc: 0.9856, precision: 0.9545, recall: 0.9844\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T16:35:58.578349, step: 500, loss: 0.3540314271664008, acc: 0.8611794871794871, auc: 0.9312230769230768, precision: 0.8339487179487178, recall: 0.9061153846153847\n",
      "2019-03-06T16:35:58.963321, step: 501, loss: 0.14308378100395203, acc: 0.9609, auc: 0.9871, precision: 0.9683, recall: 0.9531\n",
      "2019-03-06T16:35:59.350316, step: 502, loss: 0.2435833066701889, acc: 0.9062, auc: 0.9521, precision: 0.9385, recall: 0.8841\n",
      "2019-03-06T16:35:59.746227, step: 503, loss: 0.20697873830795288, acc: 0.9531, auc: 0.9558, precision: 0.9833, recall: 0.9219\n",
      "2019-03-06T16:36:00.146186, step: 504, loss: 0.12560147047042847, acc: 0.9531, auc: 0.9885, precision: 0.9683, recall: 0.9385\n",
      "2019-03-06T16:36:00.546088, step: 505, loss: 0.294284850358963, acc: 0.8594, auc: 0.9711, precision: 1.0, recall: 0.7465\n",
      "2019-03-06T16:36:00.942029, step: 506, loss: 0.11014871299266815, acc: 0.9609, auc: 0.9936, precision: 1.0, recall: 0.918\n",
      "2019-03-06T16:36:01.335976, step: 507, loss: 0.3197413384914398, acc: 0.8828, auc: 0.9301, precision: 0.9038, recall: 0.8246\n",
      "2019-03-06T16:36:01.732941, step: 508, loss: 0.15728050470352173, acc: 0.9531, auc: 0.9868, precision: 0.9559, recall: 0.9559\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:36:02.126861, step: 509, loss: 0.22346560657024384, acc: 0.9375, auc: 0.9665, precision: 0.9206, recall: 0.9508\n",
      "2019-03-06T16:36:02.512857, step: 510, loss: 0.15522661805152893, acc: 0.9688, auc: 0.9892, precision: 0.9516, recall: 0.9833\n",
      "2019-03-06T16:36:02.891815, step: 511, loss: 0.1972580999135971, acc: 0.9531, auc: 0.9744, precision: 0.9107, recall: 0.9808\n",
      "2019-03-06T16:36:03.304748, step: 512, loss: 0.24548745155334473, acc: 0.9219, auc: 0.9526, precision: 0.9194, recall: 0.9194\n",
      "2019-03-06T16:36:03.692702, step: 513, loss: 0.05045139044523239, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9821\n",
      "2019-03-06T16:36:04.072658, step: 514, loss: 0.1852482259273529, acc: 0.9297, auc: 0.9783, precision: 0.9667, recall: 0.8923\n",
      "2019-03-06T16:36:04.448652, step: 515, loss: 0.25017261505126953, acc: 0.8906, auc: 0.9695, precision: 0.98, recall: 0.7903\n",
      "2019-03-06T16:36:04.823681, step: 516, loss: 0.2093181014060974, acc: 0.9297, auc: 0.9702, precision: 0.9841, recall: 0.8857\n",
      "2019-03-06T16:36:05.219621, step: 517, loss: 0.13733845949172974, acc: 0.9297, auc: 0.9861, precision: 0.9483, recall: 0.9016\n",
      "2019-03-06T16:36:05.606589, step: 518, loss: 0.11878842860460281, acc: 0.9766, auc: 0.9939, precision: 0.9831, recall: 0.9667\n",
      "2019-03-06T16:36:05.976711, step: 519, loss: 0.17642734944820404, acc: 0.9531, auc: 0.9761, precision: 0.9016, recall: 1.0\n",
      "2019-03-06T16:36:06.355666, step: 520, loss: 0.2149847149848938, acc: 0.9453, auc: 0.9721, precision: 0.9091, recall: 0.9836\n",
      "2019-03-06T16:36:06.746621, step: 521, loss: 0.21926581859588623, acc: 0.9453, auc: 0.9786, precision: 0.9048, recall: 0.9828\n",
      "2019-03-06T16:36:07.135611, step: 522, loss: 0.1824413537979126, acc: 0.9531, auc: 0.9638, precision: 0.9667, recall: 0.9355\n",
      "2019-03-06T16:36:07.516562, step: 523, loss: 0.26319244503974915, acc: 0.9062, auc: 0.9548, precision: 0.9333, recall: 0.875\n",
      "2019-03-06T16:36:07.904556, step: 524, loss: 0.18497847020626068, acc: 0.9375, auc: 0.9712, precision: 0.9365, recall: 0.9365\n",
      "2019-03-06T16:36:08.289522, step: 525, loss: 0.1908082515001297, acc: 0.9141, auc: 0.9814, precision: 0.9815, recall: 0.8413\n",
      "2019-03-06T16:36:08.667616, step: 526, loss: 0.16872374713420868, acc: 0.9609, auc: 0.9677, precision: 1.0, recall: 0.9194\n",
      "2019-03-06T16:36:09.041586, step: 527, loss: 0.18932709097862244, acc: 0.9219, auc: 0.9761, precision: 0.9828, recall: 0.8636\n",
      "2019-03-06T16:36:09.417799, step: 528, loss: 0.0881684422492981, acc: 0.9844, auc: 0.999, precision: 0.9636, recall: 1.0\n",
      "2019-03-06T16:36:09.794791, step: 529, loss: 0.19633115828037262, acc: 0.9219, auc: 0.9734, precision: 0.9219, recall: 0.9219\n",
      "2019-03-06T16:36:10.185738, step: 530, loss: 0.15360543131828308, acc: 0.9531, auc: 0.9904, precision: 0.9344, recall: 0.9661\n",
      "2019-03-06T16:36:10.572711, step: 531, loss: 0.18304580450057983, acc: 0.9375, auc: 0.9777, precision: 0.9194, recall: 0.95\n",
      "2019-03-06T16:36:10.957652, step: 532, loss: 0.11838003247976303, acc: 0.9766, auc: 0.9822, precision: 0.9701, recall: 0.9848\n",
      "2019-03-06T16:36:11.362602, step: 533, loss: 0.18817764520645142, acc: 0.9375, auc: 0.9777, precision: 0.9722, recall: 0.9211\n",
      "2019-03-06T16:36:11.760538, step: 534, loss: 0.16524729132652283, acc: 0.9531, auc: 0.981, precision: 0.9851, recall: 0.9296\n",
      "2019-03-06T16:36:12.160435, step: 535, loss: 0.16876599192619324, acc: 0.9375, auc: 0.9797, precision: 0.9231, recall: 0.9524\n",
      "2019-03-06T16:36:12.550424, step: 536, loss: 0.27306222915649414, acc: 0.9062, auc: 0.9544, precision: 0.9118, recall: 0.9118\n",
      "2019-03-06T16:36:13.005176, step: 537, loss: 0.19014620780944824, acc: 0.9453, auc: 0.979, precision: 0.9365, recall: 0.9516\n",
      "2019-03-06T16:36:13.394166, step: 538, loss: 0.0786999836564064, acc: 0.9922, auc: 0.999, precision: 1.0, recall: 0.9873\n",
      "2019-03-06T16:36:13.786119, step: 539, loss: 0.14045844972133636, acc: 0.9453, auc: 0.9914, precision: 0.9429, recall: 0.9565\n",
      "2019-03-06T16:36:14.179038, step: 540, loss: 0.15175193548202515, acc: 0.9375, auc: 0.9833, precision: 0.9722, recall: 0.9211\n",
      "2019-03-06T16:36:14.574979, step: 541, loss: 0.15009544789791107, acc: 0.9609, auc: 0.978, precision: 0.9667, recall: 0.9508\n",
      "2019-03-06T16:36:14.968933, step: 542, loss: 0.26750892400741577, acc: 0.9297, auc: 0.9499, precision: 0.9444, recall: 0.9315\n",
      "2019-03-06T16:36:15.431724, step: 543, loss: 0.19817183911800385, acc: 0.9375, auc: 0.9747, precision: 0.918, recall: 0.9492\n",
      "2019-03-06T16:36:15.827660, step: 544, loss: 0.1725078821182251, acc: 0.9531, auc: 0.9763, precision: 0.9508, recall: 0.9508\n",
      "2019-03-06T16:36:16.219610, step: 545, loss: 0.12775377929210663, acc: 0.9688, auc: 0.9895, precision: 0.9844, recall: 0.9545\n",
      "2019-03-06T16:36:16.610536, step: 546, loss: 0.1398855447769165, acc: 0.9766, auc: 0.9794, precision: 0.9859, recall: 0.9722\n",
      "2019-03-06T16:36:16.995506, step: 547, loss: 0.11347149312496185, acc: 0.9453, auc: 0.9951, precision: 0.9365, recall: 0.9516\n",
      "2019-03-06T16:36:17.394468, step: 548, loss: 0.16322021186351776, acc: 0.9375, auc: 0.9854, precision: 0.9804, recall: 0.8772\n",
      "2019-03-06T16:36:17.788386, step: 549, loss: 0.10989703238010406, acc: 0.9453, auc: 0.996, precision: 0.9592, recall: 0.9038\n",
      "2019-03-06T16:36:18.180338, step: 550, loss: 0.18403369188308716, acc: 0.9453, auc: 0.97, precision: 0.9667, recall: 0.9206\n",
      "2019-03-06T16:36:18.594261, step: 551, loss: 0.13222703337669373, acc: 0.9453, auc: 0.9949, precision: 1.0, recall: 0.8906\n",
      "2019-03-06T16:36:18.994197, step: 552, loss: 0.1493327021598816, acc: 0.9609, auc: 0.9824, precision: 1.0, recall: 0.9242\n",
      "2019-03-06T16:36:19.377172, step: 553, loss: 0.19144503772258759, acc: 0.9375, auc: 0.9739, precision: 0.9531, recall: 0.9242\n",
      "2019-03-06T16:36:19.758155, step: 554, loss: 0.18775638937950134, acc: 0.9375, auc: 0.9634, precision: 0.9437, recall: 0.9437\n",
      "2019-03-06T16:36:20.141095, step: 555, loss: 0.15821576118469238, acc: 0.9688, auc: 0.9875, precision: 0.9661, recall: 0.9661\n",
      "2019-03-06T16:36:20.528060, step: 556, loss: 0.24075815081596375, acc: 0.9297, auc: 0.9619, precision: 0.8846, recall: 1.0\n",
      "2019-03-06T16:36:20.915056, step: 557, loss: 0.1418159008026123, acc: 0.9609, auc: 0.9854, precision: 0.9583, recall: 0.9718\n",
      "2019-03-06T16:36:21.287185, step: 558, loss: 0.14383578300476074, acc: 0.9609, auc: 0.9832, precision: 0.9643, recall: 0.9474\n",
      "2019-03-06T16:36:21.684123, step: 559, loss: 0.2025066763162613, acc: 0.9375, auc: 0.9619, precision: 0.9677, recall: 0.9091\n",
      "2019-03-06T16:36:22.074083, step: 560, loss: 0.24823513627052307, acc: 0.8984, auc: 0.9629, precision: 0.9474, recall: 0.8438\n",
      "2019-03-06T16:36:22.457027, step: 561, loss: 0.22890430688858032, acc: 0.9297, auc: 0.9569, precision: 0.9848, recall: 0.8904\n",
      "2019-03-06T16:36:22.851971, step: 562, loss: 0.18115559220314026, acc: 0.9453, auc: 0.9707, precision: 0.9516, recall: 0.9365\n",
      "2019-03-06T16:36:23.250904, step: 563, loss: 0.19193753600120544, acc: 0.9531, auc: 0.9739, precision: 0.9385, recall: 0.9683\n",
      "2019-03-06T16:36:23.634914, step: 564, loss: 0.1711370050907135, acc: 0.9453, auc: 0.9803, precision: 0.9342, recall: 0.9726\n",
      "2019-03-06T16:36:24.021871, step: 565, loss: 0.12111269682645798, acc: 0.9844, auc: 0.9897, precision: 0.9688, recall: 1.0\n",
      "2019-03-06T16:36:24.418813, step: 566, loss: 0.15034863352775574, acc: 0.9531, auc: 0.9831, precision: 0.9692, recall: 0.9403\n",
      "2019-03-06T16:36:24.807773, step: 567, loss: 0.14705544710159302, acc: 0.9766, auc: 0.98, precision: 0.9839, recall: 0.9683\n",
      "2019-03-06T16:36:25.194708, step: 568, loss: 0.19724240899085999, acc: 0.9375, auc: 0.9675, precision: 0.9508, recall: 0.9206\n",
      "2019-03-06T16:36:25.572726, step: 569, loss: 0.17831484973430634, acc: 0.9375, auc: 0.9784, precision: 0.9483, recall: 0.9167\n",
      "2019-03-06T16:36:25.956838, step: 570, loss: 0.1656581461429596, acc: 0.9219, auc: 0.9892, precision: 0.9848, recall: 0.8784\n",
      "2019-03-06T16:36:26.344801, step: 571, loss: 0.18956120312213898, acc: 0.9375, auc: 0.9753, precision: 0.9833, recall: 0.8939\n",
      "2019-03-06T16:36:26.736779, step: 572, loss: 0.18478965759277344, acc: 0.9219, auc: 0.9788, precision: 0.9565, recall: 0.9041\n",
      "2019-03-06T16:36:27.120756, step: 573, loss: 0.2286757230758667, acc: 0.9219, auc: 0.9634, precision: 0.9231, recall: 0.9231\n",
      "2019-03-06T16:36:27.505727, step: 574, loss: 0.28778180480003357, acc: 0.8984, auc: 0.9562, precision: 0.8421, recall: 0.9231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:36:27.892691, step: 575, loss: 0.16737483441829681, acc: 0.9531, auc: 0.9816, precision: 0.9508, recall: 0.9508\n",
      "2019-03-06T16:36:28.291626, step: 576, loss: 0.169311985373497, acc: 0.9297, auc: 0.9898, precision: 0.9437, recall: 0.9306\n",
      "2019-03-06T16:36:28.678591, step: 577, loss: 0.19982048869132996, acc: 0.9141, auc: 0.9757, precision: 0.9138, recall: 0.8983\n",
      "2019-03-06T16:36:29.069515, step: 578, loss: 0.16043375432491302, acc: 0.9453, auc: 0.9777, precision: 0.9412, recall: 0.9552\n",
      "2019-03-06T16:36:29.460469, step: 579, loss: 0.1060013473033905, acc: 0.9766, auc: 0.9954, precision: 0.9677, recall: 0.9836\n",
      "2019-03-06T16:36:29.862394, step: 580, loss: 0.21538220345973969, acc: 0.9219, auc: 0.9674, precision: 0.963, recall: 0.8667\n",
      "2019-03-06T16:36:30.253359, step: 581, loss: 0.1918971836566925, acc: 0.9375, auc: 0.9726, precision: 0.9718, recall: 0.92\n",
      "2019-03-06T16:36:30.645301, step: 582, loss: 0.1578889787197113, acc: 0.9297, auc: 0.9817, precision: 0.9508, recall: 0.9062\n",
      "2019-03-06T16:36:31.052213, step: 583, loss: 0.2080444097518921, acc: 0.9453, auc: 0.9602, precision: 0.92, recall: 0.9388\n",
      "2019-03-06T16:36:31.453141, step: 584, loss: 0.15638582408428192, acc: 0.9453, auc: 0.9778, precision: 0.9524, recall: 0.9375\n",
      "2019-03-06T16:36:31.851108, step: 585, loss: 0.14423441886901855, acc: 0.9531, auc: 0.9805, precision: 0.9672, recall: 0.9365\n",
      "2019-03-06T16:36:32.244056, step: 586, loss: 0.1108270138502121, acc: 0.9766, auc: 0.9914, precision: 0.9836, recall: 0.9677\n",
      "2019-03-06T16:36:32.649984, step: 587, loss: 0.17355799674987793, acc: 0.9453, auc: 0.9851, precision: 0.9265, recall: 0.9692\n",
      "2019-03-06T16:36:33.040900, step: 588, loss: 0.11922654509544373, acc: 0.9609, auc: 0.9929, precision: 0.9677, recall: 0.9524\n",
      "2019-03-06T16:36:33.425052, step: 589, loss: 0.1179971992969513, acc: 0.9453, auc: 0.9956, precision: 0.9322, recall: 0.9483\n",
      "2019-03-06T16:36:33.801047, step: 590, loss: 0.15252259373664856, acc: 0.9453, auc: 0.9839, precision: 0.963, recall: 0.9123\n",
      "2019-03-06T16:36:34.185049, step: 591, loss: 0.21671617031097412, acc: 0.9297, auc: 0.967, precision: 0.9677, recall: 0.8955\n",
      "2019-03-06T16:36:34.575974, step: 592, loss: 0.1437644362449646, acc: 0.9375, auc: 0.9861, precision: 0.9831, recall: 0.8923\n",
      "2019-03-06T16:36:34.958981, step: 593, loss: 0.1996494084596634, acc: 0.9453, auc: 0.9522, precision: 0.9492, recall: 0.9333\n",
      "2019-03-06T16:36:35.358880, step: 594, loss: 0.15904393792152405, acc: 0.9609, auc: 0.9867, precision: 0.9643, recall: 0.9474\n",
      "2019-03-06T16:36:35.747877, step: 595, loss: 0.34322553873062134, acc: 0.8906, auc: 0.9494, precision: 0.8281, recall: 0.9464\n",
      "2019-03-06T16:36:36.134806, step: 596, loss: 0.12027382850646973, acc: 0.9688, auc: 0.9924, precision: 0.9571, recall: 0.9853\n",
      "2019-03-06T16:36:36.528753, step: 597, loss: 0.19147473573684692, acc: 0.9141, auc: 0.9772, precision: 0.9118, recall: 0.9254\n",
      "2019-03-06T16:36:36.901756, step: 598, loss: 0.1767735481262207, acc: 0.9297, auc: 0.9905, precision: 0.9828, recall: 0.8769\n",
      "2019-03-06T16:36:37.304678, step: 599, loss: 0.2914315462112427, acc: 0.8906, auc: 0.9657, precision: 0.9492, recall: 0.8358\n",
      "2019-03-06T16:36:37.683690, step: 600, loss: 0.11607653647661209, acc: 0.9453, auc: 0.9931, precision: 0.9643, recall: 0.9153\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T16:36:52.329281, step: 600, loss: 0.3449581674276254, acc: 0.8687871794871793, auc: 0.934282051282051, precision: 0.8761410256410256, recall: 0.8619897435897437\n",
      "2019-03-06T16:36:52.709230, step: 601, loss: 0.10358235239982605, acc: 0.9531, auc: 0.9944, precision: 0.9667, recall: 0.9355\n",
      "2019-03-06T16:36:53.097222, step: 602, loss: 0.12790055572986603, acc: 0.9609, auc: 0.9839, precision: 0.9559, recall: 0.9701\n",
      "2019-03-06T16:36:53.469198, step: 603, loss: 0.17601987719535828, acc: 0.9453, auc: 0.9783, precision: 0.9538, recall: 0.9394\n",
      "2019-03-06T16:36:53.842202, step: 604, loss: 0.15619021654129028, acc: 0.9453, auc: 0.9812, precision: 0.9242, recall: 0.9683\n",
      "2019-03-06T16:36:54.234182, step: 605, loss: 0.188569575548172, acc: 0.9375, auc: 0.9807, precision: 0.8889, recall: 0.9825\n",
      "2019-03-06T16:36:54.613140, step: 606, loss: 0.1613168865442276, acc: 0.9609, auc: 0.9837, precision: 0.9492, recall: 0.9655\n",
      "2019-03-06T16:36:54.995119, step: 607, loss: 0.18032588064670563, acc: 0.9219, auc: 0.9834, precision: 0.9434, recall: 0.8772\n",
      "2019-03-06T16:36:55.397075, step: 608, loss: 0.21214839816093445, acc: 0.9219, auc: 0.9673, precision: 0.9344, recall: 0.9048\n",
      "2019-03-06T16:36:55.773080, step: 609, loss: 0.13232779502868652, acc: 0.9609, auc: 0.9843, precision: 0.963, recall: 0.9455\n",
      "2019-03-06T16:36:56.160177, step: 610, loss: 0.15903964638710022, acc: 0.9375, auc: 0.9645, precision: 0.9559, recall: 0.9286\n",
      "2019-03-06T16:36:56.541160, step: 611, loss: 0.09052217751741409, acc: 0.9688, auc: 0.9978, precision: 0.9853, recall: 0.9571\n",
      "2019-03-06T16:36:56.926101, step: 612, loss: 0.11238659918308258, acc: 0.9766, auc: 0.9811, precision: 0.9815, recall: 0.9636\n",
      "2019-03-06T16:36:57.308109, step: 613, loss: 0.2501890957355499, acc: 0.9062, auc: 0.9643, precision: 0.9677, recall: 0.8571\n",
      "2019-03-06T16:36:57.683076, step: 614, loss: 0.10997867584228516, acc: 0.9609, auc: 0.9917, precision: 0.9677, recall: 0.9524\n",
      "2019-03-06T16:36:58.062094, step: 615, loss: 0.0914994478225708, acc: 0.9688, auc: 0.9978, precision: 0.9683, recall: 0.9683\n",
      "2019-03-06T16:36:58.440052, step: 616, loss: 0.18118757009506226, acc: 0.9531, auc: 0.9799, precision: 0.9342, recall: 0.9861\n",
      "2019-03-06T16:36:58.838016, step: 617, loss: 0.22730204463005066, acc: 0.9453, auc: 0.9655, precision: 0.9123, recall: 0.963\n",
      "2019-03-06T16:36:59.242905, step: 618, loss: 0.08255090564489365, acc: 0.9844, auc: 0.998, precision: 0.9859, recall: 0.9859\n",
      "2019-03-06T16:36:59.631866, step: 619, loss: 0.11200152337551117, acc: 0.9531, auc: 0.9945, precision: 0.9455, recall: 0.9455\n",
      "2019-03-06T16:37:00.023817, step: 620, loss: 0.2929517924785614, acc: 0.8828, auc: 0.9538, precision: 0.9333, recall: 0.8358\n",
      "2019-03-06T16:37:00.423780, step: 621, loss: 0.25096607208251953, acc: 0.9062, auc: 0.9641, precision: 0.9333, recall: 0.8235\n",
      "2019-03-06T16:37:00.817727, step: 622, loss: 0.2303089201450348, acc: 0.9219, auc: 0.9595, precision: 0.9492, recall: 0.8889\n",
      "2019-03-06T16:37:01.207651, step: 623, loss: 0.1276649832725525, acc: 0.9609, auc: 0.9824, precision: 0.9692, recall: 0.9545\n",
      "2019-03-06T16:37:01.607582, step: 624, loss: 0.11544725298881531, acc: 0.9766, auc: 0.9946, precision: 0.9595, recall: 1.0\n",
      "start training model\n",
      "2019-03-06T16:37:02.010536, step: 625, loss: 0.08411803841590881, acc: 0.9844, auc: 0.9912, precision: 1.0, recall: 0.9688\n",
      "2019-03-06T16:37:02.386532, step: 626, loss: 0.13856732845306396, acc: 0.9609, auc: 0.9873, precision: 0.9516, recall: 0.9672\n",
      "2019-03-06T16:37:02.774491, step: 627, loss: 0.08737868070602417, acc: 0.9844, auc: 0.9931, precision: 0.9831, recall: 0.9831\n",
      "2019-03-06T16:37:03.164419, step: 628, loss: 0.15130756795406342, acc: 0.9531, auc: 0.9837, precision: 0.9333, recall: 0.9655\n",
      "2019-03-06T16:37:03.554377, step: 629, loss: 0.06942395120859146, acc: 0.9844, auc: 0.9976, precision: 0.9692, recall: 1.0\n",
      "2019-03-06T16:37:03.929404, step: 630, loss: 0.1860966980457306, acc: 0.9375, auc: 0.9756, precision: 0.9245, recall: 0.9245\n",
      "2019-03-06T16:37:04.301378, step: 631, loss: 0.08359608799219131, acc: 0.9688, auc: 0.9941, precision: 0.9821, recall: 0.9483\n",
      "2019-03-06T16:37:04.695357, step: 632, loss: 0.19822649657726288, acc: 0.9219, auc: 0.9815, precision: 0.9434, recall: 0.8772\n",
      "2019-03-06T16:37:05.082321, step: 633, loss: 0.0887419655919075, acc: 0.9844, auc: 0.9944, precision: 1.0, recall: 0.9683\n",
      "2019-03-06T16:37:05.468258, step: 634, loss: 0.14742358028888702, acc: 0.9609, auc: 0.9726, precision: 0.9516, recall: 0.9672\n",
      "2019-03-06T16:37:05.851270, step: 635, loss: 0.051026832312345505, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9836\n",
      "2019-03-06T16:37:06.235208, step: 636, loss: 0.09698435664176941, acc: 0.9844, auc: 0.9914, precision: 0.9839, recall: 0.9839\n",
      "2019-03-06T16:37:06.621250, step: 637, loss: 0.136858269572258, acc: 0.9609, auc: 0.9772, precision: 0.9811, recall: 0.9286\n",
      "2019-03-06T16:37:07.022177, step: 638, loss: 0.11641986668109894, acc: 0.9766, auc: 0.9814, precision: 1.0, recall: 0.9516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:37:07.395148, step: 639, loss: 0.13551145792007446, acc: 0.9453, auc: 0.9821, precision: 0.9483, recall: 0.9322\n",
      "2019-03-06T16:37:07.781116, step: 640, loss: 0.11447060108184814, acc: 0.9688, auc: 0.9944, precision: 0.9692, recall: 0.9692\n",
      "2019-03-06T16:37:08.169107, step: 641, loss: 0.10509653389453888, acc: 0.9688, auc: 0.9966, precision: 0.971, recall: 0.971\n",
      "2019-03-06T16:37:08.540118, step: 642, loss: 0.12137935310602188, acc: 0.9688, auc: 0.987, precision: 0.9697, recall: 0.9697\n",
      "2019-03-06T16:37:08.929046, step: 643, loss: 0.10981277376413345, acc: 0.9766, auc: 0.99, precision: 0.9836, recall: 0.9677\n",
      "2019-03-06T16:37:09.315048, step: 644, loss: 0.11244655400514603, acc: 0.9688, auc: 0.9905, precision: 0.9677, recall: 0.9677\n",
      "2019-03-06T16:37:09.695996, step: 645, loss: 0.1127539575099945, acc: 0.9766, auc: 0.9852, precision: 0.9661, recall: 0.9828\n",
      "2019-03-06T16:37:10.080997, step: 646, loss: 0.07975557446479797, acc: 0.9844, auc: 0.9954, precision: 0.9841, recall: 0.9841\n",
      "2019-03-06T16:37:10.470952, step: 647, loss: 0.07078761607408524, acc: 0.9844, auc: 0.9946, precision: 0.9848, recall: 0.9848\n",
      "2019-03-06T16:37:10.861906, step: 648, loss: 0.11073116958141327, acc: 0.9688, auc: 0.9898, precision: 0.9811, recall: 0.9455\n",
      "2019-03-06T16:37:11.241898, step: 649, loss: 0.06544449180364609, acc: 0.9844, auc: 0.9988, precision: 1.0, recall: 0.9672\n",
      "2019-03-06T16:37:11.614895, step: 650, loss: 0.07821895182132721, acc: 0.9766, auc: 0.9958, precision: 0.9821, recall: 0.9649\n",
      "2019-03-06T16:37:12.016820, step: 651, loss: 0.09640537202358246, acc: 0.9688, auc: 0.9907, precision: 0.9851, recall: 0.9565\n",
      "2019-03-06T16:37:12.405779, step: 652, loss: 0.11081096529960632, acc: 0.9688, auc: 0.9916, precision: 0.9452, recall: 1.0\n",
      "2019-03-06T16:37:12.782773, step: 653, loss: 0.1094348281621933, acc: 0.9766, auc: 0.9892, precision: 0.9583, recall: 1.0\n",
      "2019-03-06T16:37:13.160762, step: 654, loss: 0.1135631799697876, acc: 0.9766, auc: 0.9916, precision: 0.9508, recall: 1.0\n",
      "2019-03-06T16:37:13.544733, step: 655, loss: 0.06967844069004059, acc: 0.9922, auc: 0.9936, precision: 1.0, recall: 0.9857\n",
      "2019-03-06T16:37:13.933695, step: 656, loss: 0.12159706652164459, acc: 0.9609, auc: 0.9834, precision: 0.9559, recall: 0.9701\n",
      "2019-03-06T16:37:14.327611, step: 657, loss: 0.11432241648435593, acc: 0.9609, auc: 0.9885, precision: 0.9821, recall: 0.9322\n",
      "2019-03-06T16:37:14.715175, step: 658, loss: 0.10720295459032059, acc: 0.9688, auc: 0.9863, precision: 1.0, recall: 0.9375\n",
      "2019-03-06T16:37:15.114078, step: 659, loss: 0.12334814667701721, acc: 0.9688, auc: 0.9885, precision: 0.9831, recall: 0.9508\n",
      "2019-03-06T16:37:15.495059, step: 660, loss: 0.09086526930332184, acc: 0.9609, auc: 0.9993, precision: 1.0, recall: 0.9275\n",
      "2019-03-06T16:37:15.868378, step: 661, loss: 0.11060741543769836, acc: 0.9766, auc: 0.9924, precision: 1.0, recall: 0.9516\n",
      "2019-03-06T16:37:16.254315, step: 662, loss: 0.0645870715379715, acc: 0.9922, auc: 0.9912, precision: 1.0, recall: 0.9841\n",
      "2019-03-06T16:37:16.652251, step: 663, loss: 0.08996982872486115, acc: 0.9844, auc: 0.9897, precision: 0.9836, recall: 0.9836\n",
      "2019-03-06T16:37:17.051213, step: 664, loss: 0.045759670436382294, acc: 0.9922, auc: 0.9998, precision: 0.9855, recall: 1.0\n",
      "2019-03-06T16:37:17.443137, step: 665, loss: 0.044563308358192444, acc: 0.9844, auc: 0.9995, precision: 0.9851, recall: 0.9851\n",
      "2019-03-06T16:37:17.849051, step: 666, loss: 0.11702603101730347, acc: 0.9609, auc: 0.9946, precision: 0.9265, recall: 1.0\n",
      "2019-03-06T16:37:18.238041, step: 667, loss: 0.08711398392915726, acc: 0.9688, auc: 0.9954, precision: 0.9538, recall: 0.9841\n",
      "2019-03-06T16:37:18.632955, step: 668, loss: 0.23295851051807404, acc: 0.9219, auc: 0.9632, precision: 0.942, recall: 0.9155\n",
      "2019-03-06T16:37:19.019948, step: 669, loss: 0.08520843088626862, acc: 0.9609, auc: 0.9954, precision: 0.9697, recall: 0.9552\n",
      "2019-03-06T16:37:19.407883, step: 670, loss: 0.1346355378627777, acc: 0.9531, auc: 0.9915, precision: 0.9583, recall: 0.92\n",
      "2019-03-06T16:37:19.786900, step: 671, loss: 0.1491667479276657, acc: 0.9453, auc: 0.982, precision: 0.9796, recall: 0.8889\n",
      "2019-03-06T16:37:20.183807, step: 672, loss: 0.07124108076095581, acc: 0.9922, auc: 0.9915, precision: 0.9865, recall: 1.0\n",
      "2019-03-06T16:37:20.568808, step: 673, loss: 0.10063498467206955, acc: 0.9844, auc: 0.9888, precision: 0.9839, recall: 0.9839\n",
      "2019-03-06T16:37:20.970732, step: 674, loss: 0.10171729326248169, acc: 0.9688, auc: 0.9963, precision: 0.9726, recall: 0.9726\n",
      "2019-03-06T16:37:21.358696, step: 675, loss: 0.20014017820358276, acc: 0.9531, auc: 0.9748, precision: 0.942, recall: 0.9701\n",
      "2019-03-06T16:37:21.747626, step: 676, loss: 0.17780014872550964, acc: 0.9609, auc: 0.9696, precision: 0.9595, recall: 0.9726\n",
      "2019-03-06T16:37:22.143596, step: 677, loss: 0.09124600142240524, acc: 0.9688, auc: 0.9978, precision: 0.9595, recall: 0.9861\n",
      "2019-03-06T16:37:22.522586, step: 678, loss: 0.08893297612667084, acc: 0.9688, auc: 0.9973, precision: 1.0, recall: 0.942\n",
      "2019-03-06T16:37:22.901540, step: 679, loss: 0.08516725897789001, acc: 0.9688, auc: 0.9978, precision: 1.0, recall: 0.9365\n",
      "2019-03-06T16:37:23.282522, step: 680, loss: 0.08610397577285767, acc: 0.9844, auc: 0.9973, precision: 0.9828, recall: 0.9828\n",
      "2019-03-06T16:37:23.670484, step: 681, loss: 0.11226217448711395, acc: 0.9609, auc: 0.9901, precision: 0.9636, recall: 0.9464\n",
      "2019-03-06T16:37:24.062465, step: 682, loss: 0.1838887631893158, acc: 0.9375, auc: 0.9748, precision: 0.9455, recall: 0.9123\n",
      "2019-03-06T16:37:24.451881, step: 683, loss: 0.11019756644964218, acc: 0.9688, auc: 0.9839, precision: 0.9804, recall: 0.9434\n",
      "2019-03-06T16:37:24.836852, step: 684, loss: 0.03716721013188362, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-03-06T16:37:25.223847, step: 685, loss: 0.10123328119516373, acc: 0.9766, auc: 0.9906, precision: 0.9726, recall: 0.9861\n",
      "2019-03-06T16:37:25.600841, step: 686, loss: 0.11977103352546692, acc: 0.9609, auc: 0.9877, precision: 0.9706, recall: 0.9565\n",
      "2019-03-06T16:37:25.980954, step: 687, loss: 0.1613146960735321, acc: 0.9453, auc: 0.9842, precision: 0.9437, recall: 0.9571\n",
      "2019-03-06T16:37:26.362933, step: 688, loss: 0.1568983644247055, acc: 0.9609, auc: 0.9809, precision: 0.9429, recall: 0.9851\n",
      "2019-03-06T16:37:26.759904, step: 689, loss: 0.10012947767972946, acc: 0.9688, auc: 0.9887, precision: 0.9655, recall: 0.9655\n",
      "2019-03-06T16:37:27.154860, step: 690, loss: 0.054360926151275635, acc: 0.9922, auc: 0.9993, precision: 0.9831, recall: 1.0\n",
      "2019-03-06T16:37:27.541781, step: 691, loss: 0.0917506068944931, acc: 0.9688, auc: 0.9931, precision: 0.9589, recall: 0.9859\n",
      "2019-03-06T16:37:27.914007, step: 692, loss: 0.07094478607177734, acc: 0.9766, auc: 0.9978, precision: 0.9839, recall: 0.9683\n",
      "2019-03-06T16:37:28.309961, step: 693, loss: 0.135949045419693, acc: 0.9609, auc: 0.9825, precision: 0.9577, recall: 0.9714\n",
      "2019-03-06T16:37:28.686971, step: 694, loss: 0.18279477953910828, acc: 0.9375, auc: 0.9753, precision: 0.9538, recall: 0.9254\n",
      "2019-03-06T16:37:29.075900, step: 695, loss: 0.17875166237354279, acc: 0.9531, auc: 0.959, precision: 0.9818, recall: 0.9153\n",
      "2019-03-06T16:37:29.458011, step: 696, loss: 0.05502304434776306, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9672\n",
      "2019-03-06T16:37:29.830045, step: 697, loss: 0.056603219360113144, acc: 0.9844, auc: 0.9993, precision: 0.9851, recall: 0.9851\n",
      "2019-03-06T16:37:30.212098, step: 698, loss: 0.11810623109340668, acc: 0.9766, auc: 0.9914, precision: 0.9531, recall: 1.0\n",
      "2019-03-06T16:37:30.597101, step: 699, loss: 0.1120716780424118, acc: 0.9766, auc: 0.9856, precision: 0.9531, recall: 1.0\n",
      "2019-03-06T16:37:30.976086, step: 700, loss: 0.10110203921794891, acc: 0.9688, auc: 0.9953, precision: 1.0, recall: 0.942\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T16:37:45.690030, step: 700, loss: 0.34886945440218997, acc: 0.8697897435897437, auc: 0.9337666666666666, precision: 0.8612025641025639, recall: 0.8845461538461539\n",
      "2019-03-06T16:37:46.078992, step: 701, loss: 0.05465651676058769, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9865\n",
      "2019-03-06T16:37:46.467982, step: 702, loss: 0.13418100774288177, acc: 0.9609, auc: 0.9776, precision: 0.9818, recall: 0.931\n",
      "2019-03-06T16:37:46.867912, step: 703, loss: 0.07026852667331696, acc: 0.9766, auc: 0.9941, precision: 0.9841, recall: 0.9688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:37:47.248894, step: 704, loss: 0.1115427240729332, acc: 0.9609, auc: 0.9919, precision: 1.0, recall: 0.9242\n",
      "2019-03-06T16:37:47.617954, step: 705, loss: 0.1410830169916153, acc: 0.9609, auc: 0.9846, precision: 0.9437, recall: 0.9853\n",
      "2019-03-06T16:37:48.000962, step: 706, loss: 0.0952000617980957, acc: 0.9766, auc: 0.986, precision: 0.971, recall: 0.9853\n",
      "2019-03-06T16:37:48.384931, step: 707, loss: 0.1425037533044815, acc: 0.9609, auc: 0.9821, precision: 0.9559, recall: 0.9701\n",
      "2019-03-06T16:37:48.759929, step: 708, loss: 0.1053902879357338, acc: 0.9609, auc: 0.9936, precision: 0.9692, recall: 0.9545\n",
      "2019-03-06T16:37:49.150888, step: 709, loss: 0.08749024569988251, acc: 0.9844, auc: 0.9939, precision: 1.0, recall: 0.9706\n",
      "2019-03-06T16:37:49.554775, step: 710, loss: 0.14649221301078796, acc: 0.9609, auc: 0.9787, precision: 0.9846, recall: 0.9412\n",
      "2019-03-06T16:37:49.940774, step: 711, loss: 0.1304495930671692, acc: 0.9609, auc: 0.9892, precision: 1.0, recall: 0.9359\n",
      "2019-03-06T16:37:50.351675, step: 712, loss: 0.12095163762569427, acc: 0.9688, auc: 0.9961, precision: 0.9231, recall: 1.0\n",
      "2019-03-06T16:37:50.736614, step: 713, loss: 0.08706793189048767, acc: 0.9766, auc: 0.9909, precision: 0.9722, recall: 0.9859\n",
      "2019-03-06T16:37:51.127569, step: 714, loss: 0.09566213935613632, acc: 0.9766, auc: 0.9875, precision: 0.971, recall: 0.9853\n",
      "2019-03-06T16:37:51.508681, step: 715, loss: 0.09656816720962524, acc: 0.9766, auc: 0.9863, precision: 0.9697, recall: 0.9846\n",
      "2019-03-06T16:37:51.886701, step: 716, loss: 0.09925724565982819, acc: 0.9766, auc: 0.9924, precision: 0.9846, recall: 0.9697\n",
      "2019-03-06T16:37:52.261669, step: 717, loss: 0.13973155617713928, acc: 0.9453, auc: 0.9802, precision: 0.9836, recall: 0.9091\n",
      "2019-03-06T16:37:52.641681, step: 718, loss: 0.1461649239063263, acc: 0.9297, auc: 0.9924, precision: 0.9818, recall: 0.871\n",
      "2019-03-06T16:37:53.022660, step: 719, loss: 0.06188994646072388, acc: 0.9844, auc: 0.999, precision: 1.0, recall: 0.9697\n",
      "2019-03-06T16:37:53.428578, step: 720, loss: 0.14921832084655762, acc: 0.9531, auc: 0.9719, precision: 0.9508, recall: 0.9508\n",
      "2019-03-06T16:37:53.825710, step: 721, loss: 0.07329019904136658, acc: 0.9922, auc: 0.9856, precision: 0.9848, recall: 1.0\n",
      "2019-03-06T16:37:54.195723, step: 722, loss: 0.20488086342811584, acc: 0.9453, auc: 0.9736, precision: 0.9412, recall: 0.9552\n",
      "2019-03-06T16:37:54.580691, step: 723, loss: 0.1156630665063858, acc: 0.9688, auc: 0.9869, precision: 0.9718, recall: 0.9718\n",
      "2019-03-06T16:37:54.961673, step: 724, loss: 0.15912660956382751, acc: 0.9609, auc: 0.9943, precision: 0.9167, recall: 1.0\n",
      "2019-03-06T16:37:55.349643, step: 725, loss: 0.15130598843097687, acc: 0.9609, auc: 0.9867, precision: 0.9487, recall: 0.9867\n",
      "2019-03-06T16:37:55.720614, step: 726, loss: 0.17428964376449585, acc: 0.9531, auc: 0.961, precision: 0.9592, recall: 0.9216\n",
      "2019-03-06T16:37:56.119740, step: 727, loss: 0.06817673146724701, acc: 0.9844, auc: 0.9971, precision: 0.9848, recall: 0.9848\n",
      "2019-03-06T16:37:56.490038, step: 728, loss: 0.13984961807727814, acc: 0.9531, auc: 0.9814, precision: 0.9787, recall: 0.902\n",
      "2019-03-06T16:37:56.876006, step: 729, loss: 0.1722109615802765, acc: 0.9297, auc: 0.9946, precision: 0.9811, recall: 0.8667\n",
      "2019-03-06T16:37:57.256958, step: 730, loss: 0.16121786832809448, acc: 0.9297, auc: 0.9759, precision: 0.9565, recall: 0.9167\n",
      "2019-03-06T16:37:57.674860, step: 731, loss: 0.16776302456855774, acc: 0.9219, auc: 0.9824, precision: 0.9375, recall: 0.9091\n",
      "2019-03-06T16:37:58.064831, step: 732, loss: 0.0888010635972023, acc: 0.9766, auc: 0.9958, precision: 0.9672, recall: 0.9833\n",
      "2019-03-06T16:37:58.442817, step: 733, loss: 0.13356363773345947, acc: 0.9609, auc: 0.9809, precision: 0.9429, recall: 0.9851\n",
      "2019-03-06T16:37:58.819808, step: 734, loss: 0.0919947624206543, acc: 0.9844, auc: 0.9863, precision: 0.9692, recall: 1.0\n",
      "2019-03-06T16:37:59.204781, step: 735, loss: 0.13467355072498322, acc: 0.9531, auc: 0.9874, precision: 0.9571, recall: 0.9571\n",
      "2019-03-06T16:37:59.583885, step: 736, loss: 0.15074945986270905, acc: 0.9531, auc: 0.9877, precision: 0.9434, recall: 0.9434\n",
      "2019-03-06T16:37:59.963869, step: 737, loss: 0.1536170095205307, acc: 0.9531, auc: 0.9863, precision: 0.9508, recall: 0.9508\n",
      "2019-03-06T16:38:00.341011, step: 738, loss: 0.13666215538978577, acc: 0.9609, auc: 0.9809, precision: 0.9242, recall: 1.0\n",
      "2019-03-06T16:38:00.707033, step: 739, loss: 0.15053467452526093, acc: 0.9531, auc: 0.9843, precision: 0.9552, recall: 0.9552\n",
      "2019-03-06T16:38:01.089010, step: 740, loss: 0.07512830942869186, acc: 0.9844, auc: 0.9958, precision: 0.9831, recall: 0.9831\n",
      "2019-03-06T16:38:01.470989, step: 741, loss: 0.15301305055618286, acc: 0.9375, auc: 0.9769, precision: 0.9444, recall: 0.9107\n",
      "2019-03-06T16:38:01.861975, step: 742, loss: 0.09022851288318634, acc: 0.9688, auc: 0.9915, precision: 0.9836, recall: 0.9524\n",
      "2019-03-06T16:38:02.249277, step: 743, loss: 0.17357924580574036, acc: 0.9453, auc: 0.9787, precision: 0.9692, recall: 0.9265\n",
      "2019-03-06T16:38:02.630289, step: 744, loss: 0.10077416896820068, acc: 0.9609, auc: 0.9885, precision: 0.9667, recall: 0.9508\n",
      "2019-03-06T16:38:03.014262, step: 745, loss: 0.15373766422271729, acc: 0.9531, auc: 0.9797, precision: 1.0, recall: 0.9118\n",
      "2019-03-06T16:38:03.393249, step: 746, loss: 0.12184065580368042, acc: 0.9609, auc: 0.9872, precision: 0.9571, recall: 0.971\n",
      "2019-03-06T16:38:03.777191, step: 747, loss: 0.07836396992206573, acc: 0.9844, auc: 0.9972, precision: 1.0, recall: 0.9733\n",
      "2019-03-06T16:38:04.166182, step: 748, loss: 0.1410512626171112, acc: 0.9609, auc: 0.9775, precision: 0.9429, recall: 0.9851\n",
      "2019-03-06T16:38:04.540619, step: 749, loss: 0.08366160094738007, acc: 0.9844, auc: 0.9953, precision: 0.9855, recall: 0.9855\n",
      "2019-03-06T16:38:04.916582, step: 750, loss: 0.06399473547935486, acc: 0.9922, auc: 0.9968, precision: 0.9836, recall: 1.0\n",
      "2019-03-06T16:38:05.305573, step: 751, loss: 0.0301535464823246, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:38:05.676784, step: 752, loss: 0.08680073171854019, acc: 0.9844, auc: 0.9925, precision: 0.9863, recall: 0.9863\n",
      "2019-03-06T16:38:06.048790, step: 753, loss: 0.08617331087589264, acc: 0.9688, auc: 0.9928, precision: 0.9815, recall: 0.9464\n",
      "2019-03-06T16:38:06.418800, step: 754, loss: 0.2700670659542084, acc: 0.9297, auc: 0.9548, precision: 0.9545, recall: 0.913\n",
      "2019-03-06T16:38:06.788809, step: 755, loss: 0.18572033941745758, acc: 0.9609, auc: 0.9571, precision: 1.0, recall: 0.9167\n",
      "2019-03-06T16:38:07.165802, step: 756, loss: 0.12879136204719543, acc: 0.9688, auc: 0.9792, precision: 0.9677, recall: 0.9677\n",
      "2019-03-06T16:38:07.548747, step: 757, loss: 0.10259927064180374, acc: 0.9766, auc: 0.9724, precision: 0.9718, recall: 0.9857\n",
      "2019-03-06T16:38:07.926764, step: 758, loss: 0.06333990395069122, acc: 0.9688, auc: 0.9985, precision: 0.9841, recall: 0.9538\n",
      "2019-03-06T16:38:08.305723, step: 759, loss: 0.15892952680587769, acc: 0.9531, auc: 0.9706, precision: 0.9429, recall: 0.9706\n",
      "2019-03-06T16:38:08.689820, step: 760, loss: 0.09143885970115662, acc: 0.9688, auc: 0.9885, precision: 0.9836, recall: 0.9524\n",
      "2019-03-06T16:38:09.074824, step: 761, loss: 0.1264965534210205, acc: 0.9688, auc: 0.9911, precision: 0.9655, recall: 0.9655\n",
      "2019-03-06T16:38:09.448026, step: 762, loss: 0.1400758922100067, acc: 0.9688, auc: 0.9848, precision: 0.9516, recall: 0.9833\n",
      "2019-03-06T16:38:09.825984, step: 763, loss: 0.20296603441238403, acc: 0.9297, auc: 0.9754, precision: 0.9067, recall: 0.9714\n",
      "2019-03-06T16:38:10.203005, step: 764, loss: 0.1138991266489029, acc: 0.9688, auc: 0.9855, precision: 0.9714, recall: 0.9714\n",
      "2019-03-06T16:38:10.590938, step: 765, loss: 0.13520784676074982, acc: 0.9531, auc: 0.9883, precision: 0.9375, recall: 0.9677\n",
      "2019-03-06T16:38:10.977917, step: 766, loss: 0.08091771602630615, acc: 0.9766, auc: 0.9956, precision: 0.9839, recall: 0.9683\n",
      "2019-03-06T16:38:11.360879, step: 767, loss: 0.05403227359056473, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9844\n",
      "2019-03-06T16:38:11.737905, step: 768, loss: 0.16193407773971558, acc: 0.9453, auc: 0.9697, precision: 0.9412, recall: 0.9552\n",
      "2019-03-06T16:38:12.121876, step: 769, loss: 0.19438305497169495, acc: 0.9375, auc: 0.9619, precision: 0.9655, recall: 0.9032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:38:12.508982, step: 770, loss: 0.11007937043905258, acc: 0.9688, auc: 0.9963, precision: 1.0, recall: 0.9385\n",
      "2019-03-06T16:38:12.894951, step: 771, loss: 0.1491137444972992, acc: 0.9688, auc: 0.9755, precision: 0.9667, recall: 0.9667\n",
      "2019-03-06T16:38:13.282884, step: 772, loss: 0.17546840012073517, acc: 0.9531, auc: 0.9673, precision: 0.9649, recall: 0.9322\n",
      "2019-03-06T16:38:13.669880, step: 773, loss: 0.11399781703948975, acc: 0.9688, auc: 0.989, precision: 1.0, recall: 0.9375\n",
      "2019-03-06T16:38:14.053430, step: 774, loss: 0.11922482401132584, acc: 0.9531, auc: 0.9961, precision: 0.9344, recall: 0.9661\n",
      "2019-03-06T16:38:14.435923, step: 775, loss: 0.26109907031059265, acc: 0.9297, auc: 0.9619, precision: 0.8955, recall: 0.9677\n",
      "2019-03-06T16:38:14.815936, step: 776, loss: 0.13741227984428406, acc: 0.9688, auc: 0.9955, precision: 0.931, recall: 1.0\n",
      "2019-03-06T16:38:15.196888, step: 777, loss: 0.11946490406990051, acc: 0.9609, auc: 0.9905, precision: 0.9844, recall: 0.9403\n",
      "2019-03-06T16:38:15.585889, step: 778, loss: 0.06236565113067627, acc: 0.9766, auc: 0.998, precision: 0.9855, recall: 0.9714\n",
      "2019-03-06T16:38:15.962104, step: 779, loss: 0.178217351436615, acc: 0.9297, auc: 0.98, precision: 0.9667, recall: 0.8923\n",
      "2019-03-06T16:38:16.362998, step: 780, loss: 0.09215246886014938, acc: 0.9531, auc: 0.9949, precision: 0.9688, recall: 0.9394\n",
      "start training model\n",
      "2019-03-06T16:38:16.761931, step: 781, loss: 0.12688928842544556, acc: 0.9609, auc: 0.9857, precision: 0.9851, recall: 0.9429\n",
      "2019-03-06T16:38:17.134962, step: 782, loss: 0.07203800976276398, acc: 0.9844, auc: 0.9944, precision: 0.9851, recall: 0.9851\n",
      "2019-03-06T16:38:17.519200, step: 783, loss: 0.06417937576770782, acc: 0.9688, auc: 0.9992, precision: 0.9868, recall: 0.9615\n",
      "2019-03-06T16:38:17.895208, step: 784, loss: 0.04109540581703186, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9815\n",
      "2019-03-06T16:38:18.278184, step: 785, loss: 0.15236611664295197, acc: 0.9453, auc: 0.9812, precision: 0.9516, recall: 0.9365\n",
      "2019-03-06T16:38:18.656170, step: 786, loss: 0.06059815734624863, acc: 0.9922, auc: 0.9958, precision: 0.9831, recall: 1.0\n",
      "2019-03-06T16:38:19.028147, step: 787, loss: 0.1049339547753334, acc: 0.9766, auc: 0.9937, precision: 0.9697, recall: 0.9846\n",
      "2019-03-06T16:38:19.407134, step: 788, loss: 0.0703507512807846, acc: 0.9766, auc: 0.999, precision: 0.9667, recall: 0.9831\n",
      "2019-03-06T16:38:19.783128, step: 789, loss: 0.06212523579597473, acc: 0.9922, auc: 0.9907, precision: 0.9841, recall: 1.0\n",
      "2019-03-06T16:38:20.168131, step: 790, loss: 0.04240822046995163, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9848\n",
      "2019-03-06T16:38:20.557253, step: 791, loss: 0.10338635742664337, acc: 0.9688, auc: 0.9936, precision: 0.9667, recall: 0.9667\n",
      "2019-03-06T16:38:20.938234, step: 792, loss: 0.0806124359369278, acc: 0.9844, auc: 0.9875, precision: 1.0, recall: 0.9677\n",
      "2019-03-06T16:38:21.317381, step: 793, loss: 0.05532456934452057, acc: 0.9844, auc: 0.997, precision: 0.9861, recall: 0.9861\n",
      "2019-03-06T16:38:21.706622, step: 794, loss: 0.028187349438667297, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:38:22.081620, step: 795, loss: 0.1983702927827835, acc: 0.9531, auc: 0.9718, precision: 0.9492, recall: 0.9492\n",
      "2019-03-06T16:38:22.464596, step: 796, loss: 0.09531184285879135, acc: 0.9766, auc: 0.9895, precision: 0.9733, recall: 0.9865\n",
      "2019-03-06T16:38:22.832611, step: 797, loss: 0.104990653693676, acc: 0.9766, auc: 0.9836, precision: 0.9726, recall: 0.9861\n",
      "2019-03-06T16:38:23.201625, step: 798, loss: 0.09137826412916183, acc: 0.9688, auc: 0.9966, precision: 1.0, recall: 0.9385\n",
      "2019-03-06T16:38:23.576884, step: 799, loss: 0.08016957342624664, acc: 0.9766, auc: 0.9941, precision: 0.9701, recall: 0.9848\n",
      "2019-03-06T16:38:23.983767, step: 800, loss: 0.08749845623970032, acc: 0.9844, auc: 0.9844, precision: 0.9825, recall: 0.9825\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T16:38:38.623935, step: 800, loss: 0.40040915172833663, acc: 0.8675923076923078, auc: 0.9252589743589744, precision: 0.8486897435897436, recall: 0.8980051282051281\n",
      "2019-03-06T16:38:39.002922, step: 801, loss: 0.10688839107751846, acc: 0.9766, auc: 0.9886, precision: 0.9492, recall: 1.0\n",
      "2019-03-06T16:38:39.395019, step: 802, loss: 0.054916005581617355, acc: 0.9922, auc: 0.9944, precision: 0.9844, recall: 1.0\n",
      "2019-03-06T16:38:39.778991, step: 803, loss: 0.04934919998049736, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9677\n",
      "2019-03-06T16:38:40.146979, step: 804, loss: 0.07797873765230179, acc: 0.9844, auc: 0.9924, precision: 0.9677, recall: 1.0\n",
      "2019-03-06T16:38:40.519014, step: 805, loss: 0.06656411290168762, acc: 0.9922, auc: 0.99, precision: 0.9818, recall: 1.0\n",
      "2019-03-06T16:38:40.895976, step: 806, loss: 0.022166602313518524, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:38:41.273965, step: 807, loss: 0.064677894115448, acc: 0.9922, auc: 0.9905, precision: 1.0, recall: 0.9839\n",
      "2019-03-06T16:38:41.652980, step: 808, loss: 0.03313995897769928, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:38:42.027949, step: 809, loss: 0.025287717580795288, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:38:42.395996, step: 810, loss: 0.08155830204486847, acc: 0.9688, auc: 0.9934, precision: 0.9836, recall: 0.9524\n",
      "2019-03-06T16:38:42.771999, step: 811, loss: 0.14709220826625824, acc: 0.9531, auc: 0.9842, precision: 0.9595, recall: 0.9595\n",
      "2019-03-06T16:38:43.151972, step: 812, loss: 0.04506595805287361, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9508\n",
      "2019-03-06T16:38:43.538909, step: 813, loss: 0.034281522035598755, acc: 0.9922, auc: 1.0, precision: 0.9828, recall: 1.0\n",
      "2019-03-06T16:38:43.913935, step: 814, loss: 0.05574888736009598, acc: 0.9922, auc: 0.9983, precision: 1.0, recall: 0.9846\n",
      "2019-03-06T16:38:44.298909, step: 815, loss: 0.09996069967746735, acc: 0.9766, auc: 0.9909, precision: 1.0, recall: 0.9605\n",
      "2019-03-06T16:38:44.689627, step: 816, loss: 0.02370498515665531, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:38:45.073631, step: 817, loss: 0.06776472926139832, acc: 0.9688, auc: 0.9993, precision: 0.9571, recall: 0.9853\n",
      "2019-03-06T16:38:45.452709, step: 818, loss: 0.19401739537715912, acc: 0.9531, auc: 0.9701, precision: 0.9365, recall: 0.9672\n",
      "2019-03-06T16:38:45.834689, step: 819, loss: 0.03931817412376404, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9643\n",
      "2019-03-06T16:38:46.206829, step: 820, loss: 0.055601298809051514, acc: 0.9844, auc: 0.9955, precision: 0.9877, recall: 0.9877\n",
      "2019-03-06T16:38:46.587782, step: 821, loss: 0.09704464673995972, acc: 0.9688, auc: 0.9896, precision: 1.0, recall: 0.9437\n",
      "2019-03-06T16:38:46.971780, step: 822, loss: 0.06757519394159317, acc: 0.9844, auc: 0.9973, precision: 0.9846, recall: 0.9846\n",
      "2019-03-06T16:38:47.342762, step: 823, loss: 0.02360437996685505, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:38:47.728083, step: 824, loss: 0.07325901836156845, acc: 0.9844, auc: 0.9961, precision: 1.0, recall: 0.9688\n",
      "2019-03-06T16:38:48.116046, step: 825, loss: 0.02317865751683712, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:38:48.505156, step: 826, loss: 0.1264689713716507, acc: 0.9609, auc: 0.99, precision: 0.9677, recall: 0.9524\n",
      "2019-03-06T16:38:48.893148, step: 827, loss: 0.0661185011267662, acc: 0.9844, auc: 0.9975, precision: 0.9643, recall: 1.0\n",
      "2019-03-06T16:38:49.266151, step: 828, loss: 0.173132985830307, acc: 0.9609, auc: 0.9729, precision: 0.9552, recall: 0.9697\n",
      "2019-03-06T16:38:49.635163, step: 829, loss: 0.10427269339561462, acc: 0.9688, auc: 0.9751, precision: 0.9726, recall: 0.9726\n",
      "2019-03-06T16:38:50.019109, step: 830, loss: 0.05727057904005051, acc: 0.9766, auc: 0.9995, precision: 1.0, recall: 0.9508\n",
      "2019-03-06T16:38:50.397097, step: 831, loss: 0.02976391091942787, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:38:50.781321, step: 832, loss: 0.11030491441488266, acc: 0.9766, auc: 0.986, precision: 0.9853, recall: 0.971\n",
      "2019-03-06T16:38:51.170539, step: 833, loss: 0.019541334360837936, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:38:51.549496, step: 834, loss: 0.0631098672747612, acc: 0.9766, auc: 0.9985, precision: 0.9643, recall: 0.9818\n",
      "2019-03-06T16:38:51.945656, step: 835, loss: 0.07504414767026901, acc: 0.9844, auc: 0.9958, precision: 1.0, recall: 0.9683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:38:52.331624, step: 836, loss: 0.15519197285175323, acc: 0.9688, auc: 0.9606, precision: 0.9583, recall: 0.9857\n",
      "2019-03-06T16:38:52.728562, step: 837, loss: 0.07674767822027206, acc: 0.9844, auc: 0.9928, precision: 0.9655, recall: 1.0\n",
      "2019-03-06T16:38:53.118522, step: 838, loss: 0.06129465252161026, acc: 0.9844, auc: 0.9983, precision: 0.9836, recall: 0.9836\n",
      "2019-03-06T16:38:53.486831, step: 839, loss: 0.07497403770685196, acc: 0.9766, auc: 0.9946, precision: 0.9833, recall: 0.9672\n",
      "2019-03-06T16:38:53.855848, step: 840, loss: 0.07671160995960236, acc: 0.9609, auc: 0.9982, precision: 0.9863, recall: 0.9474\n",
      "2019-03-06T16:38:54.230022, step: 841, loss: 0.05110688880085945, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9692\n",
      "2019-03-06T16:38:54.600466, step: 842, loss: 0.03045986406505108, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9688\n",
      "2019-03-06T16:38:54.979481, step: 843, loss: 0.08996998518705368, acc: 0.9844, auc: 0.9834, precision: 0.9861, recall: 0.9861\n",
      "2019-03-06T16:38:55.347497, step: 844, loss: 0.061361536383628845, acc: 0.9922, auc: 0.9939, precision: 0.9855, recall: 1.0\n",
      "2019-03-06T16:38:55.722466, step: 845, loss: 0.0888703241944313, acc: 0.9766, auc: 0.9941, precision: 0.9714, recall: 0.9855\n",
      "2019-03-06T16:38:56.096495, step: 846, loss: 0.036151688545942307, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:38:56.463515, step: 847, loss: 0.01795361563563347, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:38:56.839479, step: 848, loss: 0.014649970456957817, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:38:57.220734, step: 849, loss: 0.11006674915552139, acc: 0.9766, auc: 0.9901, precision: 0.9655, recall: 0.9825\n",
      "2019-03-06T16:38:57.594730, step: 850, loss: 0.08037525415420532, acc: 0.9766, auc: 0.999, precision: 1.0, recall: 0.9524\n",
      "2019-03-06T16:38:57.973688, step: 851, loss: 0.10948485136032104, acc: 0.9688, auc: 0.9914, precision: 0.9623, recall: 0.9623\n",
      "2019-03-06T16:38:58.351709, step: 852, loss: 0.0346575602889061, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2019-03-06T16:38:58.729683, step: 853, loss: 0.08162569999694824, acc: 0.9844, auc: 0.9868, precision: 0.9688, recall: 1.0\n",
      "2019-03-06T16:38:59.108653, step: 854, loss: 0.08458312600851059, acc: 0.9766, auc: 0.9972, precision: 0.98, recall: 0.9608\n",
      "2019-03-06T16:38:59.484868, step: 855, loss: 0.0828801840543747, acc: 0.9844, auc: 0.9926, precision: 0.9859, recall: 0.9859\n",
      "2019-03-06T16:38:59.860893, step: 856, loss: 0.05737606808543205, acc: 0.9844, auc: 0.9978, precision: 0.9848, recall: 0.9848\n",
      "2019-03-06T16:39:00.232162, step: 857, loss: 0.14106333255767822, acc: 0.9688, auc: 0.9844, precision: 0.9545, recall: 0.9844\n",
      "2019-03-06T16:39:00.613112, step: 858, loss: 0.0936427116394043, acc: 0.9688, auc: 0.9971, precision: 0.9412, recall: 1.0\n",
      "2019-03-06T16:39:00.997086, step: 859, loss: 0.11200043559074402, acc: 0.9766, auc: 0.9787, precision: 0.9836, recall: 0.9677\n",
      "2019-03-06T16:39:01.389066, step: 860, loss: 0.023828744888305664, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:39:01.760076, step: 861, loss: 0.05645471811294556, acc: 0.9766, auc: 0.9976, precision: 0.9846, recall: 0.9697\n",
      "2019-03-06T16:39:02.139063, step: 862, loss: 0.1403352916240692, acc: 0.9453, auc: 0.9789, precision: 0.9692, recall: 0.9265\n",
      "2019-03-06T16:39:02.527322, step: 863, loss: 0.0934460312128067, acc: 0.9766, auc: 0.9873, precision: 1.0, recall: 0.95\n",
      "2019-03-06T16:39:02.914257, step: 864, loss: 0.06582871079444885, acc: 0.9844, auc: 0.9943, precision: 0.9821, recall: 0.9821\n",
      "2019-03-06T16:39:03.301250, step: 865, loss: 0.04655015468597412, acc: 0.9844, auc: 0.999, precision: 0.9872, recall: 0.9872\n",
      "2019-03-06T16:39:03.679241, step: 866, loss: 0.09306605160236359, acc: 0.9844, auc: 0.9939, precision: 1.0, recall: 0.9706\n",
      "2019-03-06T16:39:04.062216, step: 867, loss: 0.05299556255340576, acc: 0.9844, auc: 0.999, precision: 0.9608, recall: 1.0\n",
      "2019-03-06T16:39:04.433820, step: 868, loss: 0.13998357951641083, acc: 0.9609, auc: 0.9869, precision: 0.9333, recall: 0.9825\n",
      "2019-03-06T16:39:04.830761, step: 869, loss: 0.14401499927043915, acc: 0.9609, auc: 0.9774, precision: 0.9589, recall: 0.9722\n",
      "2019-03-06T16:39:05.218728, step: 870, loss: 0.05285627767443657, acc: 0.9922, auc: 0.9998, precision: 0.9844, recall: 1.0\n",
      "2019-03-06T16:39:05.593989, step: 871, loss: 0.10038593411445618, acc: 0.9688, auc: 0.9916, precision: 0.9714, recall: 0.9714\n",
      "2019-03-06T16:39:05.968986, step: 872, loss: 0.10451795905828476, acc: 0.9688, auc: 0.9888, precision: 0.9688, recall: 0.9688\n",
      "2019-03-06T16:39:06.350114, step: 873, loss: 0.18793630599975586, acc: 0.9453, auc: 0.9738, precision: 0.9167, recall: 0.9649\n",
      "2019-03-06T16:39:06.727135, step: 874, loss: 0.08110596984624863, acc: 0.9766, auc: 0.9971, precision: 0.9846, recall: 0.9697\n",
      "2019-03-06T16:39:07.096119, step: 875, loss: 0.08338885009288788, acc: 0.9609, auc: 1.0, precision: 1.0, recall: 0.918\n",
      "2019-03-06T16:39:07.481117, step: 876, loss: 0.07199767976999283, acc: 0.9688, auc: 0.9907, precision: 0.9836, recall: 0.9524\n",
      "2019-03-06T16:39:07.859111, step: 877, loss: 0.1069573312997818, acc: 0.9766, auc: 0.978, precision: 0.9833, recall: 0.9672\n",
      "2019-03-06T16:39:08.241058, step: 878, loss: 0.042189229279756546, acc: 0.9922, auc: 0.9997, precision: 0.9875, recall: 1.0\n",
      "2019-03-06T16:39:08.648107, step: 879, loss: 0.04519781470298767, acc: 0.9922, auc: 0.9998, precision: 0.9844, recall: 1.0\n",
      "2019-03-06T16:39:09.032080, step: 880, loss: 0.17466537654399872, acc: 0.9531, auc: 0.969, precision: 0.9344, recall: 0.9661\n",
      "2019-03-06T16:39:09.421150, step: 881, loss: 0.157385915517807, acc: 0.9688, auc: 0.9736, precision: 0.9714, recall: 0.9714\n",
      "2019-03-06T16:39:09.796119, step: 882, loss: 0.07941297441720963, acc: 0.9844, auc: 0.9937, precision: 0.9846, recall: 0.9846\n",
      "2019-03-06T16:39:10.166161, step: 883, loss: 0.13376671075820923, acc: 0.9688, auc: 0.9824, precision: 0.9545, recall: 0.9844\n",
      "2019-03-06T16:39:10.554091, step: 884, loss: 0.11538692563772202, acc: 0.9688, auc: 0.9946, precision: 0.9853, recall: 0.9571\n",
      "2019-03-06T16:39:10.960014, step: 885, loss: 0.18051475286483765, acc: 0.9453, auc: 0.9748, precision: 0.9841, recall: 0.9118\n",
      "2019-03-06T16:39:11.336001, step: 886, loss: 0.07415838539600372, acc: 0.9609, auc: 0.9983, precision: 0.9825, recall: 0.9333\n",
      "2019-03-06T16:39:11.719205, step: 887, loss: 0.07908202707767487, acc: 0.9688, auc: 0.9895, precision: 1.0, recall: 0.9394\n",
      "2019-03-06T16:39:12.108160, step: 888, loss: 0.06428571045398712, acc: 0.9922, auc: 0.9936, precision: 1.0, recall: 0.9831\n",
      "2019-03-06T16:39:12.498215, step: 889, loss: 0.05321619659662247, acc: 0.9844, auc: 0.9993, precision: 0.9851, recall: 0.9851\n",
      "2019-03-06T16:39:12.876206, step: 890, loss: 0.02653036080300808, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-03-06T16:39:13.255191, step: 891, loss: 0.08731494843959808, acc: 0.9688, auc: 0.9882, precision: 0.9464, recall: 0.9815\n",
      "2019-03-06T16:39:13.639197, step: 892, loss: 0.06734161823987961, acc: 0.9766, auc: 0.9976, precision: 1.0, recall: 0.9516\n",
      "2019-03-06T16:39:14.022172, step: 893, loss: 0.14879289269447327, acc: 0.9688, auc: 0.9721, precision: 0.9667, recall: 0.9667\n",
      "2019-03-06T16:39:14.421628, step: 894, loss: 0.161380797624588, acc: 0.9531, auc: 0.9784, precision: 0.9565, recall: 0.9565\n",
      "2019-03-06T16:39:14.807828, step: 895, loss: 0.16115011274814606, acc: 0.9531, auc: 0.9802, precision: 0.9595, recall: 0.9595\n",
      "2019-03-06T16:39:15.203948, step: 896, loss: 0.049849703907966614, acc: 0.9922, auc: 0.999, precision: 1.0, recall: 0.9868\n",
      "2019-03-06T16:39:15.590882, step: 897, loss: 0.043159231543540955, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:39:15.994963, step: 898, loss: 0.07251937687397003, acc: 0.9844, auc: 0.9953, precision: 0.9831, recall: 0.9831\n",
      "2019-03-06T16:39:16.375974, step: 899, loss: 0.07842805981636047, acc: 0.9766, auc: 0.9968, precision: 1.0, recall: 0.9464\n",
      "2019-03-06T16:39:16.753965, step: 900, loss: 0.05916418135166168, acc: 0.9844, auc: 0.9985, precision: 0.9839, recall: 0.9839\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T16:39:31.453822, step: 900, loss: 0.40336213127160686, acc: 0.8619871794871796, auc: 0.9302974358974359, precision: 0.8502179487179486, recall: 0.8814666666666664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:39:31.834835, step: 901, loss: 0.11240370571613312, acc: 0.9688, auc: 0.9931, precision: 0.9846, recall: 0.9552\n",
      "2019-03-06T16:39:32.245976, step: 902, loss: 0.033245377242565155, acc: 0.9922, auc: 1.0, precision: 0.9859, recall: 1.0\n",
      "2019-03-06T16:39:32.627450, step: 903, loss: 0.11102414131164551, acc: 0.9688, auc: 0.9931, precision: 0.9706, recall: 0.9706\n",
      "2019-03-06T16:39:33.008765, step: 904, loss: 0.047839920967817307, acc: 0.9844, auc: 0.9993, precision: 0.9831, recall: 0.9831\n",
      "2019-03-06T16:39:33.384756, step: 905, loss: 0.057823728770017624, acc: 0.9922, auc: 0.9901, precision: 0.9828, recall: 1.0\n",
      "2019-03-06T16:39:33.772748, step: 906, loss: 0.08367849886417389, acc: 0.9844, auc: 0.9866, precision: 0.9846, recall: 0.9846\n",
      "2019-03-06T16:39:34.163674, step: 907, loss: 0.029925484210252762, acc: 0.9922, auc: 1.0, precision: 0.9861, recall: 1.0\n",
      "2019-03-06T16:39:34.564106, step: 908, loss: 0.09407505393028259, acc: 0.9844, auc: 0.9785, precision: 0.9839, recall: 0.9839\n",
      "2019-03-06T16:39:34.951042, step: 909, loss: 0.022347010672092438, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:39:35.329032, step: 910, loss: 0.10356906801462173, acc: 0.9609, auc: 0.9924, precision: 0.9667, recall: 0.9508\n",
      "2019-03-06T16:39:35.716190, step: 911, loss: 0.1233665943145752, acc: 0.9766, auc: 0.9873, precision: 0.9701, recall: 0.9848\n",
      "2019-03-06T16:39:36.099322, step: 912, loss: 0.0670880377292633, acc: 0.9922, auc: 0.9878, precision: 1.0, recall: 0.9861\n",
      "2019-03-06T16:39:36.486256, step: 913, loss: 0.1492728292942047, acc: 0.9688, auc: 0.978, precision: 0.9688, recall: 0.9688\n",
      "2019-03-06T16:39:36.888210, step: 914, loss: 0.13892097771167755, acc: 0.9609, auc: 0.9816, precision: 0.9636, recall: 0.9464\n",
      "2019-03-06T16:39:37.273183, step: 915, loss: 0.07161261886358261, acc: 0.9844, auc: 0.989, precision: 0.9836, recall: 0.9836\n",
      "2019-03-06T16:39:37.657125, step: 916, loss: 0.07824894040822983, acc: 0.9766, auc: 0.9978, precision: 1.0, recall: 0.9589\n",
      "2019-03-06T16:39:38.039132, step: 917, loss: 0.08308401703834534, acc: 0.9844, auc: 0.9889, precision: 0.9828, recall: 0.9828\n",
      "2019-03-06T16:39:38.419118, step: 918, loss: 0.02275417558848858, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:39:38.791263, step: 919, loss: 0.07838763296604156, acc: 0.9844, auc: 0.9836, precision: 0.9853, recall: 0.9853\n",
      "2019-03-06T16:39:39.172249, step: 920, loss: 0.16076262295246124, acc: 0.9453, auc: 0.9795, precision: 0.9385, recall: 0.9531\n",
      "2019-03-06T16:39:39.559212, step: 921, loss: 0.10961481928825378, acc: 0.9609, auc: 0.9933, precision: 0.9643, recall: 0.9474\n",
      "2019-03-06T16:39:39.941192, step: 922, loss: 0.11625204235315323, acc: 0.9609, auc: 0.9931, precision: 0.9714, recall: 0.9577\n",
      "2019-03-06T16:39:40.330150, step: 923, loss: 0.08680304139852524, acc: 0.9844, auc: 0.9958, precision: 0.9836, recall: 0.9836\n",
      "2019-03-06T16:39:40.725064, step: 924, loss: 0.08893626928329468, acc: 0.9844, auc: 0.988, precision: 0.9844, recall: 0.9844\n",
      "2019-03-06T16:39:41.111062, step: 925, loss: 0.1620287001132965, acc: 0.9609, auc: 0.9794, precision: 0.9846, recall: 0.9412\n",
      "2019-03-06T16:39:41.496217, step: 926, loss: 0.11034247279167175, acc: 0.9688, auc: 0.9916, precision: 0.9615, recall: 0.9615\n",
      "2019-03-06T16:39:41.900170, step: 927, loss: 0.16656842827796936, acc: 0.9531, auc: 0.9841, precision: 0.9623, recall: 0.9273\n",
      "2019-03-06T16:39:42.282297, step: 928, loss: 0.13221782445907593, acc: 0.9453, auc: 0.9836, precision: 0.9615, recall: 0.9091\n",
      "2019-03-06T16:39:42.677211, step: 929, loss: 0.1387050747871399, acc: 0.9609, auc: 0.9824, precision: 0.9672, recall: 0.9516\n",
      "2019-03-06T16:39:43.061184, step: 930, loss: 0.03518243134021759, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9821\n",
      "2019-03-06T16:39:43.448149, step: 931, loss: 0.016544213518500328, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:39:43.834146, step: 932, loss: 0.10417552292346954, acc: 0.9766, auc: 0.9904, precision: 0.9672, recall: 0.9833\n",
      "2019-03-06T16:39:44.209143, step: 933, loss: 0.0780409425497055, acc: 0.9766, auc: 0.9984, precision: 0.9574, recall: 0.9783\n",
      "2019-03-06T16:39:44.586568, step: 934, loss: 0.06205172836780548, acc: 0.9844, auc: 0.9992, precision: 0.9615, recall: 1.0\n",
      "2019-03-06T16:39:44.975528, step: 935, loss: 0.07322660088539124, acc: 0.9766, auc: 0.9939, precision: 0.9516, recall: 1.0\n",
      "2019-03-06T16:39:45.346671, step: 936, loss: 0.10319091379642487, acc: 0.9766, auc: 0.9831, precision: 0.9726, recall: 0.9861\n",
      "start training model\n",
      "2019-03-06T16:39:45.747571, step: 937, loss: 0.0975717306137085, acc: 0.9688, auc: 0.9904, precision: 0.971, recall: 0.971\n",
      "2019-03-06T16:39:46.164822, step: 938, loss: 0.014256616123020649, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:39:46.554814, step: 939, loss: 0.060323480516672134, acc: 0.9844, auc: 0.989, precision: 1.0, recall: 0.9701\n",
      "2019-03-06T16:39:46.929778, step: 940, loss: 0.03214848041534424, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9853\n",
      "2019-03-06T16:39:47.304795, step: 941, loss: 0.0848829448223114, acc: 0.9844, auc: 0.9856, precision: 0.9839, recall: 0.9839\n",
      "2019-03-06T16:39:47.693834, step: 942, loss: 0.03410886973142624, acc: 0.9922, auc: 1.0, precision: 0.9831, recall: 1.0\n",
      "2019-03-06T16:39:48.082822, step: 943, loss: 0.07981279492378235, acc: 0.9844, auc: 0.9901, precision: 0.9861, recall: 0.9861\n",
      "2019-03-06T16:39:48.455975, step: 944, loss: 0.10673046112060547, acc: 0.9609, auc: 0.9834, precision: 0.9552, recall: 0.9697\n",
      "2019-03-06T16:39:48.835949, step: 945, loss: 0.11636677384376526, acc: 0.9688, auc: 0.9919, precision: 0.9545, recall: 0.9844\n",
      "2019-03-06T16:39:49.224877, step: 946, loss: 0.016383638605475426, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:39:49.604862, step: 947, loss: 0.019661061465740204, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:39:49.986872, step: 948, loss: 0.014108173549175262, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:39:50.359843, step: 949, loss: 0.04333300516009331, acc: 0.9922, auc: 0.9982, precision: 0.9818, recall: 1.0\n",
      "2019-03-06T16:39:50.730976, step: 950, loss: 0.034788504242897034, acc: 0.9922, auc: 0.9993, precision: 0.9828, recall: 1.0\n",
      "2019-03-06T16:39:51.096033, step: 951, loss: 0.15483804047107697, acc: 0.9609, auc: 0.9837, precision: 0.9298, recall: 0.9815\n",
      "2019-03-06T16:39:51.474213, step: 952, loss: 0.05486787110567093, acc: 0.9922, auc: 0.9927, precision: 1.0, recall: 0.9846\n",
      "2019-03-06T16:39:51.843406, step: 953, loss: 0.05433103069663048, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9661\n",
      "2019-03-06T16:39:52.222423, step: 954, loss: 0.1551346480846405, acc: 0.9297, auc: 0.9839, precision: 0.9815, recall: 0.8689\n",
      "2019-03-06T16:39:52.612383, step: 955, loss: 0.04858216270804405, acc: 0.9766, auc: 0.9995, precision: 0.9836, recall: 0.9677\n",
      "2019-03-06T16:39:53.003305, step: 956, loss: 0.05334193632006645, acc: 0.9922, auc: 0.9885, precision: 1.0, recall: 0.9844\n",
      "2019-03-06T16:39:53.391297, step: 957, loss: 0.03138211369514465, acc: 0.9922, auc: 1.0, precision: 0.9833, recall: 1.0\n",
      "2019-03-06T16:39:53.775271, step: 958, loss: 0.02986491657793522, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:39:54.150237, step: 959, loss: 0.11511334031820297, acc: 0.9609, auc: 0.9912, precision: 0.9365, recall: 0.9833\n",
      "2019-03-06T16:39:54.523736, step: 960, loss: 0.16326217353343964, acc: 0.9609, auc: 0.9784, precision: 0.9107, recall: 1.0\n",
      "2019-03-06T16:39:54.918681, step: 961, loss: 0.10766622424125671, acc: 0.9766, auc: 0.9853, precision: 0.9565, recall: 1.0\n",
      "2019-03-06T16:39:55.309636, step: 962, loss: 0.02631448395550251, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9848\n",
      "2019-03-06T16:39:55.679647, step: 963, loss: 0.029561735689640045, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-03-06T16:39:56.057637, step: 964, loss: 0.024759598076343536, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:39:56.447599, step: 965, loss: 0.06835119426250458, acc: 0.9766, auc: 0.9983, precision: 0.9828, recall: 0.9661\n",
      "2019-03-06T16:39:56.852480, step: 966, loss: 0.014658007770776749, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:39:57.246428, step: 967, loss: 0.06577257812023163, acc: 0.9766, auc: 0.9921, precision: 0.9818, recall: 0.9643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:39:57.637412, step: 968, loss: 0.09527981281280518, acc: 0.9844, auc: 0.9875, precision: 1.0, recall: 0.9683\n",
      "2019-03-06T16:39:58.025346, step: 969, loss: 0.08192162215709686, acc: 0.9766, auc: 0.9919, precision: 1.0, recall: 0.9516\n",
      "2019-03-06T16:39:58.429264, step: 970, loss: 0.07113605737686157, acc: 0.9766, auc: 0.998, precision: 0.9828, recall: 0.9661\n",
      "2019-03-06T16:39:58.820218, step: 971, loss: 0.09679391980171204, acc: 0.9766, auc: 0.9841, precision: 0.9565, recall: 1.0\n",
      "2019-03-06T16:39:59.217157, step: 972, loss: 0.027989640831947327, acc: 0.9922, auc: 0.9998, precision: 0.9855, recall: 1.0\n",
      "2019-03-06T16:39:59.607116, step: 973, loss: 0.015245376154780388, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:40:00.009040, step: 974, loss: 0.07407805323600769, acc: 0.9766, auc: 0.9963, precision: 0.9846, recall: 0.9697\n",
      "2019-03-06T16:40:00.402987, step: 975, loss: 0.013496222905814648, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:40:00.792944, step: 976, loss: 0.07555222511291504, acc: 0.9766, auc: 0.9968, precision: 0.9508, recall: 1.0\n",
      "2019-03-06T16:40:01.183898, step: 977, loss: 0.021837806329131126, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:40:01.574880, step: 978, loss: 0.02146465703845024, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:40:01.956831, step: 979, loss: 0.05571809411048889, acc: 0.9922, auc: 0.9915, precision: 1.0, recall: 0.9844\n",
      "2019-03-06T16:40:02.355793, step: 980, loss: 0.08300046622753143, acc: 0.9766, auc: 0.99, precision: 0.9815, recall: 0.9636\n",
      "2019-03-06T16:40:02.741761, step: 981, loss: 0.03954637050628662, acc: 0.9844, auc: 0.9993, precision: 0.9848, recall: 0.9848\n",
      "2019-03-06T16:40:03.132687, step: 982, loss: 0.06356759369373322, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2019-03-06T16:40:03.528628, step: 983, loss: 0.14456087350845337, acc: 0.9688, auc: 0.9813, precision: 0.9655, recall: 0.9655\n",
      "2019-03-06T16:40:03.924595, step: 984, loss: 0.03341086581349373, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-03-06T16:40:04.309571, step: 985, loss: 0.04637758806347847, acc: 0.9922, auc: 0.997, precision: 1.0, recall: 0.9818\n",
      "2019-03-06T16:40:04.702490, step: 986, loss: 0.03635384142398834, acc: 0.9922, auc: 1.0, precision: 0.9831, recall: 1.0\n",
      "2019-03-06T16:40:05.080513, step: 987, loss: 0.017358966171741486, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:40:05.495370, step: 988, loss: 0.02467190846800804, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:40:05.892337, step: 989, loss: 0.01789788156747818, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:40:06.306202, step: 990, loss: 0.0866192877292633, acc: 0.9844, auc: 0.9871, precision: 0.9841, recall: 0.9841\n",
      "2019-03-06T16:40:06.706132, step: 991, loss: 0.13020634651184082, acc: 0.9766, auc: 0.9787, precision: 0.9577, recall: 1.0\n",
      "2019-03-06T16:40:07.150972, step: 992, loss: 0.055128954350948334, acc: 0.9922, auc: 0.9946, precision: 1.0, recall: 0.9853\n",
      "2019-03-06T16:40:07.532921, step: 993, loss: 0.019234754145145416, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:40:07.913934, step: 994, loss: 0.06822346150875092, acc: 0.9844, auc: 0.9973, precision: 0.9836, recall: 0.9836\n",
      "2019-03-06T16:40:08.304858, step: 995, loss: 0.09179888665676117, acc: 0.9766, auc: 0.9887, precision: 0.9855, recall: 0.9714\n",
      "2019-03-06T16:40:08.711769, step: 996, loss: 0.03959905728697777, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9844\n",
      "2019-03-06T16:40:09.086766, step: 997, loss: 0.06774826347827911, acc: 0.9766, auc: 0.9875, precision: 0.9836, recall: 0.9677\n",
      "2019-03-06T16:40:09.477720, step: 998, loss: 0.04266919195652008, acc: 0.9922, auc: 0.9983, precision: 1.0, recall: 0.9855\n",
      "2019-03-06T16:40:09.866711, step: 999, loss: 0.12084950506687164, acc: 0.9766, auc: 0.9819, precision: 0.9706, recall: 0.9851\n",
      "2019-03-06T16:40:10.245700, step: 1000, loss: 0.04785147309303284, acc: 0.9922, auc: 0.9932, precision: 0.9848, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T16:40:25.388334, step: 1000, loss: 0.5018891027340522, acc: 0.8537692307692308, auc: 0.9193205128205124, precision: 0.8183846153846153, recall: 0.9106564102564104\n",
      "2019-03-06T16:40:25.771308, step: 1001, loss: 0.07677195221185684, acc: 0.9844, auc: 0.9909, precision: 0.9831, recall: 0.9831\n",
      "2019-03-06T16:40:26.158273, step: 1002, loss: 0.022265775129199028, acc: 0.9922, auc: 1.0, precision: 0.9859, recall: 1.0\n",
      "2019-03-06T16:40:26.610091, step: 1003, loss: 0.016402967274188995, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:40:27.026921, step: 1004, loss: 0.198465958237648, acc: 0.9531, auc: 0.9853, precision: 0.9692, recall: 0.9403\n",
      "2019-03-06T16:40:27.491678, step: 1005, loss: 0.03278915956616402, acc: 0.9922, auc: 0.9997, precision: 0.98, recall: 1.0\n",
      "2019-03-06T16:40:27.877677, step: 1006, loss: 0.04353930428624153, acc: 0.9844, auc: 0.999, precision: 0.9828, recall: 0.9828\n",
      "2019-03-06T16:40:28.262617, step: 1007, loss: 0.1279762089252472, acc: 0.9688, auc: 0.9869, precision: 1.0, recall: 0.9429\n",
      "2019-03-06T16:40:28.655597, step: 1008, loss: 0.09562163054943085, acc: 0.9766, auc: 0.981, precision: 0.9747, recall: 0.9872\n",
      "2019-03-06T16:40:29.055528, step: 1009, loss: 0.08327305316925049, acc: 0.9844, auc: 0.9899, precision: 0.9825, recall: 0.9825\n",
      "2019-03-06T16:40:29.449489, step: 1010, loss: 0.018559370189905167, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-03-06T16:40:29.836409, step: 1011, loss: 0.048110999166965485, acc: 0.9922, auc: 0.9946, precision: 1.0, recall: 0.9846\n",
      "2019-03-06T16:40:30.226399, step: 1012, loss: 0.08611497282981873, acc: 0.9766, auc: 0.9888, precision: 0.9844, recall: 0.9692\n",
      "2019-03-06T16:40:30.620342, step: 1013, loss: 0.03457096219062805, acc: 0.9922, auc: 0.9995, precision: 0.9851, recall: 1.0\n",
      "2019-03-06T16:40:31.007306, step: 1014, loss: 0.05453093722462654, acc: 0.9922, auc: 0.9887, precision: 1.0, recall: 0.9851\n",
      "2019-03-06T16:40:31.391280, step: 1015, loss: 0.06301818788051605, acc: 0.9844, auc: 0.9993, precision: 0.9697, recall: 1.0\n",
      "2019-03-06T16:40:31.776221, step: 1016, loss: 0.05815913528203964, acc: 0.9844, auc: 0.9934, precision: 1.0, recall: 0.9701\n",
      "2019-03-06T16:40:32.163218, step: 1017, loss: 0.06194791942834854, acc: 0.9922, auc: 0.9966, precision: 0.9833, recall: 1.0\n",
      "2019-03-06T16:40:32.560126, step: 1018, loss: 0.05084690451622009, acc: 0.9844, auc: 1.0, precision: 0.9697, recall: 1.0\n",
      "2019-03-06T16:40:32.962050, step: 1019, loss: 0.059354063123464584, acc: 0.9844, auc: 0.9985, precision: 0.9846, recall: 0.9846\n",
      "2019-03-06T16:40:33.346053, step: 1020, loss: 0.028450369834899902, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9697\n",
      "2019-03-06T16:40:33.736013, step: 1021, loss: 0.12738549709320068, acc: 0.9609, auc: 0.9936, precision: 0.9701, recall: 0.9559\n",
      "2019-03-06T16:40:34.118958, step: 1022, loss: 0.05288340523838997, acc: 0.9922, auc: 0.9899, precision: 0.9859, recall: 1.0\n",
      "2019-03-06T16:40:34.503928, step: 1023, loss: 0.12526294589042664, acc: 0.9688, auc: 0.985, precision: 1.0, recall: 0.9429\n",
      "2019-03-06T16:40:34.880920, step: 1024, loss: 0.010922000743448734, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:40:35.273898, step: 1025, loss: 0.04741239547729492, acc: 0.9922, auc: 0.9961, precision: 0.9846, recall: 1.0\n",
      "2019-03-06T16:40:35.667829, step: 1026, loss: 0.016850769519805908, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "2019-03-06T16:40:36.056810, step: 1027, loss: 0.04575395584106445, acc: 0.9922, auc: 0.999, precision: 0.9848, recall: 1.0\n",
      "2019-03-06T16:40:36.439782, step: 1028, loss: 0.13419431447982788, acc: 0.9688, auc: 0.987, precision: 0.9697, recall: 0.9697\n",
      "2019-03-06T16:40:36.826747, step: 1029, loss: 0.02595239132642746, acc: 0.9844, auc: 0.9998, precision: 0.9836, recall: 0.9836\n",
      "2019-03-06T16:40:37.207729, step: 1030, loss: 0.08599226921796799, acc: 0.9844, auc: 0.9866, precision: 0.9841, recall: 0.9841\n",
      "2019-03-06T16:40:37.605664, step: 1031, loss: 0.08504942059516907, acc: 0.9844, auc: 0.9887, precision: 0.9851, recall: 0.9851\n",
      "2019-03-06T16:40:37.994625, step: 1032, loss: 0.020603619515895844, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-03-06T16:40:38.374609, step: 1033, loss: 0.016895540058612823, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:40:38.764536, step: 1034, loss: 0.060454994440078735, acc: 0.9844, auc: 0.9927, precision: 1.0, recall: 0.9683\n",
      "2019-03-06T16:40:39.150535, step: 1035, loss: 0.0199921652674675, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9855\n",
      "2019-03-06T16:40:39.541490, step: 1036, loss: 0.017975131049752235, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-03-06T16:40:39.935435, step: 1037, loss: 0.04932168498635292, acc: 0.9922, auc: 0.9958, precision: 1.0, recall: 0.9841\n",
      "2019-03-06T16:40:40.322401, step: 1038, loss: 0.1415071189403534, acc: 0.9688, auc: 0.9795, precision: 0.9683, recall: 0.9683\n",
      "2019-03-06T16:40:40.717344, step: 1039, loss: 0.07928277552127838, acc: 0.9766, auc: 0.9877, precision: 0.9851, recall: 0.9706\n",
      "2019-03-06T16:40:41.109295, step: 1040, loss: 0.04255308210849762, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9667\n",
      "2019-03-06T16:40:41.503242, step: 1041, loss: 0.04228915646672249, acc: 0.9844, auc: 1.0, precision: 0.9726, recall: 1.0\n",
      "2019-03-06T16:40:41.891203, step: 1042, loss: 0.1439429074525833, acc: 0.9609, auc: 0.9956, precision: 0.9206, recall: 1.0\n",
      "2019-03-06T16:40:42.271158, step: 1043, loss: 0.1546938419342041, acc: 0.9688, auc: 0.986, precision: 0.931, recall: 1.0\n",
      "2019-03-06T16:40:42.665145, step: 1044, loss: 0.08042048662900925, acc: 0.9844, auc: 0.9868, precision: 0.9848, recall: 0.9848\n",
      "2019-03-06T16:40:43.064067, step: 1045, loss: 0.04301687330007553, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9828\n",
      "2019-03-06T16:40:43.449008, step: 1046, loss: 0.03821469098329544, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9857\n",
      "2019-03-06T16:40:43.960642, step: 1047, loss: 0.023857714608311653, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-03-06T16:40:44.418418, step: 1048, loss: 0.08923570811748505, acc: 0.9688, auc: 0.9988, precision: 1.0, recall: 0.9365\n",
      "2019-03-06T16:40:44.803416, step: 1049, loss: 0.134730726480484, acc: 0.9453, auc: 0.9848, precision: 0.9839, recall: 0.9104\n",
      "2019-03-06T16:40:45.207308, step: 1050, loss: 0.01761028915643692, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:40:45.587292, step: 1051, loss: 0.06610418111085892, acc: 0.9844, auc: 0.9988, precision: 0.9688, recall: 1.0\n",
      "2019-03-06T16:40:45.987253, step: 1052, loss: 0.1644437164068222, acc: 0.9609, auc: 0.9759, precision: 0.9355, recall: 0.9831\n",
      "2019-03-06T16:40:46.368229, step: 1053, loss: 0.11455884575843811, acc: 0.9766, auc: 0.9934, precision: 0.9565, recall: 1.0\n",
      "2019-03-06T16:40:46.756197, step: 1054, loss: 0.1306736320257187, acc: 0.9609, auc: 0.9836, precision: 0.9828, recall: 0.9344\n",
      "2019-03-06T16:40:47.143159, step: 1055, loss: 0.06773141771554947, acc: 0.9844, auc: 0.9929, precision: 0.9833, recall: 0.9833\n",
      "2019-03-06T16:40:47.526136, step: 1056, loss: 0.07886415719985962, acc: 0.9844, auc: 0.988, precision: 0.9841, recall: 0.9841\n",
      "2019-03-06T16:40:47.914070, step: 1057, loss: 0.04672590643167496, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9697\n",
      "2019-03-06T16:40:48.310042, step: 1058, loss: 0.027281079441308975, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2019-03-06T16:40:48.700965, step: 1059, loss: 0.08383927494287491, acc: 0.9688, auc: 0.9951, precision: 0.9706, recall: 0.9706\n",
      "2019-03-06T16:40:49.078955, step: 1060, loss: 0.07328543812036514, acc: 0.9766, auc: 0.9934, precision: 1.0, recall: 0.9538\n",
      "2019-03-06T16:40:49.457968, step: 1061, loss: 0.041627731174230576, acc: 0.9766, auc: 1.0, precision: 1.0, recall: 0.9559\n",
      "2019-03-06T16:40:49.839957, step: 1062, loss: 0.022180410102009773, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-03-06T16:40:50.227916, step: 1063, loss: 0.032536063343286514, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9841\n",
      "2019-03-06T16:40:50.620868, step: 1064, loss: 0.10295279324054718, acc: 0.9609, auc: 0.9922, precision: 0.9688, recall: 0.9538\n",
      "2019-03-06T16:40:51.014813, step: 1065, loss: 0.1028124988079071, acc: 0.9766, auc: 0.985, precision: 0.9672, recall: 0.9833\n",
      "2019-03-06T16:40:51.406771, step: 1066, loss: 0.02207573875784874, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:40:51.797685, step: 1067, loss: 0.07464341074228287, acc: 0.9766, auc: 0.9971, precision: 0.9846, recall: 0.9697\n",
      "2019-03-06T16:40:52.186673, step: 1068, loss: 0.09370367974042892, acc: 0.9688, auc: 0.9985, precision: 0.9552, recall: 0.9846\n",
      "2019-03-06T16:40:52.577600, step: 1069, loss: 0.11134535819292068, acc: 0.9766, auc: 0.986, precision: 0.9718, recall: 0.9857\n",
      "2019-03-06T16:40:52.964565, step: 1070, loss: 0.10678064078092575, acc: 0.9766, auc: 0.9888, precision: 1.0, recall: 0.9583\n",
      "2019-03-06T16:40:53.352559, step: 1071, loss: 0.05822227895259857, acc: 0.9922, auc: 0.9927, precision: 1.0, recall: 0.9815\n",
      "2019-03-06T16:40:53.730550, step: 1072, loss: 0.022677958011627197, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-03-06T16:40:54.113526, step: 1073, loss: 0.0928681418299675, acc: 0.9844, auc: 0.9731, precision: 0.9726, recall: 1.0\n",
      "2019-03-06T16:40:54.515481, step: 1074, loss: 0.05513295531272888, acc: 0.9766, auc: 0.999, precision: 1.0, recall: 0.9545\n",
      "2019-03-06T16:40:54.909397, step: 1075, loss: 0.022250616922974586, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2019-03-06T16:40:55.306337, step: 1076, loss: 0.11118105053901672, acc: 0.9766, auc: 0.9797, precision: 0.9846, recall: 0.9697\n",
      "2019-03-06T16:40:55.781096, step: 1077, loss: 0.12090238928794861, acc: 0.9688, auc: 0.9971, precision: 1.0, recall: 0.9333\n",
      "2019-03-06T16:40:56.167035, step: 1078, loss: 0.023991700261831284, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:40:56.553003, step: 1079, loss: 0.02958711050450802, acc: 0.9922, auc: 1.0, precision: 0.9831, recall: 1.0\n",
      "2019-03-06T16:40:56.941991, step: 1080, loss: 0.04673818126320839, acc: 0.9844, auc: 1.0, precision: 0.9701, recall: 1.0\n",
      "2019-03-06T16:40:57.346880, step: 1081, loss: 0.1440299153327942, acc: 0.9688, auc: 0.9613, precision: 0.9672, recall: 0.9672\n",
      "2019-03-06T16:40:57.729856, step: 1082, loss: 0.10694494843482971, acc: 0.9766, auc: 0.9841, precision: 0.9697, recall: 0.9846\n",
      "2019-03-06T16:40:58.131781, step: 1083, loss: 0.034644123166799545, acc: 0.9922, auc: 0.9998, precision: 0.9848, recall: 1.0\n",
      "2019-03-06T16:40:58.522736, step: 1084, loss: 0.12300141900777817, acc: 0.9766, auc: 0.9824, precision: 0.9692, recall: 0.9844\n",
      "2019-03-06T16:40:58.995472, step: 1085, loss: 0.08882375061511993, acc: 0.9844, auc: 0.9868, precision: 0.9677, recall: 1.0\n",
      "2019-03-06T16:40:59.389447, step: 1086, loss: 0.03266654908657074, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9683\n",
      "2019-03-06T16:40:59.820266, step: 1087, loss: 0.10207390785217285, acc: 0.9688, auc: 0.9944, precision: 0.9839, recall: 0.9531\n",
      "2019-03-06T16:41:00.218233, step: 1088, loss: 0.040141087025403976, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9859\n",
      "2019-03-06T16:41:00.606193, step: 1089, loss: 0.08265899121761322, acc: 0.9844, auc: 0.9897, precision: 0.9839, recall: 0.9839\n",
      "2019-03-06T16:41:00.987146, step: 1090, loss: 0.035027459263801575, acc: 0.9922, auc: 0.9995, precision: 0.9836, recall: 1.0\n",
      "2019-03-06T16:41:01.379097, step: 1091, loss: 0.03789152204990387, acc: 0.9922, auc: 0.999, precision: 0.9846, recall: 1.0\n",
      "2019-03-06T16:41:01.757118, step: 1092, loss: 0.09658636152744293, acc: 0.9844, auc: 0.9775, precision: 0.9853, recall: 0.9853\n",
      "start training model\n",
      "2019-03-06T16:41:02.159012, step: 1093, loss: 0.040195874869823456, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9846\n",
      "2019-03-06T16:41:02.543982, step: 1094, loss: 0.1261141002178192, acc: 0.9688, auc: 0.9848, precision: 0.9844, recall: 0.9545\n",
      "2019-03-06T16:41:02.924994, step: 1095, loss: 0.025218050926923752, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9861\n",
      "2019-03-06T16:41:03.304978, step: 1096, loss: 0.01384978462010622, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:03.704911, step: 1097, loss: 0.015595558099448681, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:04.128745, step: 1098, loss: 0.06331856548786163, acc: 0.9766, auc: 0.9988, precision: 0.9571, recall: 1.0\n",
      "2019-03-06T16:41:04.535688, step: 1099, loss: 0.05950608104467392, acc: 0.9922, auc: 0.991, precision: 0.9841, recall: 1.0\n",
      "2019-03-06T16:41:04.913647, step: 1100, loss: 0.037656188011169434, acc: 0.9922, auc: 0.9995, precision: 0.9861, recall: 1.0\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:41:20.500999, step: 1100, loss: 0.5075234755491599, acc: 0.8645846153846154, auc: 0.9204435897435896, precision: 0.8784641025641026, recall: 0.8482974358974358\n",
      "2019-03-06T16:41:20.914888, step: 1101, loss: 0.023719510063529015, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9859\n",
      "2019-03-06T16:41:21.349730, step: 1102, loss: 0.059171926230192184, acc: 0.9922, auc: 0.9856, precision: 0.9851, recall: 1.0\n",
      "2019-03-06T16:41:21.840385, step: 1103, loss: 0.06646016985177994, acc: 0.9844, auc: 0.9875, precision: 0.9848, recall: 0.9848\n",
      "2019-03-06T16:41:22.307137, step: 1104, loss: 0.01181485690176487, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:22.745964, step: 1105, loss: 0.012564094737172127, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:23.174816, step: 1106, loss: 0.09203314781188965, acc: 0.9609, auc: 0.9963, precision: 0.9692, recall: 0.9545\n",
      "2019-03-06T16:41:23.575780, step: 1107, loss: 0.012497054412961006, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:23.963708, step: 1108, loss: 0.011717033572494984, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:24.352666, step: 1109, loss: 0.04511353373527527, acc: 0.9922, auc: 1.0, precision: 0.9825, recall: 1.0\n",
      "2019-03-06T16:41:24.749606, step: 1110, loss: 0.01096916664391756, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:25.126598, step: 1111, loss: 0.11176226288080215, acc: 0.9766, auc: 0.987, precision: 0.9672, recall: 0.9833\n",
      "2019-03-06T16:41:25.566421, step: 1112, loss: 0.09218568354845047, acc: 0.9766, auc: 0.9908, precision: 0.9726, recall: 0.9861\n",
      "2019-03-06T16:41:25.955381, step: 1113, loss: 0.16365979611873627, acc: 0.9609, auc: 0.9625, precision: 0.9474, recall: 0.9643\n",
      "2019-03-06T16:41:26.358303, step: 1114, loss: 0.046517711132764816, acc: 0.9922, auc: 0.9967, precision: 1.0, recall: 0.9868\n",
      "2019-03-06T16:41:26.747265, step: 1115, loss: 0.07331329584121704, acc: 0.9844, auc: 0.9871, precision: 0.9846, recall: 0.9846\n",
      "2019-03-06T16:41:27.133232, step: 1116, loss: 0.13253407180309296, acc: 0.9688, auc: 0.9872, precision: 0.9655, recall: 0.9655\n",
      "2019-03-06T16:41:27.526209, step: 1117, loss: 0.033499348908662796, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9831\n",
      "2019-03-06T16:41:27.910186, step: 1118, loss: 0.05551229044795036, acc: 0.9922, auc: 0.9904, precision: 1.0, recall: 0.9853\n",
      "2019-03-06T16:41:28.303104, step: 1119, loss: 0.04980103299021721, acc: 0.9922, auc: 0.9908, precision: 0.9825, recall: 1.0\n",
      "2019-03-06T16:41:28.682119, step: 1120, loss: 0.03698986396193504, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-03-06T16:41:29.075076, step: 1121, loss: 0.0922522246837616, acc: 0.9844, auc: 0.9902, precision: 1.0, recall: 0.9697\n",
      "2019-03-06T16:41:29.464000, step: 1122, loss: 0.04227665066719055, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9643\n",
      "2019-03-06T16:41:29.859941, step: 1123, loss: 0.00915837287902832, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:30.335669, step: 1124, loss: 0.0484398677945137, acc: 0.9844, auc: 0.9988, precision: 0.9841, recall: 0.9841\n",
      "2019-03-06T16:41:30.728618, step: 1125, loss: 0.024154450744390488, acc: 0.9922, auc: 1.0, precision: 0.9851, recall: 1.0\n",
      "2019-03-06T16:41:31.119601, step: 1126, loss: 0.032655104994773865, acc: 0.9922, auc: 1.0, precision: 0.9831, recall: 1.0\n",
      "2019-03-06T16:41:31.542441, step: 1127, loss: 0.061310362070798874, acc: 0.9844, auc: 0.9951, precision: 0.9692, recall: 1.0\n",
      "2019-03-06T16:41:31.972292, step: 1128, loss: 0.02664920501410961, acc: 0.9922, auc: 1.0, precision: 0.9839, recall: 1.0\n",
      "2019-03-06T16:41:32.425082, step: 1129, loss: 0.1324569284915924, acc: 0.9688, auc: 0.985, precision: 0.9608, recall: 0.9608\n",
      "2019-03-06T16:41:32.807088, step: 1130, loss: 0.05237092450261116, acc: 0.9844, auc: 0.9978, precision: 1.0, recall: 0.9636\n",
      "2019-03-06T16:41:33.194056, step: 1131, loss: 0.013096168637275696, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:33.575006, step: 1132, loss: 0.02064804546535015, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9808\n",
      "2019-03-06T16:41:34.002863, step: 1133, loss: 0.04047298803925514, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2019-03-06T16:41:34.396809, step: 1134, loss: 0.017523551359772682, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:34.793748, step: 1135, loss: 0.021176090463995934, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9851\n",
      "2019-03-06T16:41:35.177752, step: 1136, loss: 0.022274237126111984, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2019-03-06T16:41:35.555975, step: 1137, loss: 0.07398746907711029, acc: 0.9766, auc: 0.9865, precision: 1.0, recall: 0.9508\n",
      "2019-03-06T16:41:35.954137, step: 1138, loss: 0.020205382257699966, acc: 0.9922, auc: 1.0, precision: 0.9831, recall: 1.0\n",
      "2019-03-06T16:41:36.360236, step: 1139, loss: 0.040517862886190414, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9851\n",
      "2019-03-06T16:41:36.740219, step: 1140, loss: 0.012072490528225899, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:37.142144, step: 1141, loss: 0.03551623597741127, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.971\n",
      "2019-03-06T16:41:37.519169, step: 1142, loss: 0.09472549706697464, acc: 0.9766, auc: 0.9799, precision: 0.9672, recall: 0.9833\n",
      "2019-03-06T16:41:37.921062, step: 1143, loss: 0.0128093920648098, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:38.294065, step: 1144, loss: 0.04762553423643112, acc: 0.9844, auc: 0.9988, precision: 0.9844, recall: 0.9844\n",
      "2019-03-06T16:41:38.693189, step: 1145, loss: 0.04389209672808647, acc: 0.9766, auc: 0.9995, precision: 0.9688, recall: 0.9841\n",
      "2019-03-06T16:41:39.079126, step: 1146, loss: 0.014370463788509369, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:39.462222, step: 1147, loss: 0.025452585890889168, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9859\n",
      "2019-03-06T16:41:39.837219, step: 1148, loss: 0.01569301076233387, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:40.215209, step: 1149, loss: 0.008327426388859749, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:40.600180, step: 1150, loss: 0.008184067904949188, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:40.999112, step: 1151, loss: 0.07863594591617584, acc: 0.9766, auc: 0.9894, precision: 1.0, recall: 0.9483\n",
      "2019-03-06T16:41:41.398046, step: 1152, loss: 0.013901472091674805, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-03-06T16:41:41.782211, step: 1153, loss: 0.022328846156597137, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:42.164326, step: 1154, loss: 0.15086857974529266, acc: 0.9609, auc: 0.9853, precision: 0.9559, recall: 0.9701\n",
      "2019-03-06T16:41:42.535363, step: 1155, loss: 0.01235591433942318, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:42.918311, step: 1156, loss: 0.053377192467451096, acc: 0.9922, auc: 0.9954, precision: 1.0, recall: 0.9844\n",
      "2019-03-06T16:41:43.327217, step: 1157, loss: 0.12309876829385757, acc: 0.9766, auc: 0.9778, precision: 0.9836, recall: 0.9677\n",
      "2019-03-06T16:41:43.782000, step: 1158, loss: 0.012397076934576035, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:44.178968, step: 1159, loss: 0.044458627700805664, acc: 0.9922, auc: 0.9963, precision: 1.0, recall: 0.9844\n",
      "2019-03-06T16:41:44.605553, step: 1160, loss: 0.008288305252790451, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:44.996539, step: 1161, loss: 0.020206468179821968, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:45.372647, step: 1162, loss: 0.08270753175020218, acc: 0.9766, auc: 0.9954, precision: 0.9839, recall: 0.9683\n",
      "2019-03-06T16:41:45.761610, step: 1163, loss: 0.012694736942648888, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:46.178590, step: 1164, loss: 0.09143166244029999, acc: 0.9844, auc: 0.9924, precision: 0.9848, recall: 0.9848\n",
      "2019-03-06T16:41:46.558604, step: 1165, loss: 0.01228110771626234, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9859\n",
      "2019-03-06T16:41:46.938556, step: 1166, loss: 0.08977779746055603, acc: 0.9844, auc: 0.9896, precision: 0.9859, recall: 0.9859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:41:47.320566, step: 1167, loss: 0.05249079689383507, acc: 0.9922, auc: 0.988, precision: 0.9855, recall: 1.0\n",
      "2019-03-06T16:41:47.695925, step: 1168, loss: 0.021476708352565765, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-03-06T16:41:48.083856, step: 1169, loss: 0.010126940906047821, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:48.466960, step: 1170, loss: 0.12041520327329636, acc: 0.9766, auc: 0.9814, precision: 0.9672, recall: 0.9833\n",
      "2019-03-06T16:41:48.838966, step: 1171, loss: 0.00966772809624672, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:49.217981, step: 1172, loss: 0.04979132115840912, acc: 0.9922, auc: 0.9878, precision: 0.9848, recall: 1.0\n",
      "2019-03-06T16:41:49.681712, step: 1173, loss: 0.01553511805832386, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:50.049757, step: 1174, loss: 0.06162789463996887, acc: 0.9844, auc: 0.9881, precision: 1.0, recall: 0.9718\n",
      "2019-03-06T16:41:50.428744, step: 1175, loss: 0.01459692232310772, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:50.798919, step: 1176, loss: 0.008735348470509052, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:41:51.193126, step: 1177, loss: 0.07570702582597733, acc: 0.9844, auc: 0.9951, precision: 1.0, recall: 0.9649\n",
      "2019-03-06T16:41:51.580092, step: 1178, loss: 0.027594922110438347, acc: 0.9844, auc: 0.9995, precision: 0.9857, recall: 0.9857\n",
      "2019-03-06T16:41:51.957365, step: 1179, loss: 0.045062609016895294, acc: 0.9844, auc: 0.9998, precision: 0.9697, recall: 1.0\n",
      "2019-03-06T16:41:52.336391, step: 1180, loss: 0.015896854922175407, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9825\n",
      "2019-03-06T16:41:52.721354, step: 1181, loss: 0.09504775702953339, acc: 0.9766, auc: 0.9905, precision: 0.9848, recall: 0.9701\n",
      "2019-03-06T16:41:53.099344, step: 1182, loss: 0.05479230731725693, acc: 0.9922, auc: 0.9924, precision: 0.9839, recall: 1.0\n",
      "2019-03-06T16:41:53.494452, step: 1183, loss: 0.01763470470905304, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9825\n",
      "2019-03-06T16:41:53.876397, step: 1184, loss: 0.05273980274796486, acc: 0.9922, auc: 0.989, precision: 1.0, recall: 0.9853\n",
      "2019-03-06T16:41:54.260402, step: 1185, loss: 0.05286576226353645, acc: 0.9922, auc: 0.989, precision: 1.0, recall: 0.9848\n",
      "2019-03-06T16:41:54.645952, step: 1186, loss: 0.05151069536805153, acc: 0.9922, auc: 0.9907, precision: 1.0, recall: 0.9853\n",
      "2019-03-06T16:41:55.025904, step: 1187, loss: 0.04198623448610306, acc: 0.9922, auc: 0.998, precision: 1.0, recall: 0.9839\n",
      "2019-03-06T16:41:55.410908, step: 1188, loss: 0.14915157854557037, acc: 0.9609, auc: 0.983, precision: 0.9434, recall: 0.9615\n",
      "2019-03-06T16:41:55.794879, step: 1189, loss: 0.04594992473721504, acc: 0.9922, auc: 0.9976, precision: 1.0, recall: 0.9844\n",
      "2019-03-06T16:41:56.181015, step: 1190, loss: 0.0414789542555809, acc: 0.9922, auc: 0.9993, precision: 0.9851, recall: 1.0\n",
      "2019-03-06T16:41:56.555227, step: 1191, loss: 0.11593165993690491, acc: 0.9766, auc: 0.987, precision: 0.9667, recall: 0.9831\n",
      "2019-03-06T16:41:56.923213, step: 1192, loss: 0.04656565934419632, acc: 0.9922, auc: 0.9977, precision: 1.0, recall: 0.9867\n",
      "2019-03-06T16:41:57.302337, step: 1193, loss: 0.08742567896842957, acc: 0.9844, auc: 0.9869, precision: 0.9667, recall: 1.0\n",
      "2019-03-06T16:41:57.699248, step: 1194, loss: 0.02125500701367855, acc: 0.9922, auc: 1.0, precision: 0.9841, recall: 1.0\n",
      "2019-03-06T16:41:58.189935, step: 1195, loss: 0.0440346896648407, acc: 0.9922, auc: 0.9966, precision: 1.0, recall: 0.9833\n",
      "2019-03-06T16:41:58.633749, step: 1196, loss: 0.046435389667749405, acc: 0.9922, auc: 0.9976, precision: 1.0, recall: 0.9839\n",
      "2019-03-06T16:41:59.021743, step: 1197, loss: 0.14400465786457062, acc: 0.9688, auc: 0.9821, precision: 0.9516, recall: 0.9833\n",
      "2019-03-06T16:41:59.405684, step: 1198, loss: 0.030880944803357124, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9697\n",
      "2019-03-06T16:41:59.787825, step: 1199, loss: 0.042378980666399, acc: 0.9844, auc: 0.9993, precision: 0.9828, recall: 0.9828\n",
      "2019-03-06T16:42:00.168053, step: 1200, loss: 0.08087299764156342, acc: 0.9688, auc: 0.9954, precision: 0.9841, recall: 0.9538\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T16:42:15.209281, step: 1200, loss: 0.5223053991794586, acc: 0.8529666666666665, auc: 0.9160230769230768, precision: 0.8639641025641024, recall: 0.8429256410256408\n",
      "2019-03-06T16:42:15.579293, step: 1201, loss: 0.012828659266233444, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:15.975334, step: 1202, loss: 0.030634736642241478, acc: 0.9922, auc: 0.9995, precision: 0.9839, recall: 1.0\n",
      "2019-03-06T16:42:16.355318, step: 1203, loss: 0.01586790382862091, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:16.778187, step: 1204, loss: 0.015272852033376694, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:17.171136, step: 1205, loss: 0.205767422914505, acc: 0.9453, auc: 0.9674, precision: 0.9344, recall: 0.95\n",
      "2019-03-06T16:42:17.554391, step: 1206, loss: 0.04438842087984085, acc: 0.9844, auc: 1.0, precision: 0.9697, recall: 1.0\n",
      "2019-03-06T16:42:17.974301, step: 1207, loss: 0.052651576697826385, acc: 0.9922, auc: 0.993, precision: 0.9865, recall: 1.0\n",
      "2019-03-06T16:42:18.398350, step: 1208, loss: 0.05727250874042511, acc: 0.9922, auc: 0.9931, precision: 1.0, recall: 0.9851\n",
      "2019-03-06T16:42:18.823246, step: 1209, loss: 0.049049485474824905, acc: 0.9922, auc: 0.9938, precision: 0.9861, recall: 1.0\n",
      "2019-03-06T16:42:19.202229, step: 1210, loss: 0.05241512879729271, acc: 0.9922, auc: 0.9934, precision: 1.0, recall: 0.9853\n",
      "2019-03-06T16:42:19.585212, step: 1211, loss: 0.023912258446216583, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9718\n",
      "2019-03-06T16:42:20.018020, step: 1212, loss: 0.012443137355148792, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:20.437927, step: 1213, loss: 0.008417671546339989, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:20.896812, step: 1214, loss: 0.06586753576993942, acc: 0.9922, auc: 0.9946, precision: 1.0, recall: 0.9836\n",
      "2019-03-06T16:42:21.276031, step: 1215, loss: 0.05364912375807762, acc: 0.9844, auc: 0.9971, precision: 1.0, recall: 0.9677\n",
      "2019-03-06T16:42:21.724076, step: 1216, loss: 0.05203647166490555, acc: 0.9922, auc: 0.9941, precision: 1.0, recall: 0.9839\n",
      "2019-03-06T16:42:22.116995, step: 1217, loss: 0.09283918142318726, acc: 0.9766, auc: 0.9929, precision: 1.0, recall: 0.9545\n",
      "2019-03-06T16:42:22.489996, step: 1218, loss: 0.04693157970905304, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9844\n",
      "2019-03-06T16:42:22.885964, step: 1219, loss: 0.05168495327234268, acc: 0.9922, auc: 0.9962, precision: 0.9818, recall: 1.0\n",
      "2019-03-06T16:42:23.296839, step: 1220, loss: 0.05498528480529785, acc: 0.9922, auc: 0.9861, precision: 1.0, recall: 0.9848\n",
      "2019-03-06T16:42:23.687011, step: 1221, loss: 0.021903960034251213, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9722\n",
      "2019-03-06T16:42:24.076997, step: 1222, loss: 0.13070321083068848, acc: 0.9766, auc: 0.9738, precision: 0.971, recall: 0.9853\n",
      "2019-03-06T16:42:24.460624, step: 1223, loss: 0.05750204995274544, acc: 0.9922, auc: 0.9957, precision: 0.9872, recall: 1.0\n",
      "2019-03-06T16:42:24.865542, step: 1224, loss: 0.026030370965600014, acc: 0.9922, auc: 1.0, precision: 0.9836, recall: 1.0\n",
      "2019-03-06T16:42:25.250512, step: 1225, loss: 0.03570065274834633, acc: 0.9922, auc: 0.999, precision: 1.0, recall: 0.9828\n",
      "2019-03-06T16:42:25.626474, step: 1226, loss: 0.08691318333148956, acc: 0.9844, auc: 0.9902, precision: 0.9848, recall: 0.9848\n",
      "2019-03-06T16:42:26.004647, step: 1227, loss: 0.018203070387244225, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:26.375684, step: 1228, loss: 0.05411044880747795, acc: 0.9922, auc: 0.9892, precision: 0.9853, recall: 1.0\n",
      "2019-03-06T16:42:26.748938, step: 1229, loss: 0.052434515208005905, acc: 0.9922, auc: 0.9912, precision: 1.0, recall: 0.9839\n",
      "2019-03-06T16:42:27.127925, step: 1230, loss: 0.058036595582962036, acc: 0.9844, auc: 0.9877, precision: 0.9853, recall: 0.9853\n",
      "2019-03-06T16:42:27.530064, step: 1231, loss: 0.010486487299203873, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:27.936316, step: 1232, loss: 0.05143766105175018, acc: 0.9844, auc: 0.9985, precision: 0.9706, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:42:28.311313, step: 1233, loss: 0.025818243622779846, acc: 0.9922, auc: 0.9998, precision: 0.9836, recall: 1.0\n",
      "2019-03-06T16:42:28.695286, step: 1234, loss: 0.11006683111190796, acc: 0.9688, auc: 0.9775, precision: 0.9545, recall: 0.9844\n",
      "2019-03-06T16:42:29.085243, step: 1235, loss: 0.05900188535451889, acc: 0.9844, auc: 0.9938, precision: 1.0, recall: 0.9643\n",
      "2019-03-06T16:42:29.489383, step: 1236, loss: 0.03538547456264496, acc: 0.9844, auc: 0.9998, precision: 0.9841, recall: 0.9841\n",
      "2019-03-06T16:42:29.870334, step: 1237, loss: 0.06960602849721909, acc: 0.9844, auc: 0.984, precision: 0.9865, recall: 0.9865\n",
      "2019-03-06T16:42:30.246479, step: 1238, loss: 0.06697320938110352, acc: 0.9844, auc: 0.9898, precision: 0.9818, recall: 0.9818\n",
      "2019-03-06T16:42:30.640425, step: 1239, loss: 0.03037678636610508, acc: 0.9922, auc: 1.0, precision: 0.9836, recall: 1.0\n",
      "2019-03-06T16:42:31.048335, step: 1240, loss: 0.0516471341252327, acc: 0.9922, auc: 0.999, precision: 0.9846, recall: 1.0\n",
      "2019-03-06T16:42:31.449262, step: 1241, loss: 0.01860184222459793, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9825\n",
      "2019-03-06T16:42:31.848195, step: 1242, loss: 0.046447332948446274, acc: 0.9766, auc: 0.999, precision: 0.9853, recall: 0.971\n",
      "2019-03-06T16:42:32.231204, step: 1243, loss: 0.010979745537042618, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:32.613313, step: 1244, loss: 0.018061626702547073, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9828\n",
      "2019-03-06T16:42:32.989279, step: 1245, loss: 0.02423044852912426, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:33.365410, step: 1246, loss: 0.026619533076882362, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9846\n",
      "2019-03-06T16:42:33.751379, step: 1247, loss: 0.12586833536624908, acc: 0.9688, auc: 0.9836, precision: 0.9667, recall: 0.9667\n",
      "2019-03-06T16:42:34.185088, step: 1248, loss: 0.02192905731499195, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9848\n",
      "start training model\n",
      "2019-03-06T16:42:34.599449, step: 1249, loss: 0.02141820825636387, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:34.978437, step: 1250, loss: 0.014309892430901527, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:35.354429, step: 1251, loss: 0.028483305126428604, acc: 0.9922, auc: 0.9995, precision: 0.9839, recall: 1.0\n",
      "2019-03-06T16:42:35.737594, step: 1252, loss: 0.05425897613167763, acc: 0.9922, auc: 0.9884, precision: 1.0, recall: 0.9808\n",
      "2019-03-06T16:42:36.208810, step: 1253, loss: 0.053927257657051086, acc: 0.9844, auc: 0.9995, precision: 0.9701, recall: 1.0\n",
      "2019-03-06T16:42:36.633674, step: 1254, loss: 0.03922925144433975, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9836\n",
      "2019-03-06T16:42:37.025626, step: 1255, loss: 0.013017778284847736, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:37.405611, step: 1256, loss: 0.04525572434067726, acc: 0.9922, auc: 0.9975, precision: 0.9857, recall: 1.0\n",
      "2019-03-06T16:42:37.790609, step: 1257, loss: 0.009511636570096016, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:38.185525, step: 1258, loss: 0.013112608343362808, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:38.583617, step: 1259, loss: 0.009590785950422287, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:38.958582, step: 1260, loss: 0.1267899125814438, acc: 0.9766, auc: 0.9849, precision: 0.9859, recall: 0.9722\n",
      "2019-03-06T16:42:39.346710, step: 1261, loss: 0.008219948969781399, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:39.728660, step: 1262, loss: 0.03052341192960739, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9865\n",
      "2019-03-06T16:42:40.153524, step: 1263, loss: 0.009756991639733315, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:40.616287, step: 1264, loss: 0.010427441447973251, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:41.043173, step: 1265, loss: 0.008033590391278267, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:41.428145, step: 1266, loss: 0.007205801550298929, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:41.823311, step: 1267, loss: 0.008911171928048134, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:42.200412, step: 1268, loss: 0.014738542959094048, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9846\n",
      "2019-03-06T16:42:42.591336, step: 1269, loss: 0.06698710471391678, acc: 0.9844, auc: 0.9958, precision: 0.9853, recall: 0.9853\n",
      "2019-03-06T16:42:43.062075, step: 1270, loss: 0.05123337730765343, acc: 0.9922, auc: 0.9865, precision: 0.9831, recall: 1.0\n",
      "2019-03-06T16:42:43.442089, step: 1271, loss: 0.042857199907302856, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9683\n",
      "2019-03-06T16:42:43.825064, step: 1272, loss: 0.05046346038579941, acc: 0.9844, auc: 0.9954, precision: 0.9846, recall: 0.9846\n",
      "2019-03-06T16:42:44.197040, step: 1273, loss: 0.08066362142562866, acc: 0.9688, auc: 0.9976, precision: 0.9844, recall: 0.9545\n",
      "2019-03-06T16:42:44.584764, step: 1274, loss: 0.09035147726535797, acc: 0.9844, auc: 0.9932, precision: 1.0, recall: 0.9692\n",
      "2019-03-06T16:42:45.047495, step: 1275, loss: 0.08560390770435333, acc: 0.9844, auc: 0.997, precision: 1.0, recall: 0.9718\n",
      "2019-03-06T16:42:45.432593, step: 1276, loss: 0.04004766792058945, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9828\n",
      "2019-03-06T16:42:45.864973, step: 1277, loss: 0.011711052618920803, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:46.249902, step: 1278, loss: 0.023745346814393997, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9836\n",
      "2019-03-06T16:42:46.636897, step: 1279, loss: 0.0887269377708435, acc: 0.9844, auc: 0.9912, precision: 0.9718, recall: 1.0\n",
      "2019-03-06T16:42:47.038793, step: 1280, loss: 0.027954518795013428, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9839\n",
      "2019-03-06T16:42:47.410798, step: 1281, loss: 0.0290011465549469, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9844\n",
      "2019-03-06T16:42:47.812162, step: 1282, loss: 0.0390075258910656, acc: 0.9922, auc: 0.999, precision: 1.0, recall: 0.9863\n",
      "2019-03-06T16:42:48.234190, step: 1283, loss: 0.0472567193210125, acc: 0.9922, auc: 0.9972, precision: 0.9815, recall: 1.0\n",
      "2019-03-06T16:42:48.616139, step: 1284, loss: 0.0074539342895150185, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:49.038042, step: 1285, loss: 0.051742758601903915, acc: 0.9922, auc: 0.9919, precision: 0.9841, recall: 1.0\n",
      "2019-03-06T16:42:49.475848, step: 1286, loss: 0.04837930575013161, acc: 0.9922, auc: 0.9902, precision: 1.0, recall: 0.9831\n",
      "2019-03-06T16:42:49.850845, step: 1287, loss: 0.018544631078839302, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9828\n",
      "2019-03-06T16:42:50.306626, step: 1288, loss: 0.04667659476399422, acc: 0.9922, auc: 0.9936, precision: 1.0, recall: 0.9839\n",
      "2019-03-06T16:42:50.692697, step: 1289, loss: 0.08208508789539337, acc: 0.9844, auc: 0.9892, precision: 0.9853, recall: 0.9853\n",
      "2019-03-06T16:42:51.087641, step: 1290, loss: 0.050213176757097244, acc: 0.9922, auc: 0.994, precision: 0.9815, recall: 1.0\n",
      "2019-03-06T16:42:51.478893, step: 1291, loss: 0.007402412593364716, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:51.859082, step: 1292, loss: 0.09134452790021896, acc: 0.9844, auc: 0.9796, precision: 0.9863, recall: 0.9863\n",
      "2019-03-06T16:42:52.289929, step: 1293, loss: 0.008155254647135735, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:52.669913, step: 1294, loss: 0.03665832430124283, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9859\n",
      "2019-03-06T16:42:53.064858, step: 1295, loss: 0.06806943565607071, acc: 0.9844, auc: 0.9919, precision: 0.9844, recall: 0.9844\n",
      "2019-03-06T16:42:53.472051, step: 1296, loss: 0.03945327177643776, acc: 0.9922, auc: 0.9982, precision: 1.0, recall: 0.9868\n",
      "2019-03-06T16:42:53.974706, step: 1297, loss: 0.011435923166573048, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:54.393310, step: 1298, loss: 0.10010620951652527, acc: 0.9766, auc: 0.9944, precision: 0.9701, recall: 0.9848\n",
      "2019-03-06T16:42:54.865049, step: 1299, loss: 0.005978109780699015, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:42:55.248024, step: 1300, loss: 0.055939286947250366, acc: 0.9922, auc: 0.9868, precision: 0.9855, recall: 1.0\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:43:10.532923, step: 1300, loss: 0.5888405151856251, acc: 0.8585794871794872, auc: 0.9153717948717949, precision: 0.8605897435897436, recall: 0.8601923076923075\n",
      "2019-03-06T16:43:10.911912, step: 1301, loss: 0.1340472400188446, acc: 0.9766, auc: 0.9792, precision: 0.9683, recall: 0.9839\n",
      "2019-03-06T16:43:11.289901, step: 1302, loss: 0.048586077988147736, acc: 0.9844, auc: 0.9971, precision: 1.0, recall: 0.9667\n",
      "2019-03-06T16:43:11.671997, step: 1303, loss: 0.05303336679935455, acc: 0.9844, auc: 0.9961, precision: 1.0, recall: 0.9688\n",
      "2019-03-06T16:43:12.062921, step: 1304, loss: 0.006234294269233942, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:12.440094, step: 1305, loss: 0.07055431604385376, acc: 0.9766, auc: 0.9988, precision: 0.9846, recall: 0.9697\n",
      "2019-03-06T16:43:12.812067, step: 1306, loss: 0.050517499446868896, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9706\n",
      "2019-03-06T16:43:13.182109, step: 1307, loss: 0.00920890737324953, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:13.559100, step: 1308, loss: 0.04058438912034035, acc: 0.9922, auc: 0.999, precision: 1.0, recall: 0.9846\n",
      "2019-03-06T16:43:13.949056, step: 1309, loss: 0.007380519062280655, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:14.325053, step: 1310, loss: 0.007123266346752644, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:14.744817, step: 1311, loss: 0.06149384751915932, acc: 0.9766, auc: 0.9988, precision: 0.9583, recall: 1.0\n",
      "2019-03-06T16:43:15.138796, step: 1312, loss: 0.046668365597724915, acc: 0.9922, auc: 0.995, precision: 1.0, recall: 0.9863\n",
      "2019-03-06T16:43:15.514901, step: 1313, loss: 0.009155023843050003, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:15.887181, step: 1314, loss: 0.10593915730714798, acc: 0.9766, auc: 0.9939, precision: 0.9714, recall: 0.9855\n",
      "2019-03-06T16:43:16.275112, step: 1315, loss: 0.0440964512526989, acc: 0.9922, auc: 0.9992, precision: 0.98, recall: 1.0\n",
      "2019-03-06T16:43:16.654128, step: 1316, loss: 0.008822368457913399, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:17.043087, step: 1317, loss: 0.03277715668082237, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9853\n",
      "2019-03-06T16:43:17.429057, step: 1318, loss: 0.10967504978179932, acc: 0.9766, auc: 0.986, precision: 0.9853, recall: 0.971\n",
      "2019-03-06T16:43:17.869171, step: 1319, loss: 0.00875528622418642, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:18.278237, step: 1320, loss: 0.03555983304977417, acc: 0.9922, auc: 0.9993, precision: 1.0, recall: 0.9855\n",
      "2019-03-06T16:43:18.659219, step: 1321, loss: 0.011809981428086758, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:19.044160, step: 1322, loss: 0.008289200253784657, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:19.487973, step: 1323, loss: 0.008143209852278233, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:19.894884, step: 1324, loss: 0.08664325624704361, acc: 0.9766, auc: 0.9919, precision: 0.9844, recall: 0.9692\n",
      "2019-03-06T16:43:20.341743, step: 1325, loss: 0.10213551670312881, acc: 0.9844, auc: 0.979, precision: 0.9825, recall: 0.9825\n",
      "2019-03-06T16:43:20.733745, step: 1326, loss: 0.008257572539150715, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:21.157752, step: 1327, loss: 0.008976045995950699, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:21.539731, step: 1328, loss: 0.050965406000614166, acc: 0.9922, auc: 0.9955, precision: 0.9825, recall: 1.0\n",
      "2019-03-06T16:43:21.927986, step: 1329, loss: 0.08982647955417633, acc: 0.9844, auc: 0.9907, precision: 0.9701, recall: 1.0\n",
      "2019-03-06T16:43:22.305977, step: 1330, loss: 0.012867295183241367, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9828\n",
      "2019-03-06T16:43:22.675985, step: 1331, loss: 0.08729676902294159, acc: 0.9844, auc: 0.989, precision: 0.9688, recall: 1.0\n",
      "2019-03-06T16:43:23.053945, step: 1332, loss: 0.00908348336815834, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:23.434956, step: 1333, loss: 0.006704380270093679, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:23.815128, step: 1334, loss: 0.009286755695939064, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:24.193360, step: 1335, loss: 0.007282735779881477, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:24.569841, step: 1336, loss: 0.037678465247154236, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9873\n",
      "2019-03-06T16:43:24.950824, step: 1337, loss: 0.009689713828265667, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:25.327815, step: 1338, loss: 0.044129613786935806, acc: 0.9922, auc: 0.9973, precision: 0.9853, recall: 1.0\n",
      "2019-03-06T16:43:25.713752, step: 1339, loss: 0.007969421334564686, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:26.128874, step: 1340, loss: 0.0131861362606287, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9841\n",
      "2019-03-06T16:43:26.513876, step: 1341, loss: 0.042403917759656906, acc: 0.9922, auc: 0.9969, precision: 1.0, recall: 0.9804\n",
      "2019-03-06T16:43:26.905830, step: 1342, loss: 0.03386886417865753, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9636\n",
      "2019-03-06T16:43:27.379719, step: 1343, loss: 0.007110640872269869, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:27.771779, step: 1344, loss: 0.009460443630814552, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:28.176668, step: 1345, loss: 0.05205604434013367, acc: 0.9922, auc: 0.9852, precision: 0.9861, recall: 1.0\n",
      "2019-03-06T16:43:28.571645, step: 1346, loss: 0.06983037292957306, acc: 0.9844, auc: 0.9973, precision: 1.0, recall: 0.9672\n",
      "2019-03-06T16:43:28.963594, step: 1347, loss: 0.010886533185839653, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:29.365489, step: 1348, loss: 0.07967415452003479, acc: 0.9844, auc: 0.9932, precision: 0.9697, recall: 1.0\n",
      "2019-03-06T16:43:29.765548, step: 1349, loss: 0.00730849476531148, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:30.149521, step: 1350, loss: 0.012076431885361671, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:30.587488, step: 1351, loss: 0.09424827992916107, acc: 0.9844, auc: 0.9861, precision: 0.9848, recall: 0.9848\n",
      "2019-03-06T16:43:31.001352, step: 1352, loss: 0.0069993846118450165, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:31.375383, step: 1353, loss: 0.0074958885088562965, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:31.778275, step: 1354, loss: 0.006582729518413544, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:32.181206, step: 1355, loss: 0.01486726850271225, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:32.613290, step: 1356, loss: 0.021690236404538155, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9649\n",
      "2019-03-06T16:43:32.983316, step: 1357, loss: 0.07963401824235916, acc: 0.9844, auc: 0.9937, precision: 0.9868, recall: 0.9868\n",
      "2019-03-06T16:43:33.429265, step: 1358, loss: 0.05291059613227844, acc: 0.9844, auc: 0.9998, precision: 1.0, recall: 0.9643\n",
      "2019-03-06T16:43:33.838199, step: 1359, loss: 0.007671908475458622, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:34.217461, step: 1360, loss: 0.011037332937121391, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:34.590132, step: 1361, loss: 0.007356330286711454, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:34.996016, step: 1362, loss: 0.06584109365940094, acc: 0.9844, auc: 0.9978, precision: 0.9848, recall: 0.9848\n",
      "2019-03-06T16:43:35.379024, step: 1363, loss: 0.04928683862090111, acc: 0.9922, auc: 0.998, precision: 0.9825, recall: 1.0\n",
      "2019-03-06T16:43:35.753171, step: 1364, loss: 0.08872543275356293, acc: 0.9688, auc: 0.999, precision: 0.9455, recall: 0.9811\n",
      "2019-03-06T16:43:36.133329, step: 1365, loss: 0.00916192214936018, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:36.504544, step: 1366, loss: 0.06313854455947876, acc: 0.9844, auc: 0.9975, precision: 1.0, recall: 0.973\n",
      "2019-03-06T16:43:36.898461, step: 1367, loss: 0.09110870957374573, acc: 0.9844, auc: 0.9853, precision: 1.0, recall: 0.9672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:43:37.323354, step: 1368, loss: 0.04650575667619705, acc: 0.9922, auc: 0.9975, precision: 1.0, recall: 0.9804\n",
      "2019-03-06T16:43:37.716273, step: 1369, loss: 0.010333584621548653, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:38.122213, step: 1370, loss: 0.04885723069310188, acc: 0.9922, auc: 0.9928, precision: 1.0, recall: 0.9861\n",
      "2019-03-06T16:43:38.516257, step: 1371, loss: 0.04750198870897293, acc: 0.9922, auc: 0.997, precision: 0.9828, recall: 1.0\n",
      "2019-03-06T16:43:38.903251, step: 1372, loss: 0.0604175440967083, acc: 0.9844, auc: 0.9956, precision: 0.9851, recall: 0.9851\n",
      "2019-03-06T16:43:39.322234, step: 1373, loss: 0.005827221553772688, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:39.720198, step: 1374, loss: 0.09743715822696686, acc: 0.9766, auc: 0.9863, precision: 0.971, recall: 0.9853\n",
      "2019-03-06T16:43:40.142043, step: 1375, loss: 0.09328581392765045, acc: 0.9844, auc: 0.9897, precision: 0.9815, recall: 0.9815\n",
      "2019-03-06T16:43:40.587862, step: 1376, loss: 0.05285830795764923, acc: 0.9922, auc: 0.9949, precision: 1.0, recall: 0.9851\n",
      "2019-03-06T16:43:40.979814, step: 1377, loss: 0.031326934695243835, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-03-06T16:43:41.407669, step: 1378, loss: 0.0057783955708146095, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:41.789758, step: 1379, loss: 0.07644213736057281, acc: 0.9844, auc: 0.9926, precision: 0.9825, recall: 0.9825\n",
      "2019-03-06T16:43:42.237751, step: 1380, loss: 0.08194397389888763, acc: 0.9766, auc: 0.9882, precision: 1.0, recall: 0.9483\n",
      "2019-03-06T16:43:42.666654, step: 1381, loss: 0.1022823303937912, acc: 0.9766, auc: 0.9871, precision: 0.9692, recall: 0.9844\n",
      "2019-03-06T16:43:43.057565, step: 1382, loss: 0.0364413782954216, acc: 0.9844, auc: 0.9995, precision: 0.9844, recall: 0.9844\n",
      "2019-03-06T16:43:43.446524, step: 1383, loss: 0.04680595546960831, acc: 0.9922, auc: 0.9976, precision: 1.0, recall: 0.9844\n",
      "2019-03-06T16:43:43.890338, step: 1384, loss: 0.009248105809092522, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:44.295255, step: 1385, loss: 0.09228324145078659, acc: 0.9766, auc: 0.9844, precision: 0.9737, recall: 0.9867\n",
      "2019-03-06T16:43:44.734764, step: 1386, loss: 0.08822430670261383, acc: 0.9766, auc: 0.9953, precision: 0.971, recall: 0.9853\n",
      "2019-03-06T16:43:45.112755, step: 1387, loss: 0.07001151144504547, acc: 0.9766, auc: 0.9856, precision: 0.9841, recall: 0.9688\n",
      "2019-03-06T16:43:45.490907, step: 1388, loss: 0.0815630629658699, acc: 0.9844, auc: 0.9936, precision: 0.9833, recall: 0.9833\n",
      "2019-03-06T16:43:45.879988, step: 1389, loss: 0.0066458918154239655, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:46.260004, step: 1390, loss: 0.045796461403369904, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9714\n",
      "2019-03-06T16:43:46.634970, step: 1391, loss: 0.04029884189367294, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9818\n",
      "2019-03-06T16:43:47.034932, step: 1392, loss: 0.01099217589944601, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:47.416910, step: 1393, loss: 0.053653448820114136, acc: 0.9922, auc: 0.9944, precision: 0.9836, recall: 1.0\n",
      "2019-03-06T16:43:47.796177, step: 1394, loss: 0.03745061531662941, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9841\n",
      "2019-03-06T16:43:48.174464, step: 1395, loss: 0.04670635610818863, acc: 0.9922, auc: 0.9968, precision: 1.0, recall: 0.9848\n",
      "2019-03-06T16:43:48.557438, step: 1396, loss: 0.0425461083650589, acc: 0.9844, auc: 0.999, precision: 1.0, recall: 0.9688\n",
      "2019-03-06T16:43:48.948363, step: 1397, loss: 0.07538184523582458, acc: 0.9766, auc: 0.9978, precision: 1.0, recall: 0.9589\n",
      "2019-03-06T16:43:49.324389, step: 1398, loss: 0.00907130166888237, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:49.778144, step: 1399, loss: 0.007788554299622774, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:43:50.160123, step: 1400, loss: 0.09809936583042145, acc: 0.9766, auc: 0.9971, precision: 0.9559, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T16:44:04.942962, step: 1400, loss: 0.6615884326971494, acc: 0.8479538461538462, auc: 0.9097923076923078, precision: 0.810294871794872, recall: 0.9142743589743588\n",
      "2019-03-06T16:44:05.328898, step: 1401, loss: 0.018101446330547333, acc: 0.9922, auc: 1.0, precision: 0.9863, recall: 1.0\n",
      "2019-03-06T16:44:05.701118, step: 1402, loss: 0.006023951340466738, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:06.080210, step: 1403, loss: 0.013717793859541416, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:06.470305, step: 1404, loss: 0.007646465674042702, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "start training model\n",
      "2019-03-06T16:44:06.884198, step: 1405, loss: 0.053665585815906525, acc: 0.9844, auc: 0.9934, precision: 0.9851, recall: 0.9851\n",
      "2019-03-06T16:44:07.261222, step: 1406, loss: 0.028637811541557312, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9863\n",
      "2019-03-06T16:44:07.639391, step: 1407, loss: 0.07235097140073776, acc: 0.9766, auc: 0.9968, precision: 0.9831, recall: 0.9667\n",
      "2019-03-06T16:44:08.024007, step: 1408, loss: 0.018692970275878906, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9839\n",
      "2019-03-06T16:44:08.411449, step: 1409, loss: 0.012601821683347225, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2019-03-06T16:44:08.783682, step: 1410, loss: 0.008065387606620789, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:09.176872, step: 1411, loss: 0.009340105578303337, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:09.561814, step: 1412, loss: 0.021952811628580093, acc: 0.9922, auc: 1.0, precision: 0.987, recall: 1.0\n",
      "2019-03-06T16:44:09.942794, step: 1413, loss: 0.011200333945453167, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9859\n",
      "2019-03-06T16:44:10.371648, step: 1414, loss: 0.05144689232110977, acc: 0.9922, auc: 0.9908, precision: 0.9825, recall: 1.0\n",
      "2019-03-06T16:44:10.768586, step: 1415, loss: 0.017720472067594528, acc: 0.9922, auc: 1.0, precision: 0.9841, recall: 1.0\n",
      "2019-03-06T16:44:11.148570, step: 1416, loss: 0.017304470762610435, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2019-03-06T16:44:11.541688, step: 1417, loss: 0.04828523471951485, acc: 0.9922, auc: 0.9926, precision: 0.9859, recall: 1.0\n",
      "2019-03-06T16:44:11.922701, step: 1418, loss: 0.07858746498823166, acc: 0.9844, auc: 0.9963, precision: 0.9839, recall: 0.9839\n",
      "2019-03-06T16:44:12.292815, step: 1419, loss: 0.028366439044475555, acc: 0.9922, auc: 1.0, precision: 0.9833, recall: 1.0\n",
      "2019-03-06T16:44:12.662826, step: 1420, loss: 0.03452140837907791, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9865\n",
      "2019-03-06T16:44:13.053812, step: 1421, loss: 0.05903488025069237, acc: 0.9922, auc: 0.9888, precision: 0.9844, recall: 1.0\n",
      "2019-03-06T16:44:13.444734, step: 1422, loss: 0.04592263698577881, acc: 0.9922, auc: 0.994, precision: 1.0, recall: 0.9821\n",
      "2019-03-06T16:44:13.815743, step: 1423, loss: 0.0065310560166835785, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:14.196438, step: 1424, loss: 0.0117532042786479, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:14.581152, step: 1425, loss: 0.006714026443660259, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:14.951162, step: 1426, loss: 0.00682060606777668, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:15.333304, step: 1427, loss: 0.05054449290037155, acc: 0.9922, auc: 0.9904, precision: 0.9855, recall: 1.0\n",
      "2019-03-06T16:44:15.707271, step: 1428, loss: 0.05339084565639496, acc: 0.9922, auc: 0.9931, precision: 1.0, recall: 0.9851\n",
      "2019-03-06T16:44:16.081453, step: 1429, loss: 0.009790528565645218, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:16.462434, step: 1430, loss: 0.07920515537261963, acc: 0.9688, auc: 0.9929, precision: 1.0, recall: 0.9355\n",
      "2019-03-06T16:44:16.842418, step: 1431, loss: 0.04944555461406708, acc: 0.9922, auc: 0.9928, precision: 0.9825, recall: 1.0\n",
      "2019-03-06T16:44:17.218444, step: 1432, loss: 0.006274368613958359, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:17.587677, step: 1433, loss: 0.007491834461688995, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:44:17.965667, step: 1434, loss: 0.07743426412343979, acc: 0.9844, auc: 0.9927, precision: 0.9848, recall: 0.9848\n",
      "2019-03-06T16:44:18.361703, step: 1435, loss: 0.010762064717710018, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:18.755653, step: 1436, loss: 0.05526698753237724, acc: 0.9922, auc: 0.9907, precision: 0.9853, recall: 1.0\n",
      "2019-03-06T16:44:19.139624, step: 1437, loss: 0.010822732001543045, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:19.511629, step: 1438, loss: 0.0066413888707757, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:19.900557, step: 1439, loss: 0.006833563558757305, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:20.305475, step: 1440, loss: 0.048160385340452194, acc: 0.9922, auc: 0.9939, precision: 0.9857, recall: 1.0\n",
      "2019-03-06T16:44:20.708555, step: 1441, loss: 0.005552802234888077, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:21.111508, step: 1442, loss: 0.06981098651885986, acc: 0.9844, auc: 0.9993, precision: 1.0, recall: 0.9649\n",
      "2019-03-06T16:44:21.507561, step: 1443, loss: 0.005561087746173143, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:21.928672, step: 1444, loss: 0.005412595346570015, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:22.334553, step: 1445, loss: 0.06684859097003937, acc: 0.9766, auc: 0.9983, precision: 0.9839, recall: 0.9683\n",
      "2019-03-06T16:44:22.728500, step: 1446, loss: 0.005621153861284256, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:23.135443, step: 1447, loss: 0.005948982201516628, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:23.529497, step: 1448, loss: 0.00528393778949976, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:24.023176, step: 1449, loss: 0.0045901695266366005, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:24.392836, step: 1450, loss: 0.006336803548038006, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:24.763815, step: 1451, loss: 0.05477849766612053, acc: 0.9844, auc: 1.0, precision: 0.9672, recall: 1.0\n",
      "2019-03-06T16:44:25.141805, step: 1452, loss: 0.010594025254249573, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:25.529768, step: 1453, loss: 0.005778689868748188, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:25.919918, step: 1454, loss: 0.17621386051177979, acc: 0.9531, auc: 0.9848, precision: 1.0, recall: 0.9091\n",
      "2019-03-06T16:44:26.312896, step: 1455, loss: 0.1734851896762848, acc: 0.9453, auc: 0.9966, precision: 0.9836, recall: 0.9091\n",
      "2019-03-06T16:44:26.695006, step: 1456, loss: 0.26052165031433105, acc: 0.9375, auc: 0.9882, precision: 1.0, recall: 0.8841\n",
      "2019-03-06T16:44:27.089951, step: 1457, loss: 0.15178748965263367, acc: 0.9609, auc: 0.9966, precision: 1.0, recall: 0.9167\n",
      "2019-03-06T16:44:27.466209, step: 1458, loss: 0.10074418038129807, acc: 0.9766, auc: 0.9902, precision: 1.0, recall: 0.9552\n",
      "2019-03-06T16:44:27.904619, step: 1459, loss: 0.032132700085639954, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9848\n",
      "2019-03-06T16:44:28.318543, step: 1460, loss: 0.05253521353006363, acc: 0.9844, auc: 0.9992, precision: 0.9811, recall: 0.9811\n",
      "2019-03-06T16:44:28.876053, step: 1461, loss: 0.035693537443876266, acc: 0.9922, auc: 1.0, precision: 0.9841, recall: 1.0\n",
      "2019-03-06T16:44:29.299919, step: 1462, loss: 0.07839380949735641, acc: 0.9766, auc: 1.0, precision: 0.9559, recall: 1.0\n",
      "2019-03-06T16:44:29.687977, step: 1463, loss: 0.08545244485139847, acc: 0.9766, auc: 0.9961, precision: 0.9692, recall: 0.9844\n",
      "2019-03-06T16:44:30.086910, step: 1464, loss: 0.14884093403816223, acc: 0.9688, auc: 0.9892, precision: 0.9531, recall: 0.9839\n",
      "2019-03-06T16:44:30.527856, step: 1465, loss: 0.04865684360265732, acc: 0.9844, auc: 0.999, precision: 0.9836, recall: 0.9836\n",
      "2019-03-06T16:44:30.919808, step: 1466, loss: 0.06508814543485641, acc: 0.9844, auc: 0.9905, precision: 1.0, recall: 0.9697\n",
      "2019-03-06T16:44:31.329712, step: 1467, loss: 0.1172536164522171, acc: 0.9609, auc: 0.9985, precision: 1.0, recall: 0.9153\n",
      "2019-03-06T16:44:31.778512, step: 1468, loss: 0.169181227684021, acc: 0.9375, auc: 1.0, precision: 1.0, recall: 0.875\n",
      "2019-03-06T16:44:32.169467, step: 1469, loss: 0.1700465977191925, acc: 0.9375, auc: 0.9966, precision: 1.0, recall: 0.873\n",
      "2019-03-06T16:44:32.561606, step: 1470, loss: 0.1665402501821518, acc: 0.9375, auc: 1.0, precision: 1.0, recall: 0.8689\n",
      "2019-03-06T16:44:32.956549, step: 1471, loss: 0.0693853348493576, acc: 0.9766, auc: 0.9995, precision: 1.0, recall: 0.9545\n",
      "2019-03-06T16:44:33.352619, step: 1472, loss: 0.10297841578722, acc: 0.9609, auc: 0.9993, precision: 1.0, recall: 0.9123\n",
      "2019-03-06T16:44:33.744602, step: 1473, loss: 0.08250848948955536, acc: 0.9688, auc: 0.9945, precision: 1.0, recall: 0.9444\n",
      "2019-03-06T16:44:34.123589, step: 1474, loss: 0.04563888907432556, acc: 0.9766, auc: 0.9998, precision: 1.0, recall: 0.9545\n",
      "2019-03-06T16:44:34.501171, step: 1475, loss: 0.022947126999497414, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-03-06T16:44:34.923105, step: 1476, loss: 0.01655794493854046, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:35.318080, step: 1477, loss: 0.07925744354724884, acc: 0.9766, auc: 1.0, precision: 0.96, recall: 1.0\n",
      "2019-03-06T16:44:35.772108, step: 1478, loss: 0.03621599078178406, acc: 0.9922, auc: 1.0, precision: 0.9848, recall: 1.0\n",
      "2019-03-06T16:44:36.183529, step: 1479, loss: 0.0376008115708828, acc: 0.9922, auc: 0.9998, precision: 0.9851, recall: 1.0\n",
      "2019-03-06T16:44:36.572489, step: 1480, loss: 0.1383623480796814, acc: 0.9688, auc: 0.9962, precision: 0.931, recall: 1.0\n",
      "2019-03-06T16:44:36.958457, step: 1481, loss: 0.03877837210893631, acc: 0.9844, auc: 0.9992, precision: 1.0, recall: 0.963\n",
      "2019-03-06T16:44:37.350409, step: 1482, loss: 0.047997500747442245, acc: 0.9844, auc: 0.999, precision: 1.0, recall: 0.9661\n",
      "2019-03-06T16:44:37.773307, step: 1483, loss: 0.09528353810310364, acc: 0.9766, auc: 0.9939, precision: 0.9846, recall: 0.9697\n",
      "2019-03-06T16:44:38.164232, step: 1484, loss: 0.04185027629137039, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9677\n",
      "2019-03-06T16:44:38.556351, step: 1485, loss: 0.041504137217998505, acc: 0.9844, auc: 0.9995, precision: 1.0, recall: 0.9722\n",
      "2019-03-06T16:44:38.944313, step: 1486, loss: 0.017347965389490128, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:39.368350, step: 1487, loss: 0.07047703117132187, acc: 0.9688, auc: 0.9958, precision: 1.0, recall: 0.9437\n",
      "2019-03-06T16:44:39.760335, step: 1488, loss: 0.020415911450982094, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:40.156272, step: 1489, loss: 0.09918761253356934, acc: 0.9844, auc: 0.9842, precision: 0.9859, recall: 0.9859\n",
      "2019-03-06T16:44:40.535230, step: 1490, loss: 0.03205319494009018, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9844\n",
      "2019-03-06T16:44:40.923223, step: 1491, loss: 0.1406436711549759, acc: 0.9609, auc: 0.9853, precision: 0.9655, recall: 0.9492\n",
      "2019-03-06T16:44:41.336118, step: 1492, loss: 0.10451552271842957, acc: 0.9844, auc: 0.9965, precision: 0.9636, recall: 1.0\n",
      "2019-03-06T16:44:41.719193, step: 1493, loss: 0.01975751668214798, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:42.181131, step: 1494, loss: 0.022525515407323837, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9836\n",
      "2019-03-06T16:44:42.562143, step: 1495, loss: 0.07327353954315186, acc: 0.9844, auc: 0.9848, precision: 0.9836, recall: 0.9836\n",
      "2019-03-06T16:44:42.938107, step: 1496, loss: 0.016953183338046074, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:43.302163, step: 1497, loss: 0.02686852589249611, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:44:43.674171, step: 1498, loss: 0.04472442343831062, acc: 0.9922, auc: 0.9956, precision: 1.0, recall: 0.9853\n",
      "2019-03-06T16:44:44.056149, step: 1499, loss: 0.07443451136350632, acc: 0.9844, auc: 0.9917, precision: 1.0, recall: 0.9672\n",
      "2019-03-06T16:44:44.432579, step: 1500, loss: 0.06579974293708801, acc: 0.9844, auc: 0.9963, precision: 0.9701, recall: 1.0\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T16:45:00.067849, step: 1500, loss: 0.5738219435398395, acc: 0.8571769230769228, auc: 0.9201846153846156, precision: 0.862697435897436, recall: 0.8541282051282051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T16:45:00.449990, step: 1501, loss: 0.011677064001560211, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:00.860864, step: 1502, loss: 0.01839117705821991, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9857\n",
      "2019-03-06T16:45:01.233868, step: 1503, loss: 0.07604608684778214, acc: 0.9766, auc: 0.9985, precision: 1.0, recall: 0.9516\n",
      "2019-03-06T16:45:01.666709, step: 1504, loss: 0.04400978982448578, acc: 0.9922, auc: 0.9995, precision: 1.0, recall: 0.9851\n",
      "2019-03-06T16:45:02.038745, step: 1505, loss: 0.00975782424211502, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:02.417731, step: 1506, loss: 0.01306744571775198, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:02.794826, step: 1507, loss: 0.02674892172217369, acc: 0.9922, auc: 0.9998, precision: 0.9865, recall: 1.0\n",
      "2019-03-06T16:45:03.174953, step: 1508, loss: 0.10819491744041443, acc: 0.9844, auc: 0.9743, precision: 0.9677, recall: 1.0\n",
      "2019-03-06T16:45:03.564939, step: 1509, loss: 0.011950873769819736, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9853\n",
      "2019-03-06T16:45:03.955864, step: 1510, loss: 0.04977555572986603, acc: 0.9922, auc: 0.9958, precision: 0.9855, recall: 1.0\n",
      "2019-03-06T16:45:04.336875, step: 1511, loss: 0.01301906630396843, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9833\n",
      "2019-03-06T16:45:04.713422, step: 1512, loss: 0.010559801943600178, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:05.100042, step: 1513, loss: 0.05552683398127556, acc: 0.9922, auc: 0.9936, precision: 0.9833, recall: 1.0\n",
      "2019-03-06T16:45:05.473242, step: 1514, loss: 0.011891236528754234, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:05.854225, step: 1515, loss: 0.016114886850118637, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:06.239584, step: 1516, loss: 0.04953931272029877, acc: 0.9922, auc: 0.9902, precision: 1.0, recall: 0.9839\n",
      "2019-03-06T16:45:06.620565, step: 1517, loss: 0.050451360642910004, acc: 0.9922, auc: 0.9898, precision: 1.0, recall: 0.9821\n",
      "2019-03-06T16:45:07.017534, step: 1518, loss: 0.052164215594530106, acc: 0.9922, auc: 0.9946, precision: 0.9839, recall: 1.0\n",
      "2019-03-06T16:45:07.396490, step: 1519, loss: 0.04379325732588768, acc: 0.9844, auc: 1.0, precision: 1.0, recall: 0.9697\n",
      "2019-03-06T16:45:07.875211, step: 1520, loss: 0.08912569284439087, acc: 0.9844, auc: 0.998, precision: 0.9677, recall: 1.0\n",
      "2019-03-06T16:45:08.255225, step: 1521, loss: 0.008684445172548294, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:08.640324, step: 1522, loss: 0.03946356102824211, acc: 0.9922, auc: 0.9993, precision: 0.9853, recall: 1.0\n",
      "2019-03-06T16:45:09.032245, step: 1523, loss: 0.006297241430729628, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:09.549988, step: 1524, loss: 0.006577485240995884, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:09.922990, step: 1525, loss: 0.06979082524776459, acc: 0.9844, auc: 0.99, precision: 0.9865, recall: 0.9865\n",
      "2019-03-06T16:45:10.304002, step: 1526, loss: 0.007645061239600182, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:10.683985, step: 1527, loss: 0.06640053540468216, acc: 0.9844, auc: 0.9945, precision: 0.9818, recall: 0.9818\n",
      "2019-03-06T16:45:11.063971, step: 1528, loss: 0.04645579308271408, acc: 0.9922, auc: 0.9975, precision: 1.0, recall: 0.9857\n",
      "2019-03-06T16:45:11.437939, step: 1529, loss: 0.008962048217654228, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:11.817191, step: 1530, loss: 0.07853080332279205, acc: 0.9766, auc: 0.9922, precision: 1.0, recall: 0.9444\n",
      "2019-03-06T16:45:12.201391, step: 1531, loss: 0.01622246392071247, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9831\n",
      "2019-03-06T16:45:12.576419, step: 1532, loss: 0.005758840590715408, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:12.949423, step: 1533, loss: 0.09414700418710709, acc: 0.9766, auc: 0.9971, precision: 0.9672, recall: 0.9833\n",
      "2019-03-06T16:45:13.337388, step: 1534, loss: 0.010836128145456314, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:13.732326, step: 1535, loss: 0.00610888097435236, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:14.109320, step: 1536, loss: 0.03855598345398903, acc: 0.9922, auc: 0.9988, precision: 1.0, recall: 0.9836\n",
      "2019-03-06T16:45:14.485240, step: 1537, loss: 0.05018767714500427, acc: 0.9922, auc: 0.9951, precision: 0.9831, recall: 1.0\n",
      "2019-03-06T16:45:14.876165, step: 1538, loss: 0.03549204766750336, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9836\n",
      "2019-03-06T16:45:15.302178, step: 1539, loss: 0.05393047630786896, acc: 0.9844, auc: 0.9968, precision: 1.0, recall: 0.9706\n",
      "2019-03-06T16:45:15.691107, step: 1540, loss: 0.012402634136378765, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:16.151000, step: 1541, loss: 0.0464489683508873, acc: 0.9922, auc: 0.9907, precision: 1.0, recall: 0.9841\n",
      "2019-03-06T16:45:16.539959, step: 1542, loss: 0.007326681632548571, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:16.967815, step: 1543, loss: 0.015474611893296242, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9811\n",
      "2019-03-06T16:45:17.367778, step: 1544, loss: 0.058378104120492935, acc: 0.9766, auc: 0.9978, precision: 0.9839, recall: 0.9683\n",
      "2019-03-06T16:45:17.757953, step: 1545, loss: 0.011014716699719429, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:18.238786, step: 1546, loss: 0.08192747831344604, acc: 0.9844, auc: 0.9934, precision: 0.971, recall: 1.0\n",
      "2019-03-06T16:45:18.614811, step: 1547, loss: 0.10338128358125687, acc: 0.9688, auc: 0.998, precision: 0.9718, recall: 0.9718\n",
      "2019-03-06T16:45:18.995763, step: 1548, loss: 0.027062900364398956, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9821\n",
      "2019-03-06T16:45:19.376743, step: 1549, loss: 0.06049911677837372, acc: 0.9766, auc: 0.9983, precision: 0.9859, recall: 0.9722\n",
      "2019-03-06T16:45:19.752767, step: 1550, loss: 0.01457362063229084, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:20.135714, step: 1551, loss: 0.028206437826156616, acc: 0.9922, auc: 0.9998, precision: 0.9839, recall: 1.0\n",
      "2019-03-06T16:45:20.511903, step: 1552, loss: 0.049078796058893204, acc: 0.9922, auc: 0.9968, precision: 0.9831, recall: 1.0\n",
      "2019-03-06T16:45:20.887926, step: 1553, loss: 0.03155144304037094, acc: 0.9922, auc: 0.9998, precision: 1.0, recall: 0.9825\n",
      "2019-03-06T16:45:21.262006, step: 1554, loss: 0.11089659482240677, acc: 0.9766, auc: 0.9872, precision: 0.9825, recall: 0.9655\n",
      "2019-03-06T16:45:21.631048, step: 1555, loss: 0.010028230957686901, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:22.015212, step: 1556, loss: 0.026296701282262802, acc: 0.9922, auc: 0.9998, precision: 0.9851, recall: 1.0\n",
      "2019-03-06T16:45:22.391206, step: 1557, loss: 0.12153483182191849, acc: 0.9688, auc: 0.9966, precision: 1.0, recall: 0.9394\n",
      "2019-03-06T16:45:22.767201, step: 1558, loss: 0.08499594032764435, acc: 0.9844, auc: 0.9906, precision: 0.9825, recall: 0.9825\n",
      "2019-03-06T16:45:23.142229, step: 1559, loss: 0.006448781583458185, acc: 1.0, auc: 1.0, precision: 1.0, recall: 1.0\n",
      "2019-03-06T16:45:23.516458, step: 1560, loss: 0.020326385274529457, acc: 0.9922, auc: 1.0, precision: 1.0, recall: 0.9857\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = RCNN(config, wordEmbedding)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Bi-LSTM/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "#                 if currentStep % config.training.checkpointEvery == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/Bi-LSTM/model/my-model\", global_step=currentStep)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(lstm.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#         builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
