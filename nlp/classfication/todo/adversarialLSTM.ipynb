{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import json\n",
    "import threading\n",
    "\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "\n",
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置参数\n",
    "\n",
    "class TrainingConfig(object):\n",
    "    epoches = 5\n",
    "    evaluateEvery = 100\n",
    "    checkpointEvery = 100\n",
    "    learningRate = 0.001\n",
    "    \n",
    "class ModelConfig(object):\n",
    "    embeddingSize = 200\n",
    "    \n",
    "    hiddenSizes = 128  # LSTM结构的神经元个数\n",
    "    \n",
    "    dropoutKeepProb = 0.5\n",
    "    l2RegLambda = 0.0\n",
    "    epsilon = 5\n",
    "    \n",
    "class Config(object):\n",
    "    sequenceLength = 200  # 取了所有序列长度的均值\n",
    "    batchSize = 128\n",
    "    \n",
    "    dataSource = \"../data/preProcess/labeledTrain.csv\"\n",
    "    \n",
    "    stopWordSource = \"../data/english\"\n",
    "    \n",
    "    numClasses = 2\n",
    "    \n",
    "    rate = 0.8  # 训练集的比例\n",
    "    \n",
    "    training = TrainingConfig()\n",
    "    \n",
    "    model = ModelConfig()\n",
    "\n",
    "    \n",
    "# 实例化配置参数对象\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理的类，生成训练集和测试集\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, config):\n",
    "        self._dataSource = config.dataSource\n",
    "        self._stopWordSource = config.stopWordSource  \n",
    "        \n",
    "        self._sequenceLength = config.sequenceLength  # 每条输入的序列处理为定长\n",
    "        self._embeddingSize = config.model.embeddingSize\n",
    "        self._batchSize = config.batchSize\n",
    "        self._rate = config.rate\n",
    "        \n",
    "        self._stopWordDict = {}\n",
    "        \n",
    "        self.trainReviews = []\n",
    "        self.trainLabels = []\n",
    "        \n",
    "        self.evalReviews = []\n",
    "        self.evalLabels = []\n",
    "        \n",
    "        self.wordEmbedding =None\n",
    "        \n",
    "        self.indexFreqs = []  # 统计词空间中的词在出现在多少个review中\n",
    "        \n",
    "        self._wordToIndex = {}\n",
    "        self._indexToWord = {}\n",
    "        \n",
    "    def _readData(self, filePath):\n",
    "        \"\"\"\n",
    "        从csv文件中读取数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(filePath)\n",
    "        labels = df[\"sentiment\"].tolist()\n",
    "        review = df[\"review\"].tolist()\n",
    "        reviews = [line.strip().split() for line in review]\n",
    "\n",
    "        return reviews, labels\n",
    "\n",
    "    def _reviewProcess(self, review, sequenceLength, wordToIndex):\n",
    "        \"\"\"\n",
    "        将数据集中的每条评论用index表示\n",
    "        wordToIndex中“pad”对应的index为0\n",
    "        \"\"\"\n",
    "        \n",
    "        reviewVec = np.zeros((sequenceLength))\n",
    "        sequenceLen = sequenceLength\n",
    "        \n",
    "        # 判断当前的序列是否小于定义的固定序列长度\n",
    "        if len(review) < sequenceLength:\n",
    "            sequenceLen = len(review)\n",
    "            \n",
    "        for i in range(sequenceLen):\n",
    "            if review[i] in wordToIndex:\n",
    "                reviewVec[i] = wordToIndex[review[i]]\n",
    "            else:\n",
    "                reviewVec[i] = wordToIndex[\"UNK\"]\n",
    "\n",
    "        return reviewVec\n",
    "\n",
    "    def _genTrainEvalData(self, x, y, rate):\n",
    "        \"\"\"\n",
    "        生成训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        reviews = []\n",
    "        labels = []\n",
    "        \n",
    "        # 遍历所有的文本，将文本中的词转换成index表示\n",
    "        for i in range(len(x)):\n",
    "            reviewVec = self._reviewProcess(x[i], self._sequenceLength, self._wordToIndex)\n",
    "            reviews.append(reviewVec)\n",
    "            \n",
    "            labels.append([y[i]])\n",
    "            \n",
    "        trainIndex = int(len(x) * rate)\n",
    "        \n",
    "        trainReviews = np.asarray(reviews[:trainIndex], dtype=\"int64\")\n",
    "        trainLabels = np.array(labels[:trainIndex], dtype=\"float32\")\n",
    "        \n",
    "        evalReviews = np.asarray(reviews[trainIndex:], dtype=\"int64\")\n",
    "        evalLabels = np.array(labels[trainIndex:], dtype=\"float32\")\n",
    "\n",
    "        return trainReviews, trainLabels, evalReviews, evalLabels\n",
    "        \n",
    "    def _genVocabulary(self, reviews):\n",
    "        \"\"\"\n",
    "        生成词向量和词汇-索引映射字典，可以用全数据集\n",
    "        \"\"\"\n",
    "        \n",
    "        allWords = [word for review in reviews for word in review]\n",
    "        \n",
    "        # 去掉停用词\n",
    "        subWords = [word for word in allWords if word not in self.stopWordDict]\n",
    "        \n",
    "        wordCount = Counter(subWords)  # 统计词频\n",
    "        sortWordCount = sorted(wordCount.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 去除低频词\n",
    "        words = [item[0] for item in sortWordCount if item[1] >= 5]\n",
    "        \n",
    "        vocab, wordEmbedding = self._getWordEmbedding(words)\n",
    "        self.wordEmbedding = wordEmbedding\n",
    "        \n",
    "        self._wordToIndex = dict(zip(vocab, list(range(len(vocab)))))\n",
    "        self._indexToWord = dict(zip(list(range(len(vocab))), vocab))\n",
    "        \n",
    "        # 得到逆词频\n",
    "        self._getWordIndexFreq(vocab, reviews)\n",
    "        \n",
    "        # 将词汇-索引映射表保存为json数据，之后做inference时直接加载来处理数据\n",
    "        with open(\"../data/wordJson/wordToIndex.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._wordToIndex, f)\n",
    "        \n",
    "        with open(\"../data/wordJson/indexToWord.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(self._indexToWord, f)\n",
    "            \n",
    "    def _getWordEmbedding(self, words):\n",
    "        \"\"\"\n",
    "        按照我们的数据集中的单词取出预训练好的word2vec中的词向量\n",
    "        \"\"\"\n",
    "        \n",
    "        wordVec = gensim.models.KeyedVectors.load_word2vec_format(\"../word2vec/word2Vec.bin\", binary=True)\n",
    "        vocab = []\n",
    "        wordEmbedding = []\n",
    "        \n",
    "        # 添加 \"pad\" 和 \"UNK\", \n",
    "        vocab.append(\"pad\")\n",
    "        vocab.append(\"UNK\")\n",
    "        wordEmbedding.append(np.zeros(self._embeddingSize))\n",
    "        wordEmbedding.append(np.random.randn(self._embeddingSize))\n",
    "        \n",
    "        for word in words:\n",
    "            try:\n",
    "                vector = wordVec.wv[word]\n",
    "                vocab.append(word)\n",
    "                wordEmbedding.append(vector)\n",
    "            except:\n",
    "                print(word + \"不存在于词向量中\")\n",
    "                \n",
    "        return vocab, np.array(wordEmbedding)\n",
    "    \n",
    "    def _getWordIndexFreq(self, vocab, reviews):\n",
    "        \"\"\"\n",
    "        统计词汇空间中各个词出现在多少个文本中\n",
    "        \"\"\"\n",
    "        reviewDicts = [dict(zip(review, range(len(review)))) for review in reviews]\n",
    "        indexFreqs = [0] * len(vocab)\n",
    "        for word in vocab:\n",
    "            count = 0\n",
    "            for review in reviewDicts:\n",
    "                if word in review:\n",
    "                    count += 1\n",
    "            indexFreqs[self._wordToIndex[word]] = count\n",
    "        \n",
    "        self.indexFreqs = indexFreqs\n",
    "    \n",
    "    def _readStopWord(self, stopWordPath):\n",
    "        \"\"\"\n",
    "        读取停用词\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(stopWordPath, \"r\") as f:\n",
    "            stopWords = f.read()\n",
    "            stopWordList = stopWords.splitlines()\n",
    "            # 将停用词用列表的形式生成，之后查找停用词时会比较快\n",
    "            self.stopWordDict = dict(zip(stopWordList, list(range(len(stopWordList)))))\n",
    "            \n",
    "    def dataGen(self):\n",
    "        \"\"\"\n",
    "        初始化训练集和验证集\n",
    "        \"\"\"\n",
    "        \n",
    "        # 初始化停用词\n",
    "        self._readStopWord(self._stopWordSource)\n",
    "        \n",
    "        # 初始化数据集\n",
    "        reviews, labels = self._readData(self._dataSource)\n",
    "        \n",
    "        # 初始化词汇-索引映射表和词向量矩阵\n",
    "        self._genVocabulary(reviews)\n",
    "        \n",
    "        # 初始化训练集和测试集\n",
    "        trainReviews, trainLabels, evalReviews, evalLabels = self._genTrainEvalData(reviews, labels, self._rate)\n",
    "        self.trainReviews = trainReviews\n",
    "        self.trainLabels = trainLabels\n",
    "        \n",
    "        self.evalReviews = evalReviews\n",
    "        self.evalLabels = evalLabels\n",
    "        \n",
    "        \n",
    "data = Dataset(config)\n",
    "data.dataGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (20000, 200)\n",
      "train label shape: (20000, 1)\n",
      "eval data shape: (5000, 200)\n"
     ]
    }
   ],
   "source": [
    "print(\"train data shape: {}\".format(data.trainReviews.shape))\n",
    "print(\"train label shape: {}\".format(data.trainLabels.shape))\n",
    "print(\"eval data shape: {}\".format(data.evalReviews.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31983, 200)\n"
     ]
    }
   ],
   "source": [
    "print(data.wordEmbedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出batch数据集\n",
    "\n",
    "def nextBatch(x, y, batchSize):\n",
    "        \"\"\"\n",
    "        生成batch数据集，用生成器的方式输出\n",
    "        \"\"\"\n",
    "    \n",
    "        perm = np.arange(len(x))\n",
    "        np.random.shuffle(perm)\n",
    "        x = x[perm]\n",
    "        y = y[perm]\n",
    "        \n",
    "        numBatches = len(x) // batchSize\n",
    "\n",
    "        for i in range(numBatches):\n",
    "            start = i * batchSize\n",
    "            end = start + batchSize\n",
    "            batchX = np.array(x[start: end], dtype=\"int64\")\n",
    "            batchY = np.array(y[start: end], dtype=\"float32\")\n",
    "            \n",
    "            yield batchX, batchY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建模型\n",
    "class AdversarialLSTM(object):\n",
    "    \"\"\"\n",
    "    Text CNN 用于文本分类\n",
    "    \"\"\"\n",
    "    def __init__(self, config, wordEmbedding, indexFreqs):\n",
    "\n",
    "        # 定义模型的输入\n",
    "        self.inputX = tf.placeholder(tf.int32, [None, config.sequenceLength], name=\"inputX\")\n",
    "        self.inputY = tf.placeholder(tf.float32, [None, 1], name=\"inputY\")\n",
    "        \n",
    "        self.dropoutKeepProb = tf.placeholder(tf.float32, name=\"dropoutKeepProb\")\n",
    "        self.config = config\n",
    "        \n",
    "        # 根据词的频率计算权重\n",
    "        indexFreqs[0], indexFreqs[1] = 20000, 10000\n",
    "        weights = tf.cast(tf.reshape(indexFreqs / tf.reduce_sum(indexFreqs), [1, len(indexFreqs)]), dtype=tf.float32)\n",
    "        \n",
    "        # 词嵌入层\n",
    "        with tf.name_scope(\"embedding\"):\n",
    "\n",
    "            # 利用词频计算新的词嵌入矩阵\n",
    "            normWordEmbedding = self._normalize(tf.cast(wordEmbedding, dtype=tf.float32, name=\"word2vec\"), weights)\n",
    "            \n",
    "            # 利用词嵌入矩阵将输入的数据中的词转换成词向量，维度[batch_size, sequence_length, embedding_size]\n",
    "            self.embeddedWords = tf.nn.embedding_lookup(normWordEmbedding, self.inputX)\n",
    "            \n",
    "         # 计算二元交叉熵损失 \n",
    "        with tf.name_scope(\"loss\"):\n",
    "            with tf.variable_scope(\"Bi-LSTM\", reuse=None):\n",
    "                self.predictions = self._Bi_LSTMAttention(self.embeddedWords)\n",
    "                self.binaryPreds = tf.cast(tf.greater_equal(self.predictions, 0.5), tf.float32, name=\"binaryPreds\")\n",
    "                losses = tf.nn.sigmoid_cross_entropy_with_logits(logits=self.predictions, labels=self.inputY)\n",
    "                loss = tf.reduce_mean(losses)\n",
    "        \n",
    "        with tf.name_scope(\"perturLoss\"):\n",
    "            with tf.variable_scope(\"Bi-LSTM\", reuse=True):\n",
    "                perturWordEmbedding = self._addPerturbation(self.embeddedWords, loss)\n",
    "                perturPredictions = self._Bi_LSTMAttention(perturWordEmbedding)\n",
    "                perturLosses = tf.nn.sigmoid_cross_entropy_with_logits(logits=perturPredictions, labels=self.inputY)\n",
    "                perturLoss = tf.reduce_mean(perturLosses)\n",
    "        \n",
    "        self.loss = loss + perturLoss\n",
    "            \n",
    "    def _Bi_LSTMAttention(self, embeddedWords):\n",
    "        \"\"\"\n",
    "        Bi-LSTM + Attention 的模型结构\n",
    "        \"\"\"\n",
    "        \n",
    "        config = self.config\n",
    "        \n",
    "        # 定义双向LSTM的模型结构\n",
    "        with tf.name_scope(\"Bi-LSTM\"):\n",
    "           \n",
    "            # 定义前向LSTM结构\n",
    "            lstmFwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=config.model.hiddenSizes, state_is_tuple=True),\n",
    "                                                         output_keep_prob=self.dropoutKeepProb)\n",
    "            # 定义反向LSTM结构\n",
    "            lstmBwCell = tf.nn.rnn_cell.DropoutWrapper(tf.nn.rnn_cell.LSTMCell(num_units=config.model.hiddenSizes, state_is_tuple=True),\n",
    "                                                         output_keep_prob=self.dropoutKeepProb)\n",
    "\n",
    "\n",
    "            # 采用动态rnn，可以动态的输入序列的长度，若没有输入，则取序列的全长\n",
    "            # outputs是一个元祖(output_fw, output_bw)，其中两个元素的维度都是[batch_size, max_time, hidden_size],fw和bw的hidden_size一样\n",
    "            # self.current_state 是最终的状态，二元组(state_fw, state_bw)，state_fw=[batch_size, s]，s是一个元祖(h, c)\n",
    "            outputs, self.current_state = tf.nn.bidirectional_dynamic_rnn(lstmFwCell, lstmBwCell, \n",
    "                                                                          self.embeddedWords, dtype=tf.float32,\n",
    "                                                                          scope=\"bi-lstm\")\n",
    "\n",
    "        \n",
    "        # 在Bi-LSTM+Attention的论文中，将前向和后向的输出相加\n",
    "        with tf.name_scope(\"Attention\"):\n",
    "            H = outputs[0] + outputs[1]\n",
    "\n",
    "            # 得到Attention的输出\n",
    "            output = self._attention(H)\n",
    "            outputSize = config.model.hiddenSizes\n",
    "        \n",
    "        # 全连接层的输出\n",
    "        with tf.name_scope(\"output\"):\n",
    "            outputW = tf.get_variable(\n",
    "                \"outputW\",\n",
    "                shape=[outputSize, 1],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            outputB= tf.Variable(tf.constant(0.1, shape=[1]), name=\"outputB\")\n",
    "            predictions = tf.nn.xw_plus_b(output, outputW, outputB, name=\"predictions\")\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def _attention(self, H):\n",
    "        \"\"\"\n",
    "        利用Attention机制得到句子的向量表示\n",
    "        \"\"\"\n",
    "        # 获得最后一层LSTM的神经元数量\n",
    "        hiddenSize = config.model.hiddenSizes\n",
    "        \n",
    "        # 初始化一个权重向量，是可训练的参数\n",
    "        W = tf.Variable(tf.random_normal([hiddenSize], stddev=0.1))\n",
    "        \n",
    "        # 对Bi-LSTM的输出用激活函数做非线性转换\n",
    "        M = tf.tanh(H)\n",
    "        \n",
    "        # 对W和M做矩阵运算，W=[batch_size, time_step, hidden_size]，计算前做维度转换成[batch_size * time_step, hidden_size]\n",
    "        # newM = [batch_size, time_step, 1]，每一个时间步的输出由向量转换成一个数字\n",
    "        newM = tf.matmul(tf.reshape(M, [-1, hiddenSize]), tf.reshape(W, [-1, 1]))\n",
    "        \n",
    "        # 对newM做维度转换成[batch_size, time_step]\n",
    "        restoreM = tf.reshape(newM, [-1, config.sequenceLength])\n",
    "        \n",
    "        # 用softmax做归一化处理[batch_size, time_step]\n",
    "        self.alpha = tf.nn.softmax(restoreM)\n",
    "        \n",
    "        # 利用求得的alpha的值对H进行加权求和，用矩阵运算直接操作\n",
    "        r = tf.matmul(tf.transpose(H, [0, 2, 1]), tf.reshape(self.alpha, [-1, config.sequenceLength, 1]))\n",
    "        \n",
    "        # 将三维压缩成二维sequeezeR=[batch_size, hidden_size]\n",
    "        sequeezeR = tf.squeeze(r)\n",
    "        \n",
    "        sentenceRepren = tf.tanh(sequeezeR)\n",
    "        \n",
    "        # 对Attention的输出可以做dropout处理\n",
    "        output = tf.nn.dropout(sentenceRepren, self.dropoutKeepProb)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _normalize(self, wordEmbedding, weights):\n",
    "        \"\"\"\n",
    "        对word embedding 结合权重做标准化处理\n",
    "        \"\"\"\n",
    "        \n",
    "        mean = tf.matmul(weights, wordEmbedding)\n",
    "        print(mean)\n",
    "        powWordEmbedding = tf.pow(wordEmbedding - mean, 2.)\n",
    "        \n",
    "        var = tf.matmul(weights, powWordEmbedding)\n",
    "        print(var)\n",
    "        stddev = tf.sqrt(1e-6 + var)\n",
    "        \n",
    "        return (wordEmbedding - mean) / stddev\n",
    "    \n",
    "    def _addPerturbation(self, embedded, loss):\n",
    "        \"\"\"\n",
    "        添加波动到word embedding\n",
    "        \"\"\"\n",
    "        grad, = tf.gradients(\n",
    "            loss,\n",
    "            embedded,\n",
    "            aggregation_method=tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N)\n",
    "        grad = tf.stop_gradient(grad)\n",
    "        perturb = self._scaleL2(grad, self.config.model.epsilon)\n",
    "        return embedded + perturb\n",
    "    \n",
    "    def _scaleL2(self, x, norm_length):\n",
    "        # shape(x) = (batch, num_timesteps, d)\n",
    "        # Divide x by max(abs(x)) for a numerically stable L2 norm.\n",
    "        # 2norm(x) = a * 2norm(x/a)\n",
    "        # Scale over the full sequence, dims (1, 2)\n",
    "        alpha = tf.reduce_max(tf.abs(x), (1, 2), keepdims=True) + 1e-12\n",
    "        l2_norm = alpha * tf.sqrt(\n",
    "            tf.reduce_sum(tf.pow(x / alpha, 2), (1, 2), keepdims=True) + 1e-6)\n",
    "        x_unit = x / l2_norm\n",
    "        return norm_length * x_unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义性能指标函数\n",
    "\n",
    "def mean(item):\n",
    "    return sum(item) / len(item)\n",
    "\n",
    "\n",
    "def genMetrics(trueY, predY, binaryPredY):\n",
    "    \"\"\"\n",
    "    生成acc和auc值\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(trueY, predY)\n",
    "    accuracy = accuracy_score(trueY, binaryPredY)\n",
    "    precision = precision_score(trueY, binaryPredY)\n",
    "    recall = recall_score(trueY, binaryPredY)\n",
    "    \n",
    "    return round(accuracy, 4), round(auc, 4), round(precision, 4), round(recall, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding/MatMul:0\", shape=(1, 200), dtype=float32)\n",
      "Tensor(\"embedding/MatMul_1:0\", shape=(1, 200), dtype=float32)\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/kernel:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/kernel:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/bias:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/fw/lstm_cell/bias:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/fw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/kernel:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/kernel_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/kernel:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/kernel_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/bias:0/grad/hist is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/bias_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/bi-lstm/bw/lstm_cell/bias:0/grad/sparsity is illegal; using Bi-LSTM/bi-lstm/bw/lstm_cell/bias_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/Attention/Variable:0/grad/hist is illegal; using loss/Bi-LSTM/Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/Attention/Variable:0/grad/sparsity is illegal; using loss/Bi-LSTM/Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/outputW:0/grad/hist is illegal; using Bi-LSTM/outputW_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name Bi-LSTM/outputW:0/grad/sparsity is illegal; using Bi-LSTM/outputW_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/output/outputB:0/grad/hist is illegal; using loss/Bi-LSTM/output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name loss/Bi-LSTM/output/outputB:0/grad/sparsity is illegal; using loss/Bi-LSTM/output/outputB_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/Attention/Variable:0/grad/hist is illegal; using perturLoss/Bi-LSTM/Attention/Variable_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/Attention/Variable:0/grad/sparsity is illegal; using perturLoss/Bi-LSTM/Attention/Variable_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/output/outputB:0/grad/hist is illegal; using perturLoss/Bi-LSTM/output/outputB_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name perturLoss/Bi-LSTM/output/outputB:0/grad/sparsity is illegal; using perturLoss/Bi-LSTM/output/outputB_0/grad/sparsity instead.\n",
      "Writing to D:\\pythonRepo\\textClassifier\\adversarialLSTM\\summarys\n",
      "\n",
      "start training model\n",
      "2019-03-06T17:26:32.722225, step: 1, loss: 1.4473552703857422, acc: 0.5078, auc: 0.5243, precision: 0.625, recall: 0.0769\n",
      "2019-03-06T17:26:33.142106, step: 2, loss: 1.7395215034484863, acc: 0.5156, auc: 0.5282, precision: 0.513, recall: 0.9077\n",
      "2019-03-06T17:26:33.474218, step: 3, loss: 1.4722944498062134, acc: 0.4531, auc: 0.5664, precision: 0.4545, recall: 0.0725\n",
      "2019-03-06T17:26:33.799349, step: 4, loss: 1.4628682136535645, acc: 0.4844, auc: 0.5818, precision: 0.75, recall: 0.0857\n",
      "2019-03-06T17:26:34.123482, step: 5, loss: 1.3940163850784302, acc: 0.5703, auc: 0.4874, precision: 0.3333, recall: 0.0784\n",
      "2019-03-06T17:26:34.447619, step: 6, loss: 1.3357218503952026, acc: 0.5, auc: 0.5889, precision: 0.6154, recall: 0.1194\n",
      "2019-03-06T17:26:34.776740, step: 7, loss: 1.448425531387329, acc: 0.5078, auc: 0.4972, precision: 0.7, recall: 0.1972\n",
      "2019-03-06T17:26:35.113838, step: 8, loss: 1.3992984294891357, acc: 0.5078, auc: 0.5616, precision: 0.4286, recall: 0.2586\n",
      "2019-03-06T17:26:35.462905, step: 9, loss: 1.393059492111206, acc: 0.5234, auc: 0.5022, precision: 0.4762, recall: 0.1667\n",
      "2019-03-06T17:26:35.795017, step: 10, loss: 1.3733367919921875, acc: 0.5469, auc: 0.5991, precision: 0.5294, recall: 0.1525\n",
      "2019-03-06T17:26:36.139097, step: 11, loss: 1.4135441780090332, acc: 0.5234, auc: 0.5218, precision: 0.3077, recall: 0.0714\n",
      "2019-03-06T17:26:36.474201, step: 12, loss: 1.341850996017456, acc: 0.5703, auc: 0.6618, precision: 0.8571, recall: 0.1\n",
      "2019-03-06T17:26:36.805315, step: 13, loss: 1.2858319282531738, acc: 0.5391, auc: 0.7404, precision: 0.6667, recall: 0.0333\n",
      "2019-03-06T17:26:37.140419, step: 14, loss: 1.3381505012512207, acc: 0.5078, auc: 0.6457, precision: 0.5, recall: 0.0159\n",
      "2019-03-06T17:26:37.464552, step: 15, loss: 1.346078634262085, acc: 0.4922, auc: 0.621, precision: 0.6667, recall: 0.0597\n",
      "2019-03-06T17:26:37.787688, step: 16, loss: 1.2352454662322998, acc: 0.5625, auc: 0.749, precision: 0.7692, recall: 0.1587\n",
      "2019-03-06T17:26:38.112819, step: 17, loss: 1.303166151046753, acc: 0.6016, auc: 0.7019, precision: 0.7241, recall: 0.3281\n",
      "2019-03-06T17:26:38.437950, step: 18, loss: 1.2247974872589111, acc: 0.6641, auc: 0.7613, precision: 0.7547, recall: 0.5714\n",
      "2019-03-06T17:26:38.761085, step: 19, loss: 1.2494540214538574, acc: 0.6016, auc: 0.7009, precision: 0.6964, recall: 0.5342\n",
      "2019-03-06T17:26:39.086216, step: 20, loss: 1.3058147430419922, acc: 0.7188, auc: 0.7793, precision: 0.6667, recall: 0.6909\n",
      "2019-03-06T17:26:39.410349, step: 21, loss: 1.213831901550293, acc: 0.6641, auc: 0.7732, precision: 0.7143, recall: 0.4918\n",
      "2019-03-06T17:26:39.736477, step: 22, loss: 1.1957341432571411, acc: 0.6094, auc: 0.7463, precision: 0.75, recall: 0.3281\n",
      "2019-03-06T17:26:40.059613, step: 23, loss: 1.2033226490020752, acc: 0.5938, auc: 0.7299, precision: 0.8667, recall: 0.2063\n",
      "2019-03-06T17:26:40.388765, step: 24, loss: 1.2088325023651123, acc: 0.6016, auc: 0.7517, precision: 0.8125, recall: 0.2131\n",
      "2019-03-06T17:26:40.712867, step: 25, loss: 1.1426522731781006, acc: 0.6094, auc: 0.839, precision: 1.0, recall: 0.2424\n",
      "2019-03-06T17:26:41.035005, step: 26, loss: 1.1297929286956787, acc: 0.6484, auc: 0.8148, precision: 0.9474, recall: 0.2903\n",
      "2019-03-06T17:26:41.363127, step: 27, loss: 1.0971622467041016, acc: 0.6328, auc: 0.809, precision: 0.8333, recall: 0.4225\n",
      "2019-03-06T17:26:41.686268, step: 28, loss: 1.0220062732696533, acc: 0.7031, auc: 0.8677, precision: 0.9333, recall: 0.4375\n",
      "2019-03-06T17:26:42.012426, step: 29, loss: 0.9783099889755249, acc: 0.8125, auc: 0.8787, precision: 0.8852, recall: 0.7606\n",
      "2019-03-06T17:26:42.338524, step: 30, loss: 1.0513076782226562, acc: 0.7578, auc: 0.8728, precision: 0.7544, recall: 0.7167\n",
      "2019-03-06T17:26:42.660663, step: 31, loss: 1.071228265762329, acc: 0.7969, auc: 0.879, precision: 0.7241, recall: 0.8077\n",
      "2019-03-06T17:26:42.982801, step: 32, loss: 1.0059829950332642, acc: 0.7344, auc: 0.8305, precision: 0.807, recall: 0.6667\n",
      "2019-03-06T17:26:43.308929, step: 33, loss: 0.9894968271255493, acc: 0.7578, auc: 0.8519, precision: 0.8596, recall: 0.6806\n",
      "2019-03-06T17:26:43.632065, step: 34, loss: 0.9773646593093872, acc: 0.8047, auc: 0.8703, precision: 0.8846, recall: 0.7077\n",
      "2019-03-06T17:26:43.956199, step: 35, loss: 1.0761758089065552, acc: 0.6953, auc: 0.8069, precision: 0.7895, recall: 0.4918\n",
      "2019-03-06T17:26:44.280332, step: 36, loss: 1.0382583141326904, acc: 0.6875, auc: 0.8348, precision: 0.8125, recall: 0.4333\n",
      "2019-03-06T17:26:44.606460, step: 37, loss: 1.1762816905975342, acc: 0.6406, auc: 0.8008, precision: 0.8, recall: 0.375\n",
      "2019-03-06T17:26:44.935580, step: 38, loss: 0.7546905279159546, acc: 0.8125, auc: 0.9203, precision: 0.9038, recall: 0.7121\n",
      "2019-03-06T17:26:45.258715, step: 39, loss: 0.7931004762649536, acc: 0.7812, auc: 0.9065, precision: 0.8723, recall: 0.6508\n",
      "2019-03-06T17:26:45.583846, step: 40, loss: 0.9764833450317383, acc: 0.8047, auc: 0.8645, precision: 0.7895, recall: 0.7759\n",
      "2019-03-06T17:26:45.910972, step: 41, loss: 0.9566710591316223, acc: 0.7812, auc: 0.86, precision: 0.8421, recall: 0.7164\n",
      "2019-03-06T17:26:46.234107, step: 42, loss: 1.0365748405456543, acc: 0.7656, auc: 0.8416, precision: 0.6719, recall: 0.8269\n",
      "2019-03-06T17:26:46.562230, step: 43, loss: 0.7346909046173096, acc: 0.8438, auc: 0.9258, precision: 0.8793, recall: 0.7969\n",
      "2019-03-06T17:26:46.892351, step: 44, loss: 0.8037680387496948, acc: 0.8047, auc: 0.8998, precision: 0.94, recall: 0.6812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T17:26:47.219477, step: 45, loss: 0.8660566210746765, acc: 0.7578, auc: 0.8928, precision: 0.8913, recall: 0.6119\n",
      "2019-03-06T17:26:47.547599, step: 46, loss: 0.8819922208786011, acc: 0.7734, auc: 0.8801, precision: 0.8444, recall: 0.6333\n",
      "2019-03-06T17:26:47.872759, step: 47, loss: 0.6891520619392395, acc: 0.8281, auc: 0.9314, precision: 0.913, recall: 0.7\n",
      "2019-03-06T17:26:48.202848, step: 48, loss: 0.8492707014083862, acc: 0.7969, auc: 0.8922, precision: 0.8909, recall: 0.7101\n",
      "2019-03-06T17:26:48.527978, step: 49, loss: 0.953742504119873, acc: 0.7578, auc: 0.8643, precision: 0.7826, recall: 0.7714\n",
      "2019-03-06T17:26:48.853109, step: 50, loss: 0.7323314547538757, acc: 0.8438, auc: 0.9168, precision: 0.8841, recall: 0.8356\n",
      "2019-03-06T17:26:49.175247, step: 51, loss: 0.7496187686920166, acc: 0.8359, auc: 0.9208, precision: 0.8689, recall: 0.803\n",
      "2019-03-06T17:26:49.523317, step: 52, loss: 0.8362205028533936, acc: 0.8047, auc: 0.904, precision: 0.8125, recall: 0.8\n",
      "2019-03-06T17:26:49.849444, step: 53, loss: 0.8298002481460571, acc: 0.8438, auc: 0.9127, precision: 0.8, recall: 0.8571\n",
      "2019-03-06T17:26:50.172581, step: 54, loss: 0.793290376663208, acc: 0.8125, auc: 0.9111, precision: 0.8475, recall: 0.7692\n",
      "2019-03-06T17:26:50.498708, step: 55, loss: 0.7762449979782104, acc: 0.7891, auc: 0.9023, precision: 0.8958, recall: 0.6615\n",
      "2019-03-06T17:26:50.824837, step: 56, loss: 0.7289060354232788, acc: 0.8281, auc: 0.9308, precision: 0.9333, recall: 0.6885\n",
      "2019-03-06T17:26:51.151962, step: 57, loss: 0.8328616619110107, acc: 0.7969, auc: 0.9042, precision: 0.9216, recall: 0.6812\n",
      "2019-03-06T17:26:51.474944, step: 58, loss: 0.7887093424797058, acc: 0.8125, auc: 0.8988, precision: 0.8696, recall: 0.6897\n",
      "2019-03-06T17:26:51.803899, step: 59, loss: 0.5794283151626587, acc: 0.8516, auc: 0.9587, precision: 0.8772, recall: 0.8065\n",
      "2019-03-06T17:26:52.129030, step: 60, loss: 0.6159770488739014, acc: 0.875, auc: 0.9538, precision: 0.8594, recall: 0.8871\n",
      "2019-03-06T17:26:52.454165, step: 61, loss: 0.7511181831359863, acc: 0.8438, auc: 0.9147, precision: 0.875, recall: 0.7903\n",
      "2019-03-06T17:26:52.779296, step: 62, loss: 0.7116798162460327, acc: 0.8359, auc: 0.9283, precision: 0.8226, recall: 0.8361\n",
      "2019-03-06T17:26:53.102433, step: 63, loss: 0.8269655704498291, acc: 0.8125, auc: 0.8993, precision: 0.8594, recall: 0.7857\n",
      "2019-03-06T17:26:53.425568, step: 64, loss: 0.631223201751709, acc: 0.8828, auc: 0.9428, precision: 0.9074, recall: 0.8305\n",
      "2019-03-06T17:26:53.751696, step: 65, loss: 0.5602589845657349, acc: 0.9219, auc: 0.9688, precision: 0.9821, recall: 0.8594\n",
      "2019-03-06T17:26:54.077824, step: 66, loss: 0.7313845157623291, acc: 0.8438, auc: 0.9154, precision: 0.8966, recall: 0.7879\n",
      "2019-03-06T17:26:54.404949, step: 67, loss: 0.7194081544876099, acc: 0.8516, auc: 0.9298, precision: 0.9333, recall: 0.7887\n",
      "2019-03-06T17:26:54.731078, step: 68, loss: 0.75654536485672, acc: 0.8516, auc: 0.9102, precision: 0.8621, recall: 0.8197\n",
      "2019-03-06T17:26:55.062196, step: 69, loss: 0.7867602109909058, acc: 0.8359, auc: 0.9064, precision: 0.9123, recall: 0.7647\n",
      "2019-03-06T17:26:55.386329, step: 70, loss: 0.7878998517990112, acc: 0.8359, auc: 0.9004, precision: 0.8644, recall: 0.7969\n",
      "2019-03-06T17:26:55.714452, step: 71, loss: 0.6013903021812439, acc: 0.8516, auc: 0.955, precision: 0.9167, recall: 0.7458\n",
      "2019-03-06T17:26:56.040580, step: 72, loss: 0.7011021375656128, acc: 0.8516, auc: 0.9275, precision: 0.8548, recall: 0.8413\n",
      "2019-03-06T17:26:56.364713, step: 73, loss: 0.7253518104553223, acc: 0.8516, auc: 0.9153, precision: 0.8793, recall: 0.8095\n",
      "2019-03-06T17:26:56.683859, step: 74, loss: 0.8543796539306641, acc: 0.8203, auc: 0.8881, precision: 0.8644, recall: 0.7727\n",
      "2019-03-06T17:26:57.009988, step: 75, loss: 0.610283374786377, acc: 0.8906, auc: 0.9414, precision: 0.9474, recall: 0.8308\n",
      "2019-03-06T17:26:57.335118, step: 76, loss: 0.5632268786430359, acc: 0.8672, auc: 0.9555, precision: 0.9403, recall: 0.8289\n",
      "2019-03-06T17:26:57.662279, step: 77, loss: 0.531118631362915, acc: 0.8828, auc: 0.9622, precision: 0.9155, recall: 0.8784\n",
      "2019-03-06T17:26:57.990366, step: 78, loss: 0.7571763396263123, acc: 0.8438, auc: 0.9153, precision: 0.8413, recall: 0.8413\n",
      "2019-03-06T17:26:58.313502, step: 79, loss: 0.6276270151138306, acc: 0.875, auc: 0.9448, precision: 0.8485, recall: 0.9032\n",
      "2019-03-06T17:26:58.643620, step: 80, loss: 0.5163541436195374, acc: 0.8906, auc: 0.9646, precision: 0.9211, recall: 0.8974\n",
      "2019-03-06T17:26:58.967752, step: 81, loss: 0.6716141700744629, acc: 0.8984, auc: 0.9416, precision: 0.8689, recall: 0.9138\n",
      "2019-03-06T17:26:59.298867, step: 82, loss: 0.6122645139694214, acc: 0.8359, auc: 0.9399, precision: 0.8246, recall: 0.8103\n",
      "2019-03-06T17:26:59.625992, step: 83, loss: 0.7835071086883545, acc: 0.8125, auc: 0.914, precision: 0.875, recall: 0.7424\n",
      "2019-03-06T17:26:59.961096, step: 84, loss: 0.6398310661315918, acc: 0.8594, auc: 0.9376, precision: 0.9057, recall: 0.7869\n",
      "2019-03-06T17:27:00.295203, step: 85, loss: 0.5305187702178955, acc: 0.8594, auc: 0.9706, precision: 0.9818, recall: 0.7606\n",
      "2019-03-06T17:27:00.625320, step: 86, loss: 0.6913962364196777, acc: 0.8047, auc: 0.9406, precision: 0.902, recall: 0.697\n",
      "2019-03-06T17:27:00.949454, step: 87, loss: 0.6420642137527466, acc: 0.8906, auc: 0.9388, precision: 0.9206, recall: 0.8657\n",
      "2019-03-06T17:27:01.280568, step: 88, loss: 0.5829033255577087, acc: 0.9062, auc: 0.9614, precision: 0.8906, recall: 0.9194\n",
      "2019-03-06T17:27:01.622654, step: 89, loss: 0.7868779301643372, acc: 0.8281, auc: 0.9236, precision: 0.7945, recall: 0.8923\n",
      "2019-03-06T17:27:01.968728, step: 90, loss: 0.7559879422187805, acc: 0.8281, auc: 0.9189, precision: 0.8387, recall: 0.8125\n",
      "2019-03-06T17:27:02.307822, step: 91, loss: 0.9679207801818848, acc: 0.7734, auc: 0.8853, precision: 0.7083, recall: 0.8644\n",
      "2019-03-06T17:27:02.650908, step: 92, loss: 0.5698530673980713, acc: 0.8984, auc: 0.9438, precision: 0.9016, recall: 0.8871\n",
      "2019-03-06T17:27:02.981025, step: 93, loss: 0.558459997177124, acc: 0.8516, auc: 0.9645, precision: 0.9623, recall: 0.75\n",
      "2019-03-06T17:27:03.308151, step: 94, loss: 0.638097882270813, acc: 0.8516, auc: 0.9429, precision: 0.9483, recall: 0.7746\n",
      "2019-03-06T17:27:03.633282, step: 95, loss: 0.7897680997848511, acc: 0.8047, auc: 0.9207, precision: 0.92, recall: 0.6866\n",
      "2019-03-06T17:27:03.959409, step: 96, loss: 0.544059693813324, acc: 0.8828, auc: 0.9516, precision: 0.9444, recall: 0.8095\n",
      "2019-03-06T17:27:04.292519, step: 97, loss: 0.6573758125305176, acc: 0.8672, auc: 0.9354, precision: 0.8906, recall: 0.8507\n",
      "2019-03-06T17:27:04.626625, step: 98, loss: 0.740032434463501, acc: 0.8125, auc: 0.9207, precision: 0.8364, recall: 0.7541\n",
      "2019-03-06T17:27:04.978684, step: 99, loss: 0.7471135854721069, acc: 0.8203, auc: 0.9217, precision: 0.8667, recall: 0.7761\n",
      "2019-03-06T17:27:05.301820, step: 100, loss: 0.8032543659210205, acc: 0.8203, auc: 0.8977, precision: 0.8667, recall: 0.7761\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T17:27:21.903428, step: 100, loss: 0.6574244399865469, acc: 0.8635948717948719, auc: 0.9387307692307691, precision: 0.8582333333333333, recall: 0.8752512820512818\n",
      "2019-03-06T17:27:22.327294, step: 101, loss: 0.637224555015564, acc: 0.8438, auc: 0.9377, precision: 0.8636, recall: 0.8382\n",
      "2019-03-06T17:27:22.696308, step: 102, loss: 0.6755565404891968, acc: 0.8594, auc: 0.9365, precision: 0.8406, recall: 0.8923\n",
      "2019-03-06T17:27:23.102222, step: 103, loss: 0.6838200092315674, acc: 0.8594, auc: 0.936, precision: 0.8833, recall: 0.8281\n",
      "2019-03-06T17:27:23.434334, step: 104, loss: 0.5177892446517944, acc: 0.9141, auc: 0.9618, precision: 0.9242, recall: 0.9104\n",
      "2019-03-06T17:27:23.759465, step: 105, loss: 0.6285979151725769, acc: 0.8828, auc: 0.9468, precision: 0.9016, recall: 0.8594\n",
      "2019-03-06T17:27:24.084595, step: 106, loss: 0.6316373348236084, acc: 0.8672, auc: 0.9387, precision: 0.8644, recall: 0.85\n",
      "2019-03-06T17:27:24.415709, step: 107, loss: 0.6564775705337524, acc: 0.8438, auc: 0.9365, precision: 0.9643, recall: 0.75\n",
      "2019-03-06T17:27:24.738845, step: 108, loss: 0.613167405128479, acc: 0.8359, auc: 0.9521, precision: 0.9444, recall: 0.7391\n",
      "2019-03-06T17:27:25.065971, step: 109, loss: 0.5220214128494263, acc: 0.875, auc: 0.9572, precision: 0.9286, recall: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T17:27:25.398083, step: 110, loss: 0.6888757944107056, acc: 0.8672, auc: 0.936, precision: 0.8772, recall: 0.8333\n",
      "2019-03-06T17:27:25.727203, step: 111, loss: 0.7310185432434082, acc: 0.8359, auc: 0.9191, precision: 0.9184, recall: 0.7258\n",
      "2019-03-06T17:27:26.057321, step: 112, loss: 0.4966275990009308, acc: 0.875, auc: 0.9652, precision: 0.902, recall: 0.807\n",
      "2019-03-06T17:27:26.433316, step: 113, loss: 0.5852319598197937, acc: 0.875, auc: 0.9565, precision: 0.98, recall: 0.7656\n",
      "2019-03-06T17:27:26.828259, step: 114, loss: 0.5968042612075806, acc: 0.8516, auc: 0.9516, precision: 0.9123, recall: 0.7879\n",
      "2019-03-06T17:27:27.242152, step: 115, loss: 0.5859141945838928, acc: 0.8828, auc: 0.9524, precision: 0.9, recall: 0.8571\n",
      "2019-03-06T17:27:27.575262, step: 116, loss: 0.7663302421569824, acc: 0.8438, auc: 0.9049, precision: 0.8679, recall: 0.7797\n",
      "2019-03-06T17:27:27.910365, step: 117, loss: 0.7772823572158813, acc: 0.8359, auc: 0.9157, precision: 0.873, recall: 0.8088\n",
      "2019-03-06T17:27:28.242477, step: 118, loss: 0.8617023229598999, acc: 0.8359, auc: 0.903, precision: 0.8065, recall: 0.8475\n",
      "2019-03-06T17:27:28.571597, step: 119, loss: 0.6774479150772095, acc: 0.8516, auc: 0.9387, precision: 0.8462, recall: 0.8\n",
      "2019-03-06T17:27:28.897725, step: 120, loss: 0.5851776003837585, acc: 0.8438, auc: 0.955, precision: 0.9474, recall: 0.7606\n",
      "2019-03-06T17:27:29.224850, step: 121, loss: 0.6600490808486938, acc: 0.8438, auc: 0.9355, precision: 0.8548, recall: 0.8281\n",
      "2019-03-06T17:27:29.548988, step: 122, loss: 0.7694013118743896, acc: 0.8281, auc: 0.9177, precision: 0.8491, recall: 0.7627\n",
      "2019-03-06T17:27:29.881101, step: 123, loss: 0.6094316840171814, acc: 0.8828, auc: 0.9492, precision: 0.9474, recall: 0.8182\n",
      "2019-03-06T17:27:30.236151, step: 124, loss: 0.6268174648284912, acc: 0.8125, auc: 0.9424, precision: 0.8793, recall: 0.75\n",
      "2019-03-06T17:27:30.602173, step: 125, loss: 0.7043228149414062, acc: 0.8125, auc: 0.9248, precision: 0.8913, recall: 0.6833\n",
      "2019-03-06T17:27:30.949244, step: 126, loss: 0.9048559069633484, acc: 0.8047, auc: 0.8872, precision: 0.8545, recall: 0.7344\n",
      "2019-03-06T17:27:31.275372, step: 127, loss: 0.7341384887695312, acc: 0.8359, auc: 0.9225, precision: 0.8679, recall: 0.7667\n",
      "2019-03-06T17:27:31.615463, step: 128, loss: 0.6536570191383362, acc: 0.8672, auc: 0.9422, precision: 0.8806, recall: 0.8676\n",
      "2019-03-06T17:27:31.944583, step: 129, loss: 0.6755551099777222, acc: 0.8516, auc: 0.9313, precision: 0.8036, recall: 0.8491\n",
      "2019-03-06T17:27:32.266721, step: 130, loss: 0.5851806402206421, acc: 0.8438, auc: 0.9474, precision: 0.8939, recall: 0.8194\n",
      "2019-03-06T17:27:32.592849, step: 131, loss: 0.6474958062171936, acc: 0.875, auc: 0.9439, precision: 0.8333, recall: 0.9167\n",
      "2019-03-06T17:27:32.925958, step: 132, loss: 0.7267524003982544, acc: 0.8203, auc: 0.9369, precision: 0.7869, recall: 0.8276\n",
      "2019-03-06T17:27:33.262059, step: 133, loss: 0.4270709753036499, acc: 0.9141, auc: 0.9804, precision: 0.9636, recall: 0.8548\n",
      "2019-03-06T17:27:33.589185, step: 134, loss: 0.5781278610229492, acc: 0.875, auc: 0.9542, precision: 0.8929, recall: 0.8333\n",
      "2019-03-06T17:27:33.924289, step: 135, loss: 0.5096606016159058, acc: 0.8828, auc: 0.9581, precision: 0.9388, recall: 0.7931\n",
      "2019-03-06T17:27:34.261388, step: 136, loss: 0.6610714793205261, acc: 0.8281, auc: 0.9372, precision: 0.918, recall: 0.7671\n",
      "2019-03-06T17:27:34.592502, step: 137, loss: 0.5961139798164368, acc: 0.8516, auc: 0.9451, precision: 0.8, recall: 0.8462\n",
      "2019-03-06T17:27:34.924614, step: 138, loss: 0.7050890922546387, acc: 0.8203, auc: 0.9292, precision: 0.8491, recall: 0.75\n",
      "2019-03-06T17:27:35.249744, step: 139, loss: 0.6825734376907349, acc: 0.8359, auc: 0.9348, precision: 0.9245, recall: 0.7424\n",
      "2019-03-06T17:27:35.576870, step: 140, loss: 0.5999870300292969, acc: 0.8828, auc: 0.9467, precision: 0.9556, recall: 0.7679\n",
      "2019-03-06T17:27:35.902000, step: 141, loss: 0.5902782082557678, acc: 0.8672, auc: 0.9496, precision: 0.8387, recall: 0.8814\n",
      "2019-03-06T17:27:36.238102, step: 142, loss: 0.6854343414306641, acc: 0.8359, auc: 0.9308, precision: 0.9423, recall: 0.7313\n",
      "2019-03-06T17:27:36.568219, step: 143, loss: 0.587335467338562, acc: 0.875, auc: 0.9489, precision: 0.9118, recall: 0.8611\n",
      "2019-03-06T17:27:36.904320, step: 144, loss: 0.6434119939804077, acc: 0.8594, auc: 0.9315, precision: 0.8868, recall: 0.7966\n",
      "2019-03-06T17:27:37.260369, step: 145, loss: 0.5543471574783325, acc: 0.8906, auc: 0.956, precision: 0.8636, recall: 0.9194\n",
      "2019-03-06T17:27:37.737094, step: 146, loss: 0.6711724400520325, acc: 0.8516, auc: 0.9321, precision: 0.8772, recall: 0.8065\n",
      "2019-03-06T17:27:38.101120, step: 147, loss: 0.8118354082107544, acc: 0.8047, auc: 0.9107, precision: 0.8214, recall: 0.7541\n",
      "2019-03-06T17:27:38.426251, step: 148, loss: 0.6668152809143066, acc: 0.8516, auc: 0.9347, precision: 0.9, recall: 0.7627\n",
      "2019-03-06T17:27:38.754373, step: 149, loss: 0.7702310085296631, acc: 0.7969, auc: 0.9187, precision: 0.88, recall: 0.6875\n",
      "2019-03-06T17:27:39.079533, step: 150, loss: 0.6756513714790344, acc: 0.8672, auc: 0.9233, precision: 0.9592, recall: 0.7581\n",
      "2019-03-06T17:27:39.414608, step: 151, loss: 0.4625217318534851, acc: 0.8906, auc: 0.9735, precision: 0.95, recall: 0.8382\n",
      "2019-03-06T17:27:39.745755, step: 152, loss: 0.6661549210548401, acc: 0.8125, auc: 0.9295, precision: 0.8571, recall: 0.7826\n",
      "2019-03-06T17:27:40.075840, step: 153, loss: 0.5436389446258545, acc: 0.8906, auc: 0.9492, precision: 0.9155, recall: 0.8904\n",
      "2019-03-06T17:27:40.407952, step: 154, loss: 0.8335364460945129, acc: 0.8359, auc: 0.8993, precision: 0.8571, recall: 0.8182\n",
      "2019-03-06T17:27:40.736074, step: 155, loss: 0.5060030221939087, acc: 0.9141, auc: 0.9585, precision: 0.9649, recall: 0.8594\n",
      "2019-03-06T17:27:41.060208, step: 156, loss: 0.46899354457855225, acc: 0.9219, auc: 0.9668, precision: 0.9355, recall: 0.9062\n",
      "start training model\n",
      "2019-03-06T17:27:41.417253, step: 157, loss: 0.4267845153808594, acc: 0.9531, auc: 0.9843, precision: 0.9365, recall: 0.9672\n",
      "2019-03-06T17:27:41.739391, step: 158, loss: 0.5845301151275635, acc: 0.8828, auc: 0.9496, precision: 0.8971, recall: 0.8841\n",
      "2019-03-06T17:27:42.066517, step: 159, loss: 0.4899587631225586, acc: 0.8828, auc: 0.9653, precision: 0.9048, recall: 0.8636\n",
      "2019-03-06T17:27:42.400623, step: 160, loss: 0.7322555184364319, acc: 0.8516, auc: 0.9224, precision: 0.8955, recall: 0.8333\n",
      "2019-03-06T17:27:42.737722, step: 161, loss: 0.6133246421813965, acc: 0.8516, auc: 0.9497, precision: 0.8169, recall: 0.9062\n",
      "2019-03-06T17:27:43.060858, step: 162, loss: 0.8114805817604065, acc: 0.8125, auc: 0.8961, precision: 0.8571, recall: 0.7826\n",
      "2019-03-06T17:27:43.391005, step: 163, loss: 0.5980874300003052, acc: 0.8828, auc: 0.9533, precision: 0.918, recall: 0.8485\n",
      "2019-03-06T17:27:43.719098, step: 164, loss: 0.5620737075805664, acc: 0.8594, auc: 0.9504, precision: 0.8936, recall: 0.7636\n",
      "2019-03-06T17:27:44.042234, step: 165, loss: 0.6268497705459595, acc: 0.8438, auc: 0.9365, precision: 0.9091, recall: 0.7692\n",
      "2019-03-06T17:27:44.371353, step: 166, loss: 0.5382232666015625, acc: 0.8672, auc: 0.9679, precision: 0.9821, recall: 0.7746\n",
      "2019-03-06T17:27:44.714437, step: 167, loss: 0.6059197783470154, acc: 0.8438, auc: 0.9432, precision: 0.9014, recall: 0.8312\n",
      "2019-03-06T17:27:45.044553, step: 168, loss: 0.6129375696182251, acc: 0.8828, auc: 0.9449, precision: 0.9107, recall: 0.8361\n",
      "2019-03-06T17:27:45.372676, step: 169, loss: 0.5849683284759521, acc: 0.8906, auc: 0.9486, precision: 0.8909, recall: 0.8596\n",
      "2019-03-06T17:27:45.695812, step: 170, loss: 0.5506690144538879, acc: 0.8828, auc: 0.9591, precision: 0.8525, recall: 0.8966\n",
      "2019-03-06T17:27:46.021940, step: 171, loss: 0.590894877910614, acc: 0.875, auc: 0.9504, precision: 0.8615, recall: 0.8889\n",
      "2019-03-06T17:27:46.356047, step: 172, loss: 0.615303635597229, acc: 0.8672, auc: 0.9426, precision: 0.8955, recall: 0.8571\n",
      "2019-03-06T17:27:46.734035, step: 173, loss: 0.5278691053390503, acc: 0.8906, auc: 0.9566, precision: 0.9219, recall: 0.8676\n",
      "2019-03-06T17:27:47.135961, step: 174, loss: 0.5794639587402344, acc: 0.8594, auc: 0.9534, precision: 0.9643, recall: 0.7714\n",
      "2019-03-06T17:27:47.516943, step: 175, loss: 0.6434890031814575, acc: 0.8594, auc: 0.9397, precision: 0.9167, recall: 0.8088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T17:27:47.952777, step: 176, loss: 0.48007094860076904, acc: 0.9141, auc: 0.9686, precision: 0.9444, recall: 0.8644\n",
      "2019-03-06T17:27:48.296857, step: 177, loss: 0.49129435420036316, acc: 0.9062, auc: 0.9714, precision: 0.9194, recall: 0.8906\n",
      "2019-03-06T17:27:48.652905, step: 178, loss: 0.4345542788505554, acc: 0.9219, auc: 0.9783, precision: 0.9355, recall: 0.9062\n",
      "2019-03-06T17:27:48.991001, step: 179, loss: 0.505531370639801, acc: 0.8828, auc: 0.9587, precision: 0.9, recall: 0.8873\n",
      "2019-03-06T17:27:49.317128, step: 180, loss: 0.6076570749282837, acc: 0.8828, auc: 0.9404, precision: 0.963, recall: 0.8\n",
      "2019-03-06T17:27:49.644254, step: 181, loss: 0.6867895126342773, acc: 0.8281, auc: 0.9328, precision: 0.8644, recall: 0.7846\n",
      "2019-03-06T17:27:49.969384, step: 182, loss: 0.5382500290870667, acc: 0.8672, auc: 0.9616, precision: 0.8438, recall: 0.8852\n",
      "2019-03-06T17:27:50.295513, step: 183, loss: 0.6421418190002441, acc: 0.875, auc: 0.9345, precision: 0.8448, recall: 0.875\n",
      "2019-03-06T17:27:50.626631, step: 184, loss: 0.5818794965744019, acc: 0.8984, auc: 0.9658, precision: 0.8358, recall: 0.9655\n",
      "2019-03-06T17:27:50.954754, step: 185, loss: 0.4865577816963196, acc: 0.8516, auc: 0.9653, precision: 0.94, recall: 0.746\n",
      "2019-03-06T17:27:51.278887, step: 186, loss: 0.872829794883728, acc: 0.7891, auc: 0.9169, precision: 0.902, recall: 0.6765\n",
      "2019-03-06T17:27:51.606012, step: 187, loss: 0.43044155836105347, acc: 0.875, auc: 0.978, precision: 0.9804, recall: 0.7692\n",
      "2019-03-06T17:27:51.937127, step: 188, loss: 0.6054046154022217, acc: 0.8359, auc: 0.9468, precision: 0.92, recall: 0.7302\n",
      "2019-03-06T17:27:52.262257, step: 189, loss: 0.6323306560516357, acc: 0.8203, auc: 0.9401, precision: 0.9149, recall: 0.6935\n",
      "2019-03-06T17:27:52.588385, step: 190, loss: 0.5321765542030334, acc: 0.875, auc: 0.956, precision: 0.8923, recall: 0.8657\n",
      "2019-03-06T17:27:52.921495, step: 191, loss: 0.5117737054824829, acc: 0.8984, auc: 0.9708, precision: 0.8615, recall: 0.9333\n",
      "2019-03-06T17:27:53.248624, step: 192, loss: 0.7357382774353027, acc: 0.8281, auc: 0.9212, precision: 0.8305, recall: 0.8033\n",
      "2019-03-06T17:27:53.577744, step: 193, loss: 0.5936881303787231, acc: 0.8906, auc: 0.9467, precision: 0.9333, recall: 0.8485\n",
      "2019-03-06T17:27:53.911851, step: 194, loss: 0.5600007772445679, acc: 0.8516, auc: 0.9521, precision: 0.8525, recall: 0.8387\n",
      "2019-03-06T17:27:54.240971, step: 195, loss: 0.5565990209579468, acc: 0.8984, auc: 0.9516, precision: 0.8955, recall: 0.9091\n",
      "2019-03-06T17:27:54.570091, step: 196, loss: 0.5194085836410522, acc: 0.8594, auc: 0.9668, precision: 0.96, recall: 0.75\n",
      "2019-03-06T17:27:54.900208, step: 197, loss: 0.6555817127227783, acc: 0.8672, auc: 0.9338, precision: 0.8852, recall: 0.8438\n",
      "2019-03-06T17:27:55.224341, step: 198, loss: 0.6438523530960083, acc: 0.8281, auc: 0.937, precision: 0.8868, recall: 0.746\n",
      "2019-03-06T17:27:55.552463, step: 199, loss: 0.5339725017547607, acc: 0.8984, auc: 0.9599, precision: 0.9524, recall: 0.8571\n",
      "2019-03-06T17:27:55.880586, step: 200, loss: 0.6682679057121277, acc: 0.8516, auc: 0.9336, precision: 0.8036, recall: 0.8491\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T17:28:08.570657, step: 200, loss: 0.611860080407216, acc: 0.8665820512820513, auc: 0.944930769230769, precision: 0.8964717948717947, recall: 0.8332410256410255\n",
      "2019-03-06T17:28:08.897782, step: 201, loss: 0.5492503643035889, acc: 0.8906, auc: 0.9501, precision: 0.9492, recall: 0.8358\n",
      "2019-03-06T17:28:09.220918, step: 202, loss: 0.591515302658081, acc: 0.875, auc: 0.9534, precision: 0.9118, recall: 0.8611\n",
      "2019-03-06T17:28:09.550039, step: 203, loss: 0.5094748735427856, acc: 0.9062, auc: 0.9564, precision: 0.9265, recall: 0.9\n",
      "2019-03-06T17:28:09.883147, step: 204, loss: 0.7298675775527954, acc: 0.8203, auc: 0.9157, precision: 0.8113, recall: 0.7679\n",
      "2019-03-06T17:28:10.209275, step: 205, loss: 0.4891657829284668, acc: 0.8906, auc: 0.9631, precision: 0.9048, recall: 0.8769\n",
      "2019-03-06T17:28:10.600232, step: 206, loss: 0.6191732287406921, acc: 0.8672, auc: 0.9415, precision: 0.8889, recall: 0.8136\n",
      "2019-03-06T17:28:11.069975, step: 207, loss: 0.5368741750717163, acc: 0.9297, auc: 0.9575, precision: 0.9508, recall: 0.9062\n",
      "2019-03-06T17:28:11.398096, step: 208, loss: 0.4902817904949188, acc: 0.9062, auc: 0.9591, precision: 0.9265, recall: 0.9\n",
      "2019-03-06T17:28:11.720235, step: 209, loss: 0.5319108963012695, acc: 0.8984, auc: 0.9592, precision: 0.8889, recall: 0.8727\n",
      "2019-03-06T17:28:12.044368, step: 210, loss: 0.347870409488678, acc: 0.9297, auc: 0.9854, precision: 0.9565, recall: 0.9167\n",
      "2019-03-06T17:28:12.367504, step: 211, loss: 0.7877607345581055, acc: 0.8516, auc: 0.9135, precision: 0.8644, recall: 0.8226\n",
      "2019-03-06T17:28:12.689643, step: 212, loss: 0.5316997766494751, acc: 0.9062, auc: 0.9602, precision: 0.963, recall: 0.8387\n",
      "2019-03-06T17:28:13.020758, step: 213, loss: 0.5364784598350525, acc: 0.8828, auc: 0.9553, precision: 0.875, recall: 0.8596\n",
      "2019-03-06T17:28:13.417696, step: 214, loss: 0.49149805307388306, acc: 0.9297, auc: 0.9645, precision: 0.9434, recall: 0.8929\n",
      "2019-03-06T17:28:13.789701, step: 215, loss: 0.5910321474075317, acc: 0.8672, auc: 0.9475, precision: 0.9492, recall: 0.8\n",
      "2019-03-06T17:28:14.153728, step: 216, loss: 0.5424423217773438, acc: 0.9062, auc: 0.9504, precision: 0.918, recall: 0.8889\n",
      "2019-03-06T17:28:14.497310, step: 217, loss: 0.5296058654785156, acc: 0.8906, auc: 0.9603, precision: 0.8485, recall: 0.9333\n",
      "2019-03-06T17:28:14.875300, step: 218, loss: 0.5382631421089172, acc: 0.8594, auc: 0.957, precision: 0.8868, recall: 0.7966\n",
      "2019-03-06T17:28:15.348036, step: 219, loss: 0.4708772301673889, acc: 0.8906, auc: 0.9586, precision: 0.9074, recall: 0.8448\n",
      "2019-03-06T17:28:15.672169, step: 220, loss: 0.46136292815208435, acc: 0.9375, auc: 0.9716, precision: 0.9818, recall: 0.8852\n",
      "2019-03-06T17:28:15.999294, step: 221, loss: 0.6950026154518127, acc: 0.8672, auc: 0.9233, precision: 0.9, recall: 0.8308\n",
      "2019-03-06T17:28:16.327416, step: 222, loss: 0.5444762110710144, acc: 0.875, auc: 0.9575, precision: 0.9032, recall: 0.8485\n",
      "2019-03-06T17:28:16.652579, step: 223, loss: 0.6474565267562866, acc: 0.8594, auc: 0.9425, precision: 0.8889, recall: 0.8358\n",
      "2019-03-06T17:28:16.979672, step: 224, loss: 0.559783935546875, acc: 0.8906, auc: 0.9542, precision: 0.9242, recall: 0.8714\n",
      "2019-03-06T17:28:17.311784, step: 225, loss: 0.4661114811897278, acc: 0.875, auc: 0.9643, precision: 0.8889, recall: 0.8615\n",
      "2019-03-06T17:28:17.760584, step: 226, loss: 0.5886926054954529, acc: 0.8672, auc: 0.9479, precision: 0.8657, recall: 0.8788\n",
      "2019-03-06T17:28:18.096685, step: 227, loss: 0.5991581082344055, acc: 0.8984, auc: 0.9455, precision: 0.9062, recall: 0.8923\n",
      "2019-03-06T17:28:18.440796, step: 228, loss: 0.7076616287231445, acc: 0.8672, auc: 0.9293, precision: 0.9333, recall: 0.8116\n",
      "2019-03-06T17:28:18.844686, step: 229, loss: 0.48723700642585754, acc: 0.8828, auc: 0.9662, precision: 0.9649, recall: 0.8088\n",
      "2019-03-06T17:28:19.205720, step: 230, loss: 0.4764563739299774, acc: 0.9062, auc: 0.966, precision: 0.9531, recall: 0.8714\n",
      "2019-03-06T17:28:19.649535, step: 231, loss: 0.5224186182022095, acc: 0.8984, auc: 0.9562, precision: 0.9184, recall: 0.8333\n",
      "2019-03-06T17:28:20.021538, step: 232, loss: 0.45385339856147766, acc: 0.9219, auc: 0.9682, precision: 0.9688, recall: 0.8857\n",
      "2019-03-06T17:28:20.348664, step: 233, loss: 0.6008634567260742, acc: 0.875, auc: 0.9443, precision: 0.875, recall: 0.9\n",
      "2019-03-06T17:28:20.675789, step: 234, loss: 0.6832011938095093, acc: 0.8672, auc: 0.9397, precision: 0.8333, recall: 0.8772\n",
      "2019-03-06T17:28:20.996931, step: 235, loss: 0.6163783073425293, acc: 0.875, auc: 0.9426, precision: 0.9643, recall: 0.7941\n",
      "2019-03-06T17:28:21.319069, step: 236, loss: 0.38970327377319336, acc: 0.9219, auc: 0.9785, precision: 0.9828, recall: 0.8636\n",
      "2019-03-06T17:28:21.641208, step: 237, loss: 0.6652880907058716, acc: 0.8906, auc: 0.9365, precision: 0.8667, recall: 0.8966\n",
      "2019-03-06T17:28:21.962349, step: 238, loss: 0.6299642324447632, acc: 0.8359, auc: 0.9343, precision: 0.8852, recall: 0.7941\n",
      "2019-03-06T17:28:22.288477, step: 239, loss: 0.6485626697540283, acc: 0.8359, auc: 0.9398, precision: 0.8276, recall: 0.8136\n",
      "2019-03-06T17:28:22.614605, step: 240, loss: 0.6243115663528442, acc: 0.8906, auc: 0.9387, precision: 0.9464, recall: 0.8281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T17:28:22.940733, step: 241, loss: 0.6984232664108276, acc: 0.8594, auc: 0.9272, precision: 0.9, recall: 0.8182\n",
      "2019-03-06T17:28:23.263869, step: 242, loss: 0.5827597379684448, acc: 0.8516, auc: 0.9521, precision: 0.9388, recall: 0.7419\n",
      "2019-03-06T17:28:23.588002, step: 243, loss: 0.7520190477371216, acc: 0.8047, auc: 0.9292, precision: 0.88, recall: 0.6984\n",
      "2019-03-06T17:28:23.914130, step: 244, loss: 0.6656299829483032, acc: 0.8594, auc: 0.9356, precision: 0.9057, recall: 0.7869\n",
      "2019-03-06T17:28:24.238263, step: 245, loss: 0.5376019477844238, acc: 0.8828, auc: 0.9626, precision: 0.9322, recall: 0.8333\n",
      "2019-03-06T17:28:24.561399, step: 246, loss: 0.4750059247016907, acc: 0.9062, auc: 0.97, precision: 0.9492, recall: 0.8615\n",
      "2019-03-06T17:28:24.889522, step: 247, loss: 0.5112015008926392, acc: 0.9062, auc: 0.9604, precision: 0.931, recall: 0.871\n",
      "2019-03-06T17:28:25.211660, step: 248, loss: 0.7101033926010132, acc: 0.8438, auc: 0.9251, precision: 0.8246, recall: 0.8246\n",
      "2019-03-06T17:28:25.534796, step: 249, loss: 0.5243767499923706, acc: 0.8984, auc: 0.9579, precision: 0.9412, recall: 0.8767\n",
      "2019-03-06T17:28:25.856960, step: 250, loss: 0.5087414979934692, acc: 0.8984, auc: 0.9603, precision: 0.9254, recall: 0.8857\n",
      "2019-03-06T17:28:26.231160, step: 251, loss: 0.6797659993171692, acc: 0.8516, auc: 0.9355, precision: 0.8689, recall: 0.8281\n",
      "2019-03-06T17:28:26.553299, step: 252, loss: 0.6397929787635803, acc: 0.8672, auc: 0.9465, precision: 0.8167, recall: 0.8909\n",
      "2019-03-06T17:28:26.877432, step: 253, loss: 0.7276626229286194, acc: 0.8281, auc: 0.9137, precision: 0.8667, recall: 0.7879\n",
      "2019-03-06T17:28:27.199570, step: 254, loss: 0.5082905292510986, acc: 0.8906, auc: 0.9614, precision: 0.9298, recall: 0.8413\n",
      "2019-03-06T17:28:27.524730, step: 255, loss: 0.6783227920532227, acc: 0.8438, auc: 0.9425, precision: 0.9434, recall: 0.7463\n",
      "2019-03-06T17:28:27.847837, step: 256, loss: 0.5679775476455688, acc: 0.875, auc: 0.947, precision: 0.931, recall: 0.8182\n",
      "2019-03-06T17:28:28.170973, step: 257, loss: 0.5895138382911682, acc: 0.8828, auc: 0.9483, precision: 0.9412, recall: 0.8\n",
      "2019-03-06T17:28:28.495107, step: 258, loss: 0.5357934236526489, acc: 0.875, auc: 0.9621, precision: 0.9254, recall: 0.8493\n",
      "2019-03-06T17:28:28.817245, step: 259, loss: 0.5218487977981567, acc: 0.875, auc: 0.9582, precision: 0.8689, recall: 0.8689\n",
      "2019-03-06T17:28:29.145367, step: 260, loss: 0.4977875053882599, acc: 0.8828, auc: 0.9666, precision: 0.8929, recall: 0.8475\n",
      "2019-03-06T17:28:29.468503, step: 261, loss: 0.41436541080474854, acc: 0.9219, auc: 0.9824, precision: 0.9672, recall: 0.8806\n",
      "2019-03-06T17:28:29.792637, step: 262, loss: 0.7638986110687256, acc: 0.8047, auc: 0.9044, precision: 0.8462, recall: 0.7857\n",
      "2019-03-06T17:28:30.115772, step: 263, loss: 0.6103711724281311, acc: 0.8828, auc: 0.9423, precision: 0.8485, recall: 0.918\n",
      "2019-03-06T17:28:30.439906, step: 264, loss: 0.5668742656707764, acc: 0.875, auc: 0.9541, precision: 0.9259, recall: 0.8065\n",
      "2019-03-06T17:28:30.767034, step: 265, loss: 0.616585373878479, acc: 0.8516, auc: 0.9404, precision: 0.8596, recall: 0.8167\n",
      "2019-03-06T17:28:31.089169, step: 266, loss: 0.4094371199607849, acc: 0.9141, auc: 0.9807, precision: 1.0, recall: 0.8308\n",
      "2019-03-06T17:28:31.412306, step: 267, loss: 0.6226519346237183, acc: 0.8594, auc: 0.9348, precision: 0.8824, recall: 0.7895\n",
      "2019-03-06T17:28:31.736474, step: 268, loss: 0.48313605785369873, acc: 0.9297, auc: 0.9591, precision: 0.9677, recall: 0.8955\n",
      "2019-03-06T17:28:32.060573, step: 269, loss: 0.36381930112838745, acc: 0.9297, auc: 0.986, precision: 0.9722, recall: 0.9091\n",
      "2019-03-06T17:28:32.383708, step: 270, loss: 0.6259675621986389, acc: 0.8125, auc: 0.9435, precision: 0.8491, recall: 0.7377\n",
      "2019-03-06T17:28:32.707841, step: 271, loss: 0.6173620820045471, acc: 0.875, auc: 0.9471, precision: 0.8793, recall: 0.85\n",
      "2019-03-06T17:28:33.030978, step: 272, loss: 0.5464837551116943, acc: 0.8906, auc: 0.9648, precision: 0.9444, recall: 0.8226\n",
      "2019-03-06T17:28:33.355111, step: 273, loss: 0.5618981122970581, acc: 0.8594, auc: 0.9516, precision: 0.8462, recall: 0.873\n",
      "2019-03-06T17:28:33.679244, step: 274, loss: 0.6282188892364502, acc: 0.8594, auc: 0.9419, precision: 0.8704, recall: 0.8103\n",
      "2019-03-06T17:28:34.003377, step: 275, loss: 0.4424781799316406, acc: 0.9219, auc: 0.9716, precision: 0.918, recall: 0.918\n",
      "2019-03-06T17:28:34.325516, step: 276, loss: 0.5550763607025146, acc: 0.9062, auc: 0.9523, precision: 0.9014, recall: 0.9275\n",
      "2019-03-06T17:28:34.650647, step: 277, loss: 0.5934962034225464, acc: 0.8438, auc: 0.9433, precision: 0.9423, recall: 0.7424\n",
      "2019-03-06T17:28:34.973782, step: 278, loss: 0.5627310276031494, acc: 0.8984, auc: 0.9533, precision: 0.8966, recall: 0.8814\n",
      "2019-03-06T17:28:35.297915, step: 279, loss: 0.5077852010726929, acc: 0.8984, auc: 0.9614, precision: 0.9375, recall: 0.8696\n",
      "2019-03-06T17:28:35.619056, step: 280, loss: 0.5143572092056274, acc: 0.8672, auc: 0.9587, precision: 0.9153, recall: 0.8182\n",
      "2019-03-06T17:28:35.944188, step: 281, loss: 0.6164395213127136, acc: 0.8672, auc: 0.9474, precision: 0.9275, recall: 0.8421\n",
      "2019-03-06T17:28:36.268321, step: 282, loss: 0.6158432960510254, acc: 0.8594, auc: 0.9433, precision: 0.9123, recall: 0.8\n",
      "2019-03-06T17:28:36.593451, step: 283, loss: 0.5573166608810425, acc: 0.875, auc: 0.9502, precision: 0.8871, recall: 0.8594\n",
      "2019-03-06T17:28:36.915590, step: 284, loss: 0.5132033228874207, acc: 0.9141, auc: 0.9642, precision: 0.8889, recall: 0.8696\n",
      "2019-03-06T17:28:37.238726, step: 285, loss: 0.505145788192749, acc: 0.875, auc: 0.96, precision: 0.94, recall: 0.7833\n",
      "2019-03-06T17:28:37.565887, step: 286, loss: 0.5450621843338013, acc: 0.9062, auc: 0.9494, precision: 0.95, recall: 0.8636\n",
      "2019-03-06T17:28:37.889992, step: 287, loss: 0.7243465185165405, acc: 0.8516, auc: 0.923, precision: 0.9355, recall: 0.7945\n",
      "2019-03-06T17:28:38.214126, step: 288, loss: 0.8202234506607056, acc: 0.8281, auc: 0.9039, precision: 0.8276, recall: 0.8\n",
      "2019-03-06T17:28:38.539256, step: 289, loss: 0.6579941511154175, acc: 0.875, auc: 0.9399, precision: 0.9016, recall: 0.8462\n",
      "2019-03-06T17:28:38.865384, step: 290, loss: 0.45706796646118164, acc: 0.8828, auc: 0.9695, precision: 0.8676, recall: 0.9077\n",
      "2019-03-06T17:28:39.190515, step: 291, loss: 0.7109462022781372, acc: 0.8281, auc: 0.9358, precision: 0.8065, recall: 0.8333\n",
      "2019-03-06T17:28:39.513651, step: 292, loss: 0.576485812664032, acc: 0.8828, auc: 0.9421, precision: 0.9365, recall: 0.8429\n",
      "2019-03-06T17:28:39.835790, step: 293, loss: 0.5712561011314392, acc: 0.8906, auc: 0.9471, precision: 0.9057, recall: 0.8421\n",
      "2019-03-06T17:28:40.158925, step: 294, loss: 0.8003594875335693, acc: 0.8047, auc: 0.9212, precision: 0.881, recall: 0.6491\n",
      "2019-03-06T17:28:40.483059, step: 295, loss: 0.5288486480712891, acc: 0.8672, auc: 0.9632, precision: 0.92, recall: 0.7797\n",
      "2019-03-06T17:28:40.809186, step: 296, loss: 0.6027323603630066, acc: 0.8516, auc: 0.9445, precision: 0.9038, recall: 0.7705\n",
      "2019-03-06T17:28:41.133320, step: 297, loss: 0.47719842195510864, acc: 0.8594, auc: 0.9722, precision: 0.9348, recall: 0.7414\n",
      "2019-03-06T17:28:41.458451, step: 298, loss: 0.4120035767555237, acc: 0.8984, auc: 0.9805, precision: 0.9474, recall: 0.8438\n",
      "2019-03-06T17:28:41.785581, step: 299, loss: 0.6004199981689453, acc: 0.8906, auc: 0.9464, precision: 0.8983, recall: 0.8689\n",
      "2019-03-06T17:28:42.112706, step: 300, loss: 0.5938067436218262, acc: 0.8906, auc: 0.9435, precision: 0.9206, recall: 0.8657\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T17:28:54.767395, step: 300, loss: 0.5896612589175885, acc: 0.8772076923076924, auc: 0.9475102564102564, precision: 0.8915000000000003, recall: 0.8641384615384617\n",
      "2019-03-06T17:28:55.089534, step: 301, loss: 0.6113881468772888, acc: 0.8672, auc: 0.9483, precision: 0.9184, recall: 0.7759\n",
      "2019-03-06T17:28:55.417657, step: 302, loss: 0.5039222240447998, acc: 0.9219, auc: 0.9653, precision: 0.9194, recall: 0.9194\n",
      "2019-03-06T17:28:55.740792, step: 303, loss: 0.6005484461784363, acc: 0.8906, auc: 0.9547, precision: 0.8833, recall: 0.8833\n",
      "2019-03-06T17:28:56.063929, step: 304, loss: 0.6851838827133179, acc: 0.8438, auc: 0.9393, precision: 0.806, recall: 0.8852\n",
      "2019-03-06T17:28:56.390056, step: 305, loss: 0.6606817245483398, acc: 0.8516, auc: 0.9424, precision: 0.8095, recall: 0.8793\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T17:28:56.714190, step: 306, loss: 0.6775999665260315, acc: 0.8359, auc: 0.9314, precision: 0.9074, recall: 0.7538\n",
      "2019-03-06T17:28:57.037325, step: 307, loss: 0.6841330528259277, acc: 0.8594, auc: 0.9293, precision: 0.9057, recall: 0.7869\n",
      "2019-03-06T17:28:57.362456, step: 308, loss: 0.6372062563896179, acc: 0.8125, auc: 0.9489, precision: 0.9038, recall: 0.7121\n",
      "2019-03-06T17:28:57.684595, step: 309, loss: 0.7901373505592346, acc: 0.8203, auc: 0.9123, precision: 0.9091, recall: 0.7353\n",
      "2019-03-06T17:28:58.009726, step: 310, loss: 0.5001466274261475, acc: 0.8906, auc: 0.9697, precision: 0.9783, recall: 0.7759\n",
      "2019-03-06T17:28:58.334857, step: 311, loss: 0.5975533127784729, acc: 0.8438, auc: 0.9499, precision: 0.9298, recall: 0.7681\n",
      "2019-03-06T17:28:58.661981, step: 312, loss: 0.5767770409584045, acc: 0.8672, auc: 0.9529, precision: 0.8833, recall: 0.8413\n",
      "start training model\n",
      "2019-03-06T17:28:59.006061, step: 313, loss: 0.48696625232696533, acc: 0.9219, auc: 0.9712, precision: 0.8833, recall: 0.9464\n",
      "2019-03-06T17:28:59.329197, step: 314, loss: 0.5745482444763184, acc: 0.8906, auc: 0.9501, precision: 0.8841, recall: 0.9104\n",
      "2019-03-06T17:28:59.658317, step: 315, loss: 0.6942456960678101, acc: 0.8516, auc: 0.9386, precision: 0.8529, recall: 0.8657\n",
      "2019-03-06T17:28:59.981452, step: 316, loss: 0.5507299900054932, acc: 0.8984, auc: 0.965, precision: 0.8571, recall: 0.931\n",
      "2019-03-06T17:29:00.307581, step: 317, loss: 0.5270761251449585, acc: 0.9141, auc: 0.9665, precision: 0.9296, recall: 0.9167\n",
      "2019-03-06T17:29:00.630717, step: 318, loss: 0.5159283876419067, acc: 0.8594, auc: 0.9621, precision: 0.9388, recall: 0.7541\n",
      "2019-03-06T17:29:00.956844, step: 319, loss: 0.38484689593315125, acc: 0.9297, auc: 0.9837, precision: 0.9623, recall: 0.8793\n",
      "2019-03-06T17:29:01.281976, step: 320, loss: 0.32274818420410156, acc: 0.9453, auc: 0.9939, precision: 0.9831, recall: 0.9062\n",
      "2019-03-06T17:29:01.608103, step: 321, loss: 0.5357482433319092, acc: 0.8516, auc: 0.9659, precision: 0.975, recall: 0.6842\n",
      "2019-03-06T17:29:01.933234, step: 322, loss: 0.5345163345336914, acc: 0.8672, auc: 0.956, precision: 0.9592, recall: 0.7581\n",
      "2019-03-06T17:29:02.257367, step: 323, loss: 0.5314467549324036, acc: 0.875, auc: 0.9567, precision: 0.9091, recall: 0.8197\n",
      "2019-03-06T17:29:02.581500, step: 324, loss: 0.39074069261550903, acc: 0.9531, auc: 0.9841, precision: 0.9828, recall: 0.9194\n",
      "2019-03-06T17:29:02.908626, step: 325, loss: 0.5402886867523193, acc: 0.8828, auc: 0.956, precision: 0.9516, recall: 0.831\n",
      "2019-03-06T17:29:03.234754, step: 326, loss: 0.3628716766834259, acc: 0.9297, auc: 0.983, precision: 0.9298, recall: 0.9138\n",
      "2019-03-06T17:29:03.557890, step: 327, loss: 0.5258567929267883, acc: 0.8828, auc: 0.9579, precision: 0.8571, recall: 0.8727\n",
      "2019-03-06T17:29:03.886012, step: 328, loss: 0.4420035481452942, acc: 0.9062, auc: 0.9695, precision: 0.9322, recall: 0.873\n",
      "2019-03-06T17:29:04.211143, step: 329, loss: 0.5448437929153442, acc: 0.9141, auc: 0.9697, precision: 0.8507, recall: 0.9828\n",
      "2019-03-06T17:29:04.536274, step: 330, loss: 0.46834391355514526, acc: 0.8906, auc: 0.97, precision: 0.9153, recall: 0.8571\n",
      "2019-03-06T17:29:04.859433, step: 331, loss: 0.4245068430900574, acc: 0.8906, auc: 0.9766, precision: 0.9538, recall: 0.8493\n",
      "2019-03-06T17:29:05.184540, step: 332, loss: 0.45593470335006714, acc: 0.9062, auc: 0.9682, precision: 0.9219, recall: 0.8939\n",
      "2019-03-06T17:29:05.508673, step: 333, loss: 0.4189297556877136, acc: 0.9297, auc: 0.9748, precision: 0.95, recall: 0.9048\n",
      "2019-03-06T17:29:05.831809, step: 334, loss: 0.46827608346939087, acc: 0.9141, auc: 0.9634, precision: 0.9615, recall: 0.8475\n",
      "2019-03-06T17:29:06.155942, step: 335, loss: 0.491921067237854, acc: 0.8906, auc: 0.9602, precision: 0.9459, recall: 0.875\n",
      "2019-03-06T17:29:06.482075, step: 336, loss: 0.5407814979553223, acc: 0.8906, auc: 0.9489, precision: 0.913, recall: 0.8077\n",
      "2019-03-06T17:29:06.806208, step: 337, loss: 0.6539774537086487, acc: 0.8594, auc: 0.9372, precision: 0.8689, recall: 0.8413\n",
      "2019-03-06T17:29:07.132336, step: 338, loss: 0.47657158970832825, acc: 0.875, auc: 0.9613, precision: 0.9245, recall: 0.8033\n",
      "2019-03-06T17:29:07.457467, step: 339, loss: 0.5205057859420776, acc: 0.8984, auc: 0.9632, precision: 0.9265, recall: 0.8873\n",
      "2019-03-06T17:29:07.780602, step: 340, loss: 0.7778601050376892, acc: 0.8359, auc: 0.9206, precision: 0.8182, recall: 0.8036\n",
      "2019-03-06T17:29:08.107728, step: 341, loss: 0.5087186098098755, acc: 0.8984, auc: 0.958, precision: 0.9344, recall: 0.8636\n",
      "2019-03-06T17:29:08.432858, step: 342, loss: 0.4839349389076233, acc: 0.8828, auc: 0.959, precision: 0.9219, recall: 0.8551\n",
      "2019-03-06T17:29:08.757989, step: 343, loss: 0.37500643730163574, acc: 0.9062, auc: 0.9785, precision: 0.9324, recall: 0.9079\n",
      "2019-03-06T17:29:09.082122, step: 344, loss: 0.45737117528915405, acc: 0.9297, auc: 0.973, precision: 0.8947, recall: 0.9444\n",
      "2019-03-06T17:29:09.406255, step: 345, loss: 0.4480959177017212, acc: 0.8984, auc: 0.9704, precision: 0.9545, recall: 0.863\n",
      "2019-03-06T17:29:09.729391, step: 346, loss: 0.6405202150344849, acc: 0.8594, auc: 0.9345, precision: 0.8958, recall: 0.7679\n",
      "2019-03-06T17:29:10.054522, step: 347, loss: 0.5283461809158325, acc: 0.8906, auc: 0.9547, precision: 0.8857, recall: 0.9118\n",
      "2019-03-06T17:29:10.378655, step: 348, loss: 0.5695263147354126, acc: 0.8828, auc: 0.9477, precision: 0.9, recall: 0.8571\n",
      "2019-03-06T17:29:10.700794, step: 349, loss: 0.4792204797267914, acc: 0.9141, auc: 0.9692, precision: 0.9219, recall: 0.9077\n",
      "2019-03-06T17:29:11.023930, step: 350, loss: 0.6443691253662109, acc: 0.8438, auc: 0.9358, precision: 0.8704, recall: 0.7833\n",
      "2019-03-06T17:29:11.349060, step: 351, loss: 0.5429821610450745, acc: 0.8672, auc: 0.9626, precision: 0.9492, recall: 0.8\n",
      "2019-03-06T17:29:11.674220, step: 352, loss: 0.5320889949798584, acc: 0.8906, auc: 0.9605, precision: 0.9388, recall: 0.807\n",
      "2019-03-06T17:29:12.000319, step: 353, loss: 0.46107882261276245, acc: 0.8516, auc: 0.966, precision: 0.8772, recall: 0.8065\n",
      "2019-03-06T17:29:12.330436, step: 354, loss: 0.5620376467704773, acc: 0.875, auc: 0.9433, precision: 0.8772, recall: 0.8475\n",
      "2019-03-06T17:29:12.655567, step: 355, loss: 0.37605020403862, acc: 0.8984, auc: 0.9793, precision: 0.9038, recall: 0.8545\n",
      "2019-03-06T17:29:12.978703, step: 356, loss: 0.5957351326942444, acc: 0.8984, auc: 0.9475, precision: 0.9298, recall: 0.8548\n",
      "2019-03-06T17:29:13.306825, step: 357, loss: 0.4612763524055481, acc: 0.9219, auc: 0.9725, precision: 0.9107, recall: 0.9107\n",
      "2019-03-06T17:29:13.630959, step: 358, loss: 0.5328011512756348, acc: 0.8828, auc: 0.9567, precision: 0.8983, recall: 0.8548\n",
      "2019-03-06T17:29:13.953097, step: 359, loss: 0.4783870577812195, acc: 0.9219, auc: 0.9737, precision: 0.9697, recall: 0.8889\n",
      "2019-03-06T17:29:14.279225, step: 360, loss: 0.760460376739502, acc: 0.8516, auc: 0.924, precision: 0.8657, recall: 0.8529\n",
      "2019-03-06T17:29:14.603358, step: 361, loss: 0.6933965682983398, acc: 0.8984, auc: 0.9371, precision: 0.875, recall: 0.918\n",
      "2019-03-06T17:29:14.930484, step: 362, loss: 0.6890819072723389, acc: 0.875, auc: 0.931, precision: 0.8971, recall: 0.8714\n",
      "2019-03-06T17:29:15.256612, step: 363, loss: 0.5590654611587524, acc: 0.8828, auc: 0.9555, precision: 0.8983, recall: 0.8548\n",
      "2019-03-06T17:29:15.580745, step: 364, loss: 0.5533468127250671, acc: 0.8672, auc: 0.9535, precision: 0.9167, recall: 0.8209\n",
      "2019-03-06T17:29:15.902883, step: 365, loss: 0.5469008684158325, acc: 0.875, auc: 0.9645, precision: 0.8704, recall: 0.8393\n",
      "2019-03-06T17:29:16.227017, step: 366, loss: 0.5878914594650269, acc: 0.8828, auc: 0.948, precision: 0.931, recall: 0.8308\n",
      "2019-03-06T17:29:16.554142, step: 367, loss: 0.5213432312011719, acc: 0.9141, auc: 0.9619, precision: 0.9333, recall: 0.8889\n",
      "2019-03-06T17:29:16.876281, step: 368, loss: 0.5354466438293457, acc: 0.8984, auc: 0.9569, precision: 0.9123, recall: 0.8667\n",
      "2019-03-06T17:29:17.201412, step: 369, loss: 0.627650260925293, acc: 0.8672, auc: 0.9404, precision: 0.8936, recall: 0.7778\n",
      "2019-03-06T17:29:17.528537, step: 370, loss: 0.46003419160842896, acc: 0.9453, auc: 0.9734, precision: 0.9677, recall: 0.9231\n",
      "2019-03-06T17:29:17.855662, step: 371, loss: 0.5959577560424805, acc: 0.8516, auc: 0.9482, precision: 0.9608, recall: 0.7424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T17:29:18.179795, step: 372, loss: 0.4683660864830017, acc: 0.875, auc: 0.9757, precision: 0.9833, recall: 0.7973\n",
      "2019-03-06T17:29:18.503929, step: 373, loss: 0.5286277532577515, acc: 0.8984, auc: 0.9607, precision: 0.94, recall: 0.8246\n",
      "2019-03-06T17:29:18.828061, step: 374, loss: 0.46485182642936707, acc: 0.9219, auc: 0.9629, precision: 0.9796, recall: 0.8421\n",
      "2019-03-06T17:29:19.154189, step: 375, loss: 0.46997150778770447, acc: 0.9062, auc: 0.9668, precision: 0.9655, recall: 0.8485\n",
      "2019-03-06T17:29:19.480511, step: 376, loss: 0.6291683912277222, acc: 0.875, auc: 0.9362, precision: 0.8889, recall: 0.8276\n",
      "2019-03-06T17:29:19.804675, step: 377, loss: 0.5394988059997559, acc: 0.9062, auc: 0.9604, precision: 0.8806, recall: 0.9365\n",
      "2019-03-06T17:29:20.129774, step: 378, loss: 0.49553459882736206, acc: 0.9219, auc: 0.969, precision: 0.9286, recall: 0.8966\n",
      "2019-03-06T17:29:20.453908, step: 379, loss: 0.4738694429397583, acc: 0.8984, auc: 0.9664, precision: 0.9028, recall: 0.9155\n",
      "2019-03-06T17:29:20.781062, step: 380, loss: 0.5394502878189087, acc: 0.8984, auc: 0.9675, precision: 0.8592, recall: 0.9531\n",
      "2019-03-06T17:29:21.105170, step: 381, loss: 0.6217726469039917, acc: 0.8672, auc: 0.9454, precision: 0.8333, recall: 0.8772\n",
      "2019-03-06T17:29:21.430301, step: 382, loss: 0.5764272212982178, acc: 0.8906, auc: 0.9518, precision: 0.8841, recall: 0.9104\n",
      "2019-03-06T17:29:21.757427, step: 383, loss: 0.7479515075683594, acc: 0.8359, auc: 0.9262, precision: 0.9535, recall: 0.6833\n",
      "2019-03-06T17:29:22.082557, step: 384, loss: 0.47867050766944885, acc: 0.8906, auc: 0.9665, precision: 0.9483, recall: 0.8333\n",
      "2019-03-06T17:29:22.406690, step: 385, loss: 0.33884039521217346, acc: 0.9219, auc: 0.9875, precision: 0.9821, recall: 0.8594\n",
      "2019-03-06T17:29:22.731820, step: 386, loss: 0.41462111473083496, acc: 0.9141, auc: 0.9738, precision: 0.9623, recall: 0.85\n",
      "2019-03-06T17:29:23.055954, step: 387, loss: 0.648141622543335, acc: 0.8672, auc: 0.9389, precision: 0.9286, recall: 0.8\n",
      "2019-03-06T17:29:23.384077, step: 388, loss: 0.46089667081832886, acc: 0.8828, auc: 0.9721, precision: 0.8955, recall: 0.8824\n",
      "2019-03-06T17:29:23.710204, step: 389, loss: 0.5312821865081787, acc: 0.9062, auc: 0.9519, precision: 0.9167, recall: 0.8871\n",
      "2019-03-06T17:29:24.031346, step: 390, loss: 0.5303919911384583, acc: 0.8906, auc: 0.9521, precision: 0.9231, recall: 0.8696\n",
      "2019-03-06T17:29:24.355480, step: 391, loss: 0.49591606855392456, acc: 0.8906, auc: 0.9699, precision: 0.8507, recall: 0.9344\n",
      "2019-03-06T17:29:24.683602, step: 392, loss: 0.7155013680458069, acc: 0.8438, auc: 0.9222, precision: 0.8475, recall: 0.8197\n",
      "2019-03-06T17:29:25.012726, step: 393, loss: 0.5361711978912354, acc: 0.8984, auc: 0.9595, precision: 0.9062, recall: 0.8923\n",
      "2019-03-06T17:29:25.338854, step: 394, loss: 0.5034929513931274, acc: 0.9219, auc: 0.9624, precision: 0.8971, recall: 0.9531\n",
      "2019-03-06T17:29:25.664982, step: 395, loss: 0.6413069367408752, acc: 0.8672, auc: 0.9305, precision: 0.9038, recall: 0.7966\n",
      "2019-03-06T17:29:25.992107, step: 396, loss: 0.3279135823249817, acc: 0.9453, auc: 0.9885, precision: 0.9508, recall: 0.9355\n",
      "2019-03-06T17:29:26.319232, step: 397, loss: 0.48988187313079834, acc: 0.8828, auc: 0.9624, precision: 0.9322, recall: 0.8333\n",
      "2019-03-06T17:29:26.647355, step: 398, loss: 0.5370179414749146, acc: 0.9062, auc: 0.9571, precision: 0.9583, recall: 0.8214\n",
      "2019-03-06T17:29:26.971489, step: 399, loss: 0.5798539519309998, acc: 0.875, auc: 0.9531, precision: 0.9423, recall: 0.7903\n",
      "2019-03-06T17:29:27.295622, step: 400, loss: 0.5509856939315796, acc: 0.8516, auc: 0.9649, precision: 0.9643, recall: 0.7606\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T17:29:39.887623, step: 400, loss: 0.5852899643091055, acc: 0.8701948717948715, auc: 0.9498461538461539, precision: 0.907151282051282, recall: 0.8285256410256411\n",
      "2019-03-06T17:29:40.211728, step: 401, loss: 0.4193245470523834, acc: 0.9219, auc: 0.9711, precision: 0.9464, recall: 0.8833\n",
      "2019-03-06T17:29:40.535861, step: 402, loss: 0.5369321703910828, acc: 0.8828, auc: 0.9532, precision: 0.9077, recall: 0.8676\n",
      "2019-03-06T17:29:40.862986, step: 403, loss: 0.6316831111907959, acc: 0.8438, auc: 0.9497, precision: 0.7833, recall: 0.8704\n",
      "2019-03-06T17:29:41.186892, step: 404, loss: 0.5047832727432251, acc: 0.8984, auc: 0.9524, precision: 0.9412, recall: 0.8767\n",
      "2019-03-06T17:29:41.515015, step: 405, loss: 0.4383929967880249, acc: 0.9141, auc: 0.9748, precision: 0.9474, recall: 0.871\n",
      "2019-03-06T17:29:41.841143, step: 406, loss: 0.5537257194519043, acc: 0.8828, auc: 0.95, precision: 0.9104, recall: 0.8714\n",
      "2019-03-06T17:29:42.165275, step: 407, loss: 0.5109871625900269, acc: 0.8828, auc: 0.9619, precision: 0.9048, recall: 0.8636\n",
      "2019-03-06T17:29:42.490406, step: 408, loss: 0.5449222922325134, acc: 0.875, auc: 0.9578, precision: 0.9138, recall: 0.8281\n",
      "2019-03-06T17:29:42.820523, step: 409, loss: 0.39834994077682495, acc: 0.9297, auc: 0.9779, precision: 0.9683, recall: 0.8971\n",
      "2019-03-06T17:29:43.144657, step: 410, loss: 0.4875076413154602, acc: 0.8828, auc: 0.9655, precision: 0.9231, recall: 0.8571\n",
      "2019-03-06T17:29:43.469788, step: 411, loss: 0.3009665906429291, acc: 0.9609, auc: 0.9939, precision: 0.9508, recall: 0.9667\n",
      "2019-03-06T17:29:43.793920, step: 412, loss: 0.4135192036628723, acc: 0.9062, auc: 0.9824, precision: 0.95, recall: 0.8636\n",
      "2019-03-06T17:29:44.119051, step: 413, loss: 0.44023966789245605, acc: 0.8984, auc: 0.9673, precision: 0.9032, recall: 0.8889\n",
      "2019-03-06T17:29:44.443185, step: 414, loss: 0.46879488229751587, acc: 0.8906, auc: 0.9701, precision: 0.9259, recall: 0.8333\n",
      "2019-03-06T17:29:44.772329, step: 415, loss: 0.4827781915664673, acc: 0.8984, auc: 0.9709, precision: 0.9649, recall: 0.8333\n",
      "2019-03-06T17:29:45.099430, step: 416, loss: 0.44301578402519226, acc: 0.8984, auc: 0.9727, precision: 0.9385, recall: 0.8714\n",
      "2019-03-06T17:29:45.423568, step: 417, loss: 0.48537564277648926, acc: 0.8906, auc: 0.9679, precision: 0.9344, recall: 0.8507\n",
      "2019-03-06T17:29:45.748698, step: 418, loss: 0.48683586716651917, acc: 0.8906, auc: 0.9655, precision: 0.9344, recall: 0.8507\n",
      "2019-03-06T17:29:46.075823, step: 419, loss: 0.588229775428772, acc: 0.8594, auc: 0.9487, precision: 0.9, recall: 0.8182\n",
      "2019-03-06T17:29:46.398959, step: 420, loss: 0.5347572565078735, acc: 0.8672, auc: 0.9591, precision: 0.9464, recall: 0.791\n",
      "2019-03-06T17:29:46.725087, step: 421, loss: 0.39883673191070557, acc: 0.9062, auc: 0.9745, precision: 0.971, recall: 0.8701\n",
      "2019-03-06T17:29:47.051215, step: 422, loss: 0.4942810833454132, acc: 0.8516, auc: 0.9678, precision: 0.863, recall: 0.875\n",
      "2019-03-06T17:29:47.377343, step: 423, loss: 0.4551197588443756, acc: 0.9219, auc: 0.9683, precision: 0.9355, recall: 0.9062\n",
      "2019-03-06T17:29:47.702473, step: 424, loss: 0.7260055541992188, acc: 0.8828, auc: 0.9298, precision: 0.9194, recall: 0.8507\n",
      "2019-03-06T17:29:48.027604, step: 425, loss: 0.5985298752784729, acc: 0.8672, auc: 0.9485, precision: 0.8644, recall: 0.85\n",
      "2019-03-06T17:29:48.349743, step: 426, loss: 0.3687874674797058, acc: 0.9141, auc: 0.9814, precision: 0.9818, recall: 0.8438\n",
      "2019-03-06T17:29:48.673876, step: 427, loss: 0.6077014803886414, acc: 0.8906, auc: 0.9494, precision: 0.8393, recall: 0.9038\n",
      "2019-03-06T17:29:49.000035, step: 428, loss: 0.42137202620506287, acc: 0.9141, auc: 0.9689, precision: 0.9545, recall: 0.8235\n",
      "2019-03-06T17:29:49.330121, step: 429, loss: 0.4620189964771271, acc: 0.8984, auc: 0.968, precision: 0.9298, recall: 0.8548\n",
      "2019-03-06T17:29:49.654254, step: 430, loss: 0.3715338110923767, acc: 0.9297, auc: 0.9802, precision: 0.9818, recall: 0.871\n",
      "2019-03-06T17:29:49.977390, step: 431, loss: 0.4553428888320923, acc: 0.8906, auc: 0.9728, precision: 0.9385, recall: 0.8592\n",
      "2019-03-06T17:29:50.307508, step: 432, loss: 0.6609998345375061, acc: 0.8047, auc: 0.9469, precision: 0.9804, recall: 0.6757\n",
      "2019-03-06T17:29:50.633635, step: 433, loss: 0.494900107383728, acc: 0.8828, auc: 0.9659, precision: 0.9038, recall: 0.8246\n",
      "2019-03-06T17:29:50.958766, step: 434, loss: 0.5480467081069946, acc: 0.9141, auc: 0.9554, precision: 0.9836, recall: 0.8571\n",
      "2019-03-06T17:29:51.281902, step: 435, loss: 0.7937184572219849, acc: 0.8359, auc: 0.9151, precision: 0.871, recall: 0.806\n",
      "2019-03-06T17:29:51.607033, step: 436, loss: 0.5951496362686157, acc: 0.8906, auc: 0.9445, precision: 0.8824, recall: 0.9091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T17:29:51.930197, step: 437, loss: 0.536287784576416, acc: 0.9062, auc: 0.9786, precision: 0.8361, recall: 0.9623\n",
      "2019-03-06T17:29:52.254302, step: 438, loss: 0.5932485461235046, acc: 0.8672, auc: 0.9482, precision: 0.8621, recall: 0.8475\n",
      "2019-03-06T17:29:52.577438, step: 439, loss: 0.568769097328186, acc: 0.9141, auc: 0.9534, precision: 0.9254, recall: 0.9118\n",
      "2019-03-06T17:29:52.902568, step: 440, loss: 0.4197651147842407, acc: 0.8906, auc: 0.9767, precision: 0.9655, recall: 0.8235\n",
      "2019-03-06T17:29:53.228696, step: 441, loss: 0.4227755069732666, acc: 0.8906, auc: 0.9851, precision: 0.9815, recall: 0.803\n",
      "2019-03-06T17:29:53.556819, step: 442, loss: 0.38943707942962646, acc: 0.9062, auc: 0.9779, precision: 0.9545, recall: 0.875\n",
      "2019-03-06T17:29:53.882952, step: 443, loss: 0.482318639755249, acc: 0.8906, auc: 0.9635, precision: 0.9677, recall: 0.8333\n",
      "2019-03-06T17:29:54.207086, step: 444, loss: 0.6358328461647034, acc: 0.875, auc: 0.9355, precision: 0.8906, recall: 0.8636\n",
      "2019-03-06T17:29:54.534211, step: 445, loss: 0.5364558696746826, acc: 0.8672, auc: 0.9605, precision: 0.8387, recall: 0.8814\n",
      "2019-03-06T17:29:54.857347, step: 446, loss: 0.6309983134269714, acc: 0.8828, auc: 0.9399, precision: 0.8923, recall: 0.8788\n",
      "2019-03-06T17:29:55.182477, step: 447, loss: 0.5114028453826904, acc: 0.875, auc: 0.9621, precision: 0.873, recall: 0.873\n",
      "2019-03-06T17:29:55.506873, step: 448, loss: 0.6242634057998657, acc: 0.8906, auc: 0.9418, precision: 0.9273, recall: 0.8361\n",
      "2019-03-06T17:29:55.833998, step: 449, loss: 0.545743465423584, acc: 0.875, auc: 0.954, precision: 0.9231, recall: 0.8451\n",
      "2019-03-06T17:29:56.160127, step: 450, loss: 0.560870885848999, acc: 0.875, auc: 0.958, precision: 0.9464, recall: 0.803\n",
      "2019-03-06T17:29:56.487252, step: 451, loss: 0.405551016330719, acc: 0.9062, auc: 0.9781, precision: 0.8983, recall: 0.8983\n",
      "2019-03-06T17:29:56.811385, step: 452, loss: 0.5207327604293823, acc: 0.8828, auc: 0.9611, precision: 0.918, recall: 0.8485\n",
      "2019-03-06T17:29:57.137542, step: 453, loss: 0.4320569336414337, acc: 0.8828, auc: 0.9684, precision: 0.9483, recall: 0.8209\n",
      "2019-03-06T17:29:57.464639, step: 454, loss: 0.5808589458465576, acc: 0.8672, auc: 0.9436, precision: 0.9492, recall: 0.8\n",
      "2019-03-06T17:29:57.789769, step: 455, loss: 0.670891284942627, acc: 0.8672, auc: 0.9353, precision: 0.8644, recall: 0.85\n",
      "2019-03-06T17:29:58.117891, step: 456, loss: 0.5456944108009338, acc: 0.8672, auc: 0.9539, precision: 0.9057, recall: 0.8\n",
      "2019-03-06T17:29:58.443026, step: 457, loss: 0.6117089986801147, acc: 0.875, auc: 0.9476, precision: 0.918, recall: 0.8358\n",
      "2019-03-06T17:29:58.772147, step: 458, loss: 0.536794126033783, acc: 0.8906, auc: 0.9555, precision: 0.9062, recall: 0.8788\n",
      "2019-03-06T17:29:59.100269, step: 459, loss: 0.45793572068214417, acc: 0.9141, auc: 0.9648, precision: 0.9062, recall: 0.9206\n",
      "2019-03-06T17:29:59.423404, step: 460, loss: 0.39732760190963745, acc: 0.8906, auc: 0.9814, precision: 0.9649, recall: 0.8209\n",
      "2019-03-06T17:29:59.746540, step: 461, loss: 0.4541287422180176, acc: 0.9141, auc: 0.9653, precision: 0.9273, recall: 0.8793\n",
      "2019-03-06T17:30:00.070674, step: 462, loss: 0.5328543782234192, acc: 0.8828, auc: 0.9543, precision: 0.8657, recall: 0.9062\n",
      "2019-03-06T17:30:00.396802, step: 463, loss: 0.5383167266845703, acc: 0.9062, auc: 0.9559, precision: 0.9516, recall: 0.8676\n",
      "2019-03-06T17:30:00.722930, step: 464, loss: 0.4613935947418213, acc: 0.9219, auc: 0.9675, precision: 0.9194, recall: 0.9194\n",
      "2019-03-06T17:30:01.045068, step: 465, loss: 0.683643102645874, acc: 0.8516, auc: 0.9306, precision: 0.9123, recall: 0.7879\n",
      "2019-03-06T17:30:01.368205, step: 466, loss: 0.7325130701065063, acc: 0.8516, auc: 0.9197, precision: 0.8814, recall: 0.8125\n",
      "2019-03-06T17:30:01.690343, step: 467, loss: 0.49904829263687134, acc: 0.8672, auc: 0.9619, precision: 0.9259, recall: 0.7937\n",
      "2019-03-06T17:30:02.018465, step: 468, loss: 0.6190649271011353, acc: 0.8672, auc: 0.9402, precision: 0.9123, recall: 0.8125\n",
      "start training model\n",
      "2019-03-06T17:30:02.365537, step: 469, loss: 0.5331627130508423, acc: 0.8984, auc: 0.9512, precision: 0.9194, recall: 0.8769\n",
      "2019-03-06T17:30:02.689671, step: 470, loss: 0.35428452491760254, acc: 0.9141, auc: 0.9819, precision: 0.9167, recall: 0.9016\n",
      "2019-03-06T17:30:03.017794, step: 471, loss: 0.4758278429508209, acc: 0.8906, auc: 0.9682, precision: 0.9583, recall: 0.7931\n",
      "2019-03-06T17:30:03.342924, step: 472, loss: 0.5487098693847656, acc: 0.8828, auc: 0.9587, precision: 0.9583, recall: 0.7797\n",
      "2019-03-06T17:30:03.666060, step: 473, loss: 0.4670756459236145, acc: 0.9141, auc: 0.9659, precision: 0.9298, recall: 0.8833\n",
      "2019-03-06T17:30:03.989196, step: 474, loss: 0.8315977454185486, acc: 0.8125, auc: 0.9061, precision: 0.8929, recall: 0.7353\n",
      "2019-03-06T17:30:04.310337, step: 475, loss: 0.39246439933776855, acc: 0.9062, auc: 0.9777, precision: 0.9231, recall: 0.8955\n",
      "2019-03-06T17:30:04.635468, step: 476, loss: 0.43161702156066895, acc: 0.9453, auc: 0.9766, precision: 0.9394, recall: 0.9538\n",
      "2019-03-06T17:30:04.964588, step: 477, loss: 0.652279257774353, acc: 0.8906, auc: 0.9526, precision: 0.8871, recall: 0.8871\n",
      "2019-03-06T17:30:05.290716, step: 478, loss: 0.3944307565689087, acc: 0.9141, auc: 0.9787, precision: 0.9231, recall: 0.9091\n",
      "2019-03-06T17:30:05.618838, step: 479, loss: 0.3940950036048889, acc: 0.9297, auc: 0.9732, precision: 0.9153, recall: 0.931\n",
      "2019-03-06T17:30:05.945964, step: 480, loss: 0.5730360150337219, acc: 0.8672, auc: 0.9584, precision: 0.9032, recall: 0.8358\n",
      "2019-03-06T17:30:06.269099, step: 481, loss: 0.46606922149658203, acc: 0.875, auc: 0.967, precision: 0.9091, recall: 0.8197\n",
      "2019-03-06T17:30:06.593233, step: 482, loss: 0.4346619248390198, acc: 0.8906, auc: 0.9747, precision: 0.9565, recall: 0.7857\n",
      "2019-03-06T17:30:06.919360, step: 483, loss: 0.5454119443893433, acc: 0.875, auc: 0.9629, precision: 0.9455, recall: 0.8\n",
      "2019-03-06T17:30:07.247483, step: 484, loss: 0.43566375970840454, acc: 0.8984, auc: 0.9692, precision: 0.9808, recall: 0.8095\n",
      "2019-03-06T17:30:07.571616, step: 485, loss: 0.5102229118347168, acc: 0.8828, auc: 0.9594, precision: 0.9123, recall: 0.8387\n",
      "2019-03-06T17:30:07.897745, step: 486, loss: 0.43329939246177673, acc: 0.9297, auc: 0.9715, precision: 0.9796, recall: 0.8571\n",
      "2019-03-06T17:30:08.224870, step: 487, loss: 0.391789972782135, acc: 0.9062, auc: 0.9792, precision: 0.9074, recall: 0.875\n",
      "2019-03-06T17:30:08.548006, step: 488, loss: 0.5434973239898682, acc: 0.875, auc: 0.9507, precision: 0.9062, recall: 0.8529\n",
      "2019-03-06T17:30:08.873136, step: 489, loss: 0.41804444789886475, acc: 0.8984, auc: 0.9724, precision: 0.9649, recall: 0.8333\n",
      "2019-03-06T17:30:09.197270, step: 490, loss: 0.5761556625366211, acc: 0.8828, auc: 0.9407, precision: 0.8545, recall: 0.8704\n",
      "2019-03-06T17:30:09.520405, step: 491, loss: 0.5497267842292786, acc: 0.8828, auc: 0.9522, precision: 0.8841, recall: 0.8971\n",
      "2019-03-06T17:30:09.843542, step: 492, loss: 0.3935348391532898, acc: 0.8984, auc: 0.9746, precision: 0.9167, recall: 0.9041\n",
      "2019-03-06T17:30:10.167675, step: 493, loss: 0.5712388157844543, acc: 0.875, auc: 0.9503, precision: 0.9245, recall: 0.8033\n",
      "2019-03-06T17:30:10.494800, step: 494, loss: 0.30686861276626587, acc: 0.9219, auc: 0.9912, precision: 0.9483, recall: 0.8871\n",
      "2019-03-06T17:30:10.817936, step: 495, loss: 0.37627166509628296, acc: 0.9297, auc: 0.9814, precision: 0.9344, recall: 0.9194\n",
      "2019-03-06T17:30:11.142069, step: 496, loss: 0.5216839909553528, acc: 0.9062, auc: 0.9609, precision: 0.913, recall: 0.913\n",
      "2019-03-06T17:30:11.469194, step: 497, loss: 0.47124525904655457, acc: 0.9141, auc: 0.965, precision: 0.9091, recall: 0.8929\n",
      "2019-03-06T17:30:11.794325, step: 498, loss: 0.3762349486351013, acc: 0.9297, auc: 0.977, precision: 0.9474, recall: 0.9\n",
      "2019-03-06T17:30:12.119455, step: 499, loss: 0.377540647983551, acc: 0.9375, auc: 0.978, precision: 0.9344, recall: 0.9344\n",
      "2019-03-06T17:30:12.447578, step: 500, loss: 0.500046968460083, acc: 0.9062, auc: 0.9674, precision: 0.9726, recall: 0.8765\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T17:30:25.037912, step: 500, loss: 0.6270580215331836, acc: 0.8669794871794871, auc: 0.9496410256410255, precision: 0.9216410256410257, recall: 0.8054076923076923\n",
      "2019-03-06T17:30:25.361048, step: 501, loss: 0.6500188112258911, acc: 0.8516, auc: 0.9505, precision: 0.9298, recall: 0.7794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T17:30:25.683187, step: 502, loss: 0.45971211791038513, acc: 0.9219, auc: 0.9683, precision: 0.9429, recall: 0.9167\n",
      "2019-03-06T17:30:26.006322, step: 503, loss: 0.40945470333099365, acc: 0.9219, auc: 0.9782, precision: 0.9833, recall: 0.8676\n",
      "2019-03-06T17:30:26.349471, step: 504, loss: 0.3085762858390808, acc: 0.9375, auc: 0.9902, precision: 0.913, recall: 0.9692\n",
      "2019-03-06T17:30:26.674567, step: 505, loss: 0.5003207921981812, acc: 0.9141, auc: 0.965, precision: 0.9, recall: 0.9403\n",
      "2019-03-06T17:30:26.997703, step: 506, loss: 0.4688001871109009, acc: 0.8828, auc: 0.9668, precision: 0.8571, recall: 0.9231\n",
      "2019-03-06T17:30:27.319841, step: 507, loss: 0.40600132942199707, acc: 0.9062, auc: 0.977, precision: 0.8906, recall: 0.9194\n",
      "2019-03-06T17:30:27.644972, step: 508, loss: 0.4201846420764923, acc: 0.9141, auc: 0.9718, precision: 0.9545, recall: 0.8873\n",
      "2019-03-06T17:30:27.972098, step: 509, loss: 0.45093679428100586, acc: 0.9297, auc: 0.9757, precision: 0.8704, recall: 0.9592\n",
      "2019-03-06T17:30:28.299222, step: 510, loss: 0.4266672134399414, acc: 0.8906, auc: 0.9736, precision: 0.9362, recall: 0.8\n",
      "2019-03-06T17:30:28.623356, step: 511, loss: 0.49948418140411377, acc: 0.875, auc: 0.9621, precision: 0.9167, recall: 0.8333\n",
      "2019-03-06T17:30:28.948487, step: 512, loss: 0.3944595754146576, acc: 0.8984, auc: 0.9786, precision: 0.9375, recall: 0.8696\n",
      "2019-03-06T17:30:29.274615, step: 513, loss: 0.42189648747444153, acc: 0.8984, auc: 0.9746, precision: 0.9444, recall: 0.8361\n",
      "2019-03-06T17:30:29.600742, step: 514, loss: 0.4095524549484253, acc: 0.8906, auc: 0.981, precision: 0.9516, recall: 0.8429\n",
      "2019-03-06T17:30:29.925873, step: 515, loss: 0.4897420108318329, acc: 0.8672, auc: 0.9639, precision: 0.9219, recall: 0.831\n",
      "2019-03-06T17:30:30.249009, step: 516, loss: 0.43179017305374146, acc: 0.9297, auc: 0.9723, precision: 0.9636, recall: 0.8833\n",
      "2019-03-06T17:30:30.573142, step: 517, loss: 0.5652683973312378, acc: 0.8984, auc: 0.9564, precision: 0.9167, recall: 0.9041\n",
      "2019-03-06T17:30:30.898273, step: 518, loss: 0.6243035793304443, acc: 0.8984, auc: 0.9448, precision: 0.9048, recall: 0.8906\n",
      "2019-03-06T17:30:31.222406, step: 519, loss: 0.2896772623062134, acc: 0.9453, auc: 0.9875, precision: 0.9718, recall: 0.9324\n",
      "2019-03-06T17:30:31.545542, step: 520, loss: 0.5398893356323242, acc: 0.9062, auc: 0.9607, precision: 0.8841, recall: 0.9385\n",
      "2019-03-06T17:30:31.870673, step: 521, loss: 0.30238229036331177, acc: 0.9531, auc: 0.9953, precision: 0.9344, recall: 0.9661\n",
      "2019-03-06T17:30:32.196800, step: 522, loss: 0.6237431764602661, acc: 0.8516, auc: 0.9401, precision: 0.9216, recall: 0.7581\n",
      "2019-03-06T17:30:32.526918, step: 523, loss: 0.43583035469055176, acc: 0.9062, auc: 0.9763, precision: 0.9048, recall: 0.9048\n",
      "2019-03-06T17:30:32.852048, step: 524, loss: 0.4022534489631653, acc: 0.8906, auc: 0.9729, precision: 0.918, recall: 0.8615\n",
      "2019-03-06T17:30:33.178176, step: 525, loss: 0.48443344235420227, acc: 0.875, auc: 0.9588, precision: 0.9, recall: 0.8036\n",
      "2019-03-06T17:30:33.506299, step: 526, loss: 0.43493539094924927, acc: 0.9062, auc: 0.9751, precision: 0.963, recall: 0.8387\n",
      "2019-03-06T17:30:33.830433, step: 527, loss: 0.44004929065704346, acc: 0.9141, auc: 0.9761, precision: 0.9821, recall: 0.8462\n",
      "2019-03-06T17:30:34.152600, step: 528, loss: 0.3940694332122803, acc: 0.8984, auc: 0.9757, precision: 0.9667, recall: 0.8406\n",
      "2019-03-06T17:30:34.478699, step: 529, loss: 0.43240886926651, acc: 0.9062, auc: 0.9724, precision: 0.9206, recall: 0.8923\n",
      "2019-03-06T17:30:34.803829, step: 530, loss: 0.32186222076416016, acc: 0.9297, auc: 0.9824, precision: 0.9333, recall: 0.918\n",
      "2019-03-06T17:30:35.129958, step: 531, loss: 0.6414715647697449, acc: 0.8984, auc: 0.9407, precision: 0.8909, recall: 0.875\n",
      "2019-03-06T17:30:35.456561, step: 532, loss: 0.5138280987739563, acc: 0.9062, auc: 0.9579, precision: 0.9394, recall: 0.8857\n",
      "2019-03-06T17:30:35.777702, step: 533, loss: 0.4662589728832245, acc: 0.8984, auc: 0.968, precision: 0.8788, recall: 0.9206\n",
      "2019-03-06T17:30:36.103835, step: 534, loss: 0.5573982000350952, acc: 0.8984, auc: 0.9492, precision: 0.9206, recall: 0.8788\n",
      "2019-03-06T17:30:36.425973, step: 535, loss: 0.3621651232242584, acc: 0.9141, auc: 0.9852, precision: 0.8983, recall: 0.9138\n",
      "2019-03-06T17:30:36.753099, step: 536, loss: 0.38683056831359863, acc: 0.8984, auc: 0.9803, precision: 0.9107, recall: 0.8644\n",
      "2019-03-06T17:30:37.078230, step: 537, loss: 0.3801102042198181, acc: 0.9297, auc: 0.9785, precision: 0.9375, recall: 0.9231\n",
      "2019-03-06T17:30:37.402363, step: 538, loss: 0.39268729090690613, acc: 0.9453, auc: 0.9765, precision: 0.9848, recall: 0.9155\n",
      "2019-03-06T17:30:37.728491, step: 539, loss: 0.5879262089729309, acc: 0.8828, auc: 0.9458, precision: 0.9, recall: 0.8571\n",
      "2019-03-06T17:30:38.053621, step: 540, loss: 0.36814790964126587, acc: 0.9297, auc: 0.9761, precision: 0.9355, recall: 0.9206\n",
      "2019-03-06T17:30:38.380747, step: 541, loss: 0.40163448452949524, acc: 0.9062, auc: 0.9795, precision: 0.9821, recall: 0.8333\n",
      "2019-03-06T17:30:38.704880, step: 542, loss: 0.44606029987335205, acc: 0.8906, auc: 0.9674, precision: 0.96, recall: 0.8\n",
      "2019-03-06T17:30:39.032006, step: 543, loss: 0.6451793909072876, acc: 0.8828, auc: 0.9433, precision: 0.8871, recall: 0.873\n",
      "2019-03-06T17:30:39.354144, step: 544, loss: 0.37351617217063904, acc: 0.8984, auc: 0.9794, precision: 0.8983, recall: 0.8833\n",
      "2019-03-06T17:30:39.678277, step: 545, loss: 0.4247360825538635, acc: 0.9297, auc: 0.9707, precision: 0.9365, recall: 0.9219\n",
      "2019-03-06T17:30:40.003408, step: 546, loss: 0.49882370233535767, acc: 0.9062, auc: 0.9667, precision: 0.9615, recall: 0.8333\n",
      "2019-03-06T17:30:40.328538, step: 547, loss: 0.49608689546585083, acc: 0.8828, auc: 0.9608, precision: 0.9412, recall: 0.8\n",
      "2019-03-06T17:30:40.652671, step: 548, loss: 0.5777327418327332, acc: 0.875, auc: 0.9429, precision: 0.8824, recall: 0.8824\n",
      "2019-03-06T17:30:40.977802, step: 549, loss: 0.4324151277542114, acc: 0.8906, auc: 0.9725, precision: 0.9423, recall: 0.8167\n",
      "2019-03-06T17:30:41.302933, step: 550, loss: 0.3273172378540039, acc: 0.9297, auc: 0.9863, precision: 0.9643, recall: 0.8852\n",
      "2019-03-06T17:30:41.627066, step: 551, loss: 0.4034670889377594, acc: 0.9062, auc: 0.9751, precision: 0.931, recall: 0.871\n",
      "2019-03-06T17:30:41.954191, step: 552, loss: 0.5033714771270752, acc: 0.8828, auc: 0.9626, precision: 0.9107, recall: 0.8361\n",
      "2019-03-06T17:30:42.276329, step: 553, loss: 0.494748055934906, acc: 0.8984, auc: 0.9654, precision: 0.9661, recall: 0.8382\n",
      "2019-03-06T17:30:42.601460, step: 554, loss: 0.5098758339881897, acc: 0.9141, auc: 0.9612, precision: 0.9333, recall: 0.8889\n",
      "2019-03-06T17:30:42.926591, step: 555, loss: 0.6219290494918823, acc: 0.8906, auc: 0.943, precision: 0.9245, recall: 0.8305\n",
      "2019-03-06T17:30:43.251722, step: 556, loss: 0.3236684203147888, acc: 0.9141, auc: 0.9846, precision: 0.9322, recall: 0.8871\n",
      "2019-03-06T17:30:43.577850, step: 557, loss: 0.37671440839767456, acc: 0.9375, auc: 0.982, precision: 0.9259, recall: 0.9259\n",
      "2019-03-06T17:30:43.902980, step: 558, loss: 0.36955398321151733, acc: 0.9062, auc: 0.9806, precision: 0.9667, recall: 0.8529\n",
      "2019-03-06T17:30:44.227113, step: 559, loss: 0.6934335827827454, acc: 0.8516, auc: 0.9308, precision: 0.8871, recall: 0.8209\n",
      "2019-03-06T17:30:44.552244, step: 560, loss: 0.27973848581314087, acc: 0.9531, auc: 0.9938, precision: 0.9815, recall: 0.9138\n",
      "2019-03-06T17:30:44.877375, step: 561, loss: 0.35177022218704224, acc: 0.9062, auc: 0.9802, precision: 0.9077, recall: 0.9077\n",
      "2019-03-06T17:30:45.203503, step: 562, loss: 0.4074765741825104, acc: 0.9219, auc: 0.9762, precision: 0.9844, recall: 0.875\n",
      "2019-03-06T17:30:45.527636, step: 563, loss: 0.4423882067203522, acc: 0.8828, auc: 0.9658, precision: 0.9167, recall: 0.8462\n",
      "2019-03-06T17:30:45.851773, step: 564, loss: 0.352603554725647, acc: 0.9453, auc: 0.9846, precision: 0.9655, recall: 0.918\n",
      "2019-03-06T17:30:46.177901, step: 565, loss: 0.5113273859024048, acc: 0.8906, auc: 0.9551, precision: 0.92, recall: 0.8214\n",
      "2019-03-06T17:30:46.503032, step: 566, loss: 0.39485394954681396, acc: 0.9062, auc: 0.9754, precision: 0.9583, recall: 0.8214\n",
      "2019-03-06T17:30:46.826168, step: 567, loss: 0.38580629229545593, acc: 0.8984, auc: 0.9746, precision: 0.9194, recall: 0.8769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T17:30:47.150302, step: 568, loss: 0.43865394592285156, acc: 0.9141, auc: 0.9668, precision: 0.9492, recall: 0.875\n",
      "2019-03-06T17:30:47.475432, step: 569, loss: 0.4338987469673157, acc: 0.8906, auc: 0.9776, precision: 0.9667, recall: 0.8286\n",
      "2019-03-06T17:30:47.799565, step: 570, loss: 0.4792579412460327, acc: 0.9062, auc: 0.9696, precision: 0.9, recall: 0.9\n",
      "2019-03-06T17:30:48.125693, step: 571, loss: 0.508510172367096, acc: 0.8984, auc: 0.9567, precision: 0.9167, recall: 0.8302\n",
      "2019-03-06T17:30:48.447832, step: 572, loss: 0.46247828006744385, acc: 0.9375, auc: 0.9673, precision: 0.9322, recall: 0.9322\n",
      "2019-03-06T17:30:48.774957, step: 573, loss: 0.2518726587295532, acc: 0.9375, auc: 0.9929, precision: 1.0, recall: 0.8644\n",
      "2019-03-06T17:30:49.096131, step: 574, loss: 0.4141191840171814, acc: 0.9453, auc: 0.9746, precision: 0.9636, recall: 0.9138\n",
      "2019-03-06T17:30:49.428211, step: 575, loss: 0.3009086847305298, acc: 0.9297, auc: 0.9839, precision: 0.9483, recall: 0.9016\n",
      "2019-03-06T17:30:49.759325, step: 576, loss: 0.33469951152801514, acc: 0.9141, auc: 0.9808, precision: 0.9467, recall: 0.9103\n",
      "2019-03-06T17:30:50.113378, step: 577, loss: 0.3749810457229614, acc: 0.9219, auc: 0.9783, precision: 0.9242, recall: 0.9242\n",
      "2019-03-06T17:30:50.440503, step: 578, loss: 0.6130624413490295, acc: 0.875, auc: 0.9475, precision: 0.9032, recall: 0.8485\n",
      "2019-03-06T17:30:50.764637, step: 579, loss: 0.4488633871078491, acc: 0.9219, auc: 0.9731, precision: 0.9492, recall: 0.8889\n",
      "2019-03-06T17:30:51.087772, step: 580, loss: 0.4485622048377991, acc: 0.9141, auc: 0.974, precision: 0.9385, recall: 0.8971\n",
      "2019-03-06T17:30:51.414898, step: 581, loss: 0.3578379154205322, acc: 0.9141, auc: 0.9794, precision: 0.9615, recall: 0.8475\n",
      "2019-03-06T17:30:51.741026, step: 582, loss: 1.008455514907837, acc: 0.8125, auc: 0.8985, precision: 0.7937, recall: 0.8197\n",
      "2019-03-06T17:30:52.065159, step: 583, loss: 0.4981352686882019, acc: 0.8984, auc: 0.9619, precision: 0.9322, recall: 0.8594\n",
      "2019-03-06T17:30:52.389293, step: 584, loss: 0.35322797298431396, acc: 0.8984, auc: 0.9816, precision: 0.9667, recall: 0.8406\n",
      "2019-03-06T17:30:52.712428, step: 585, loss: 0.38816869258880615, acc: 0.9141, auc: 0.9748, precision: 0.9429, recall: 0.9041\n",
      "2019-03-06T17:30:53.037559, step: 586, loss: 0.5860041379928589, acc: 0.8594, auc: 0.9444, precision: 0.8732, recall: 0.8732\n",
      "2019-03-06T17:30:53.363687, step: 587, loss: 0.47756779193878174, acc: 0.8828, auc: 0.9651, precision: 0.9265, recall: 0.863\n",
      "2019-03-06T17:30:53.688817, step: 588, loss: 0.5151405334472656, acc: 0.8984, auc: 0.96, precision: 0.881, recall: 0.961\n",
      "2019-03-06T17:30:54.011953, step: 589, loss: 0.44045740365982056, acc: 0.9141, auc: 0.9705, precision: 0.9483, recall: 0.873\n",
      "2019-03-06T17:30:54.335089, step: 590, loss: 0.4413278102874756, acc: 0.9141, auc: 0.9746, precision: 0.9062, recall: 0.9206\n",
      "2019-03-06T17:30:54.665206, step: 591, loss: 0.4037781059741974, acc: 0.8984, auc: 0.9733, precision: 0.9677, recall: 0.8451\n",
      "2019-03-06T17:30:54.992332, step: 592, loss: 0.5506279468536377, acc: 0.8516, auc: 0.9509, precision: 0.8689, recall: 0.8281\n",
      "2019-03-06T17:30:55.316465, step: 593, loss: 0.5473653078079224, acc: 0.875, auc: 0.9578, precision: 0.8929, recall: 0.8333\n",
      "2019-03-06T17:30:55.641596, step: 594, loss: 0.44432327151298523, acc: 0.8984, auc: 0.9714, precision: 0.9623, recall: 0.8226\n",
      "2019-03-06T17:30:55.965729, step: 595, loss: 0.5273526906967163, acc: 0.8828, auc: 0.9617, precision: 0.9375, recall: 0.7895\n",
      "2019-03-06T17:30:56.290859, step: 596, loss: 0.4922519326210022, acc: 0.8984, auc: 0.9658, precision: 0.9636, recall: 0.8281\n",
      "2019-03-06T17:30:56.615271, step: 597, loss: 0.43101537227630615, acc: 0.875, auc: 0.9672, precision: 0.918, recall: 0.8358\n",
      "2019-03-06T17:30:56.940401, step: 598, loss: 0.4532371759414673, acc: 0.8594, auc: 0.9742, precision: 0.9655, recall: 0.7778\n",
      "2019-03-06T17:30:57.267526, step: 599, loss: 0.6270461678504944, acc: 0.8672, auc: 0.9457, precision: 0.9298, recall: 0.803\n",
      "2019-03-06T17:30:57.593655, step: 600, loss: 0.5073994398117065, acc: 0.9062, auc: 0.9744, precision: 0.871, recall: 0.931\n",
      "\n",
      "Evaluation:\n",
      "2019-03-06T17:31:10.320627, step: 600, loss: 0.5995322649295514, acc: 0.8794102564102565, auc: 0.9521410256410254, precision: 0.8720384615384614, recall: 0.8926358974358968\n",
      "2019-03-06T17:31:10.668696, step: 601, loss: 0.5435917377471924, acc: 0.8828, auc: 0.958, precision: 0.8667, recall: 0.8814\n",
      "2019-03-06T17:31:11.007789, step: 602, loss: 0.5233886241912842, acc: 0.9219, auc: 0.9679, precision: 0.8923, recall: 0.9508\n",
      "2019-03-06T17:31:11.340899, step: 603, loss: 0.3045922517776489, acc: 0.9453, auc: 0.9885, precision: 0.9672, recall: 0.9219\n",
      "2019-03-06T17:31:11.677000, step: 604, loss: 0.4273429811000824, acc: 0.9062, auc: 0.9758, precision: 0.9153, recall: 0.8852\n",
      "2019-03-06T17:31:12.014098, step: 605, loss: 0.7134777307510376, acc: 0.8516, auc: 0.9324, precision: 0.9074, recall: 0.7778\n",
      "2019-03-06T17:31:12.343218, step: 606, loss: 0.5453156232833862, acc: 0.8828, auc: 0.9504, precision: 0.8868, recall: 0.8393\n",
      "2019-03-06T17:31:12.672338, step: 607, loss: 0.4555880129337311, acc: 0.875, auc: 0.9761, precision: 0.9655, recall: 0.8\n",
      "2019-03-06T17:31:13.007442, step: 608, loss: 0.3363972306251526, acc: 0.8906, auc: 0.9819, precision: 0.9636, recall: 0.8154\n",
      "2019-03-06T17:31:13.352520, step: 609, loss: 0.5115252733230591, acc: 0.875, auc: 0.9651, precision: 0.9259, recall: 0.8065\n",
      "2019-03-06T17:31:13.688621, step: 610, loss: 0.4061516523361206, acc: 0.9297, auc: 0.9775, precision: 0.9189, recall: 0.9577\n",
      "2019-03-06T17:31:14.014753, step: 611, loss: 0.4669361710548401, acc: 0.8906, auc: 0.9648, precision: 0.9286, recall: 0.8387\n",
      "2019-03-06T17:31:14.341879, step: 612, loss: 0.42367270588874817, acc: 0.9062, auc: 0.9747, precision: 0.9273, recall: 0.8644\n",
      "2019-03-06T17:31:14.674988, step: 613, loss: 0.42129823565483093, acc: 0.8906, auc: 0.9745, precision: 0.8986, recall: 0.8986\n",
      "2019-03-06T17:31:14.998124, step: 614, loss: 0.3824807405471802, acc: 0.9375, auc: 0.9761, precision: 0.9524, recall: 0.9231\n",
      "2019-03-06T17:31:15.325249, step: 615, loss: 0.41500377655029297, acc: 0.9375, auc: 0.9738, precision: 0.9492, recall: 0.918\n",
      "2019-03-06T17:31:15.649382, step: 616, loss: 0.41817837953567505, acc: 0.9297, auc: 0.9719, precision: 0.9643, recall: 0.8852\n",
      "2019-03-06T17:31:15.974516, step: 617, loss: 0.3885498642921448, acc: 0.9062, auc: 0.9741, precision: 0.9552, recall: 0.8767\n",
      "2019-03-06T17:31:16.300645, step: 618, loss: 0.6932615637779236, acc: 0.8438, auc: 0.9357, precision: 0.8971, recall: 0.8243\n",
      "2019-03-06T17:31:16.624777, step: 619, loss: 0.3831702172756195, acc: 0.9141, auc: 0.9779, precision: 0.9552, recall: 0.8889\n",
      "2019-03-06T17:31:16.950905, step: 620, loss: 0.3962552547454834, acc: 0.9375, auc: 0.9786, precision: 0.918, recall: 0.9492\n",
      "2019-03-06T17:31:17.279028, step: 621, loss: 0.4015253186225891, acc: 0.9375, auc: 0.9731, precision: 0.9273, recall: 0.9273\n",
      "2019-03-06T17:31:17.606154, step: 622, loss: 0.4975261986255646, acc: 0.875, auc: 0.964, precision: 0.8611, recall: 0.9118\n",
      "2019-03-06T17:31:17.935273, step: 623, loss: 0.3219463527202606, acc: 0.9141, auc: 0.9868, precision: 0.9821, recall: 0.8462\n",
      "2019-03-06T17:31:18.265390, step: 624, loss: 0.38977116346359253, acc: 0.8984, auc: 0.976, precision: 0.9355, recall: 0.8657\n",
      "start training model\n",
      "2019-03-06T17:31:18.624430, step: 625, loss: 0.32002562284469604, acc: 0.9141, auc: 0.9855, precision: 0.9388, recall: 0.8519\n",
      "2019-03-06T17:31:18.965519, step: 626, loss: 0.33531343936920166, acc: 0.9297, auc: 0.9867, precision: 0.9851, recall: 0.8919\n",
      "2019-03-06T17:31:19.309599, step: 627, loss: 0.4761819541454315, acc: 0.9062, auc: 0.97, precision: 0.9123, recall: 0.8814\n",
      "2019-03-06T17:31:19.640713, step: 628, loss: 0.31717419624328613, acc: 0.9375, auc: 0.9892, precision: 0.9839, recall: 0.8971\n",
      "2019-03-06T17:31:19.972825, step: 629, loss: 0.32317209243774414, acc: 0.9297, auc: 0.9833, precision: 0.9296, recall: 0.9429\n",
      "2019-03-06T17:31:20.296958, step: 630, loss: 0.3622990846633911, acc: 0.9219, auc: 0.9816, precision: 0.9265, recall: 0.9265\n",
      "2019-03-06T17:31:20.625110, step: 631, loss: 0.3476544916629791, acc: 0.9531, auc: 0.9829, precision: 0.9385, recall: 0.9683\n",
      "2019-03-06T17:31:20.963177, step: 632, loss: 0.5082955956459045, acc: 0.9375, auc: 0.9621, precision: 0.9306, recall: 0.9571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-03-06T17:31:21.304264, step: 633, loss: 0.40825390815734863, acc: 0.9453, auc: 0.9726, precision: 0.9821, recall: 0.9016\n",
      "2019-03-06T17:31:21.634382, step: 634, loss: 0.46332499384880066, acc: 0.9141, auc: 0.9646, precision: 0.9104, recall: 0.9242\n",
      "2019-03-06T17:31:21.982451, step: 635, loss: 0.40926167368888855, acc: 0.8984, auc: 0.9763, precision: 0.9167, recall: 0.873\n",
      "2019-03-06T17:31:22.341491, step: 636, loss: 0.31475770473480225, acc: 0.9453, auc: 0.988, precision: 0.9831, recall: 0.9062\n",
      "2019-03-06T17:31:22.685572, step: 637, loss: 0.24524381756782532, acc: 0.9609, auc: 0.9953, precision: 0.9492, recall: 0.9655\n",
      "2019-03-06T17:31:23.027656, step: 638, loss: 0.34653908014297485, acc: 0.9375, auc: 0.9819, precision: 0.9825, recall: 0.8889\n",
      "2019-03-06T17:31:23.368744, step: 639, loss: 0.31638655066490173, acc: 0.9062, auc: 0.9888, precision: 0.9692, recall: 0.863\n",
      "2019-03-06T17:31:23.723796, step: 640, loss: 0.5795339345932007, acc: 0.8906, auc: 0.9573, precision: 0.9636, recall: 0.8154\n",
      "2019-03-06T17:31:24.057902, step: 641, loss: 0.2543436884880066, acc: 0.9453, auc: 0.9919, precision: 0.9649, recall: 0.9167\n",
      "2019-03-06T17:31:24.382035, step: 642, loss: 0.4482967257499695, acc: 0.9219, auc: 0.9821, precision: 0.8814, recall: 0.9455\n",
      "2019-03-06T17:31:24.712153, step: 643, loss: 0.37890034914016724, acc: 0.9297, auc: 0.9768, precision: 0.942, recall: 0.9286\n",
      "2019-03-06T17:31:25.039277, step: 644, loss: 0.49062812328338623, acc: 0.8984, auc: 0.9634, precision: 0.9322, recall: 0.8594\n",
      "2019-03-06T17:31:25.371390, step: 645, loss: 0.23159000277519226, acc: 0.9453, auc: 0.9949, precision: 0.9677, recall: 0.9231\n",
      "2019-03-06T17:31:25.699513, step: 646, loss: 0.5580192804336548, acc: 0.8594, auc: 0.9512, precision: 0.8621, recall: 0.8333\n",
      "2019-03-06T17:31:26.027635, step: 647, loss: 0.5021933317184448, acc: 0.8906, auc: 0.9608, precision: 0.9074, recall: 0.8448\n",
      "2019-03-06T17:31:26.356754, step: 648, loss: 0.3085891604423523, acc: 0.9141, auc: 0.9873, precision: 0.9559, recall: 0.8904\n",
      "2019-03-06T17:31:26.698869, step: 649, loss: 0.3833278715610504, acc: 0.9062, auc: 0.979, precision: 0.9153, recall: 0.8852\n",
      "2019-03-06T17:31:27.029955, step: 650, loss: 0.3938024640083313, acc: 0.9297, auc: 0.9771, precision: 0.9552, recall: 0.9143\n",
      "2019-03-06T17:31:27.359108, step: 651, loss: 0.47901105880737305, acc: 0.8828, auc: 0.9629, precision: 0.918, recall: 0.8485\n",
      "2019-03-06T17:31:27.688194, step: 652, loss: 0.2551937699317932, acc: 0.9453, auc: 0.9912, precision: 0.9508, recall: 0.9355\n",
      "2019-03-06T17:31:28.015320, step: 653, loss: 0.38277679681777954, acc: 0.9609, auc: 0.967, precision: 0.9836, recall: 0.9375\n",
      "2019-03-06T17:31:28.339453, step: 654, loss: 0.27820074558258057, acc: 0.9609, auc: 0.9875, precision: 0.9412, recall: 0.9846\n",
      "2019-03-06T17:31:28.662589, step: 655, loss: 0.33122462034225464, acc: 0.9453, auc: 0.9814, precision: 0.9692, recall: 0.9265\n",
      "2019-03-06T17:31:28.985725, step: 656, loss: 0.19800996780395508, acc: 0.9688, auc: 0.998, precision: 0.9828, recall: 0.95\n",
      "2019-03-06T17:31:29.311853, step: 657, loss: 0.26293572783470154, acc: 0.9531, auc: 0.9927, precision: 0.9552, recall: 0.9552\n",
      "2019-03-06T17:31:29.634024, step: 658, loss: 0.4104991853237152, acc: 0.875, auc: 0.9721, precision: 0.902, recall: 0.807\n",
      "2019-03-06T17:31:29.959126, step: 659, loss: 0.3082873821258545, acc: 0.9453, auc: 0.9819, precision: 0.9831, recall: 0.9062\n",
      "2019-03-06T17:31:30.285283, step: 660, loss: 0.3501749634742737, acc: 0.9141, auc: 0.9897, precision: 1.0, recall: 0.8406\n",
      "2019-03-06T17:31:30.612379, step: 661, loss: 0.3426758050918579, acc: 0.9453, auc: 0.9808, precision: 0.98, recall: 0.8909\n",
      "2019-03-06T17:31:30.941499, step: 662, loss: 0.3475199043750763, acc: 0.9297, auc: 0.9797, precision: 0.9844, recall: 0.8873\n",
      "2019-03-06T17:31:31.273611, step: 663, loss: 0.4113885760307312, acc: 0.8984, auc: 0.9734, precision: 0.9048, recall: 0.8906\n",
      "2019-03-06T17:31:31.602731, step: 664, loss: 0.4387435019016266, acc: 0.9141, auc: 0.9777, precision: 0.8906, recall: 0.9344\n",
      "2019-03-06T17:31:31.927861, step: 665, loss: 0.3620818257331848, acc: 0.9141, auc: 0.9806, precision: 0.9857, recall: 0.8734\n",
      "2019-03-06T17:31:32.259973, step: 666, loss: 0.2942354381084442, acc: 0.9609, auc: 0.9888, precision: 0.9672, recall: 0.9516\n",
      "2019-03-06T17:31:32.591088, step: 667, loss: 0.38281184434890747, acc: 0.9297, auc: 0.9826, precision: 0.9322, recall: 0.9167\n",
      "2019-03-06T17:31:32.936166, step: 668, loss: 0.38987377285957336, acc: 0.9219, auc: 0.9773, precision: 0.9194, recall: 0.9194\n",
      "2019-03-06T17:31:33.266283, step: 669, loss: 0.26447898149490356, acc: 0.9219, auc: 0.9919, precision: 0.9688, recall: 0.8857\n",
      "2019-03-06T17:31:33.592411, step: 670, loss: 0.4428046941757202, acc: 0.9297, auc: 0.968, precision: 0.9649, recall: 0.8871\n",
      "2019-03-06T17:31:33.920533, step: 671, loss: 0.359357088804245, acc: 0.9219, auc: 0.9787, precision: 0.9583, recall: 0.8519\n",
      "2019-03-06T17:31:34.256634, step: 672, loss: 0.39273035526275635, acc: 0.9062, auc: 0.979, precision: 0.9483, recall: 0.8594\n",
      "2019-03-06T17:31:34.583759, step: 673, loss: 0.4023744463920593, acc: 0.8984, auc: 0.9785, precision: 0.94, recall: 0.8246\n",
      "2019-03-06T17:31:34.911882, step: 674, loss: 0.4181559681892395, acc: 0.8984, auc: 0.9727, precision: 0.9048, recall: 0.8906\n",
      "2019-03-06T17:31:35.247983, step: 675, loss: 0.29220476746559143, acc: 0.9375, auc: 0.9823, precision: 0.9474, recall: 0.9153\n",
      "2019-03-06T17:31:35.574111, step: 676, loss: 0.468370258808136, acc: 0.9062, auc: 0.9691, precision: 0.9388, recall: 0.8364\n",
      "2019-03-06T17:31:35.904229, step: 677, loss: 0.24364203214645386, acc: 0.9531, auc: 0.9958, precision: 0.9545, recall: 0.913\n",
      "2019-03-06T17:31:36.230357, step: 678, loss: 0.38124099373817444, acc: 0.9453, auc: 0.9806, precision: 0.9286, recall: 0.9455\n",
      "2019-03-06T17:31:36.559476, step: 679, loss: 0.2780885100364685, acc: 0.9609, auc: 0.9924, precision: 0.9545, recall: 0.9692\n",
      "2019-03-06T17:31:36.892586, step: 680, loss: 0.3044205605983734, acc: 0.9531, auc: 0.9829, precision: 0.9516, recall: 0.9516\n",
      "2019-03-06T17:31:37.228688, step: 681, loss: 0.28844624757766724, acc: 0.9375, auc: 0.9917, precision: 0.9815, recall: 0.8833\n",
      "2019-03-06T17:31:37.552820, step: 682, loss: 0.45702213048934937, acc: 0.9141, auc: 0.9683, precision: 0.9492, recall: 0.875\n",
      "2019-03-06T17:31:37.882938, step: 683, loss: 0.34692275524139404, acc: 0.9453, auc: 0.9718, precision: 0.9559, recall: 0.942\n",
      "2019-03-06T17:31:38.211060, step: 684, loss: 0.3175877332687378, acc: 0.9297, auc: 0.9815, precision: 0.9412, recall: 0.8889\n",
      "2019-03-06T17:31:38.534201, step: 685, loss: 0.23613682389259338, acc: 0.9609, auc: 0.9944, precision: 0.9831, recall: 0.9355\n",
      "2019-03-06T17:31:38.858334, step: 686, loss: 0.3226642608642578, acc: 0.9531, auc: 0.9753, precision: 0.9385, recall: 0.9683\n",
      "2019-03-06T17:31:39.184462, step: 687, loss: 0.4000830054283142, acc: 0.9219, auc: 0.9812, precision: 0.9516, recall: 0.8939\n",
      "2019-03-06T17:31:39.512585, step: 688, loss: 0.34188947081565857, acc: 0.9297, auc: 0.9835, precision: 0.9688, recall: 0.8986\n",
      "2019-03-06T17:31:39.836718, step: 689, loss: 0.25572124123573303, acc: 0.9453, auc: 0.9943, precision: 0.9706, recall: 0.9296\n",
      "2019-03-06T17:31:40.160851, step: 690, loss: 0.3333640694618225, acc: 0.9141, auc: 0.9829, precision: 0.9636, recall: 0.8548\n",
      "2019-03-06T17:31:40.485982, step: 691, loss: 0.7125846743583679, acc: 0.8984, auc: 0.9376, precision: 0.8485, recall: 0.9492\n",
      "2019-03-06T17:31:40.825075, step: 692, loss: 0.42876267433166504, acc: 0.9062, auc: 0.9706, precision: 0.9455, recall: 0.8525\n",
      "2019-03-06T17:31:41.155193, step: 693, loss: 0.45112094283103943, acc: 0.8984, auc: 0.9714, precision: 0.9138, recall: 0.8689\n",
      "2019-03-06T17:31:41.483315, step: 694, loss: 0.342357873916626, acc: 0.9297, auc: 0.9816, precision: 0.9701, recall: 0.9028\n",
      "2019-03-06T17:31:41.808446, step: 695, loss: 0.2913025915622711, acc: 0.9453, auc: 0.9904, precision: 0.9726, recall: 0.9342\n",
      "2019-03-06T17:31:42.153523, step: 696, loss: 0.428297221660614, acc: 0.9219, auc: 0.9734, precision: 0.9104, recall: 0.9385\n",
      "2019-03-06T17:31:42.484638, step: 697, loss: 0.3188716769218445, acc: 0.9609, auc: 0.9837, precision: 0.9643, recall: 0.9474\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "\n",
    "# 生成训练集和验证集\n",
    "trainReviews = data.trainReviews\n",
    "trainLabels = data.trainLabels\n",
    "evalReviews = data.evalReviews\n",
    "evalLabels = data.evalLabels\n",
    "\n",
    "wordEmbedding = data.wordEmbedding\n",
    "indexFreqs = data.indexFreqs\n",
    "\n",
    "# 定义计算图\n",
    "with tf.Graph().as_default():\n",
    "\n",
    "    session_conf = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n",
    "    session_conf.gpu_options.allow_growth=True\n",
    "    session_conf.gpu_options.per_process_gpu_memory_fraction = 0.9  # 配置gpu占用率  \n",
    "\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    # 定义会话\n",
    "    with sess.as_default():\n",
    "        lstm = AdversarialLSTM(config, wordEmbedding, indexFreqs)\n",
    "        \n",
    "        globalStep = tf.Variable(0, name=\"globalStep\", trainable=False)\n",
    "        # 定义优化函数，传入学习速率参数\n",
    "        optimizer = tf.train.AdamOptimizer(config.training.learningRate)\n",
    "        # 计算梯度,得到梯度和变量\n",
    "        gradsAndVars = optimizer.compute_gradients(lstm.loss)\n",
    "        # 将梯度应用到变量下，生成训练器\n",
    "        trainOp = optimizer.apply_gradients(gradsAndVars, global_step=globalStep)\n",
    "        \n",
    "        # 用summary绘制tensorBoard\n",
    "        gradSummaries = []\n",
    "        for g, v in gradsAndVars:\n",
    "            if g is not None:\n",
    "                tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "        \n",
    "        outDir = os.path.abspath(os.path.join(os.path.curdir, \"summarys\"))\n",
    "        print(\"Writing to {}\\n\".format(outDir))\n",
    "        \n",
    "        lossSummary = tf.summary.scalar(\"loss\", lstm.loss)\n",
    "        summaryOp = tf.summary.merge_all()\n",
    "        \n",
    "        trainSummaryDir = os.path.join(outDir, \"train\")\n",
    "        trainSummaryWriter = tf.summary.FileWriter(trainSummaryDir, sess.graph)\n",
    "        \n",
    "        evalSummaryDir = os.path.join(outDir, \"eval\")\n",
    "        evalSummaryWriter = tf.summary.FileWriter(evalSummaryDir, sess.graph)\n",
    "        \n",
    "        \n",
    "        # 初始化所有变量\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "        \n",
    "        # 保存模型的一种方式，保存为pb文件\n",
    "#         builder = tf.saved_model.builder.SavedModelBuilder(\"../model/Bi-LSTM/savedModel\")\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def trainStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            训练函数\n",
    "            \"\"\"   \n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: config.model.dropoutKeepProb\n",
    "            }\n",
    "            _, summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [trainOp, summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            timeStr = datetime.datetime.now().isoformat()\n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(timeStr, step, loss, acc, auc, precision, recall))\n",
    "            trainSummaryWriter.add_summary(summary, step)\n",
    "\n",
    "        def devStep(batchX, batchY):\n",
    "            \"\"\"\n",
    "            验证函数\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              lstm.inputX: batchX,\n",
    "              lstm.inputY: batchY,\n",
    "              lstm.dropoutKeepProb: 1.0\n",
    "            }\n",
    "            summary, step, loss, predictions, binaryPreds = sess.run(\n",
    "                [summaryOp, globalStep, lstm.loss, lstm.predictions, lstm.binaryPreds],\n",
    "                feed_dict)\n",
    "            \n",
    "            acc, auc, precision, recall = genMetrics(batchY, predictions, binaryPreds)\n",
    "            \n",
    "            evalSummaryWriter.add_summary(summary, step)\n",
    "            \n",
    "            return loss, acc, auc, precision, recall\n",
    "        \n",
    "        for i in range(config.training.epoches):\n",
    "            # 训练模型\n",
    "            print(\"start training model\")\n",
    "            for batchTrain in nextBatch(trainReviews, trainLabels, config.batchSize):\n",
    "                trainStep(batchTrain[0], batchTrain[1])\n",
    "\n",
    "                currentStep = tf.train.global_step(sess, globalStep) \n",
    "                if currentStep % config.training.evaluateEvery == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    \n",
    "                    losses = []\n",
    "                    accs = []\n",
    "                    aucs = []\n",
    "                    precisions = []\n",
    "                    recalls = []\n",
    "                    \n",
    "                    for batchEval in nextBatch(evalReviews, evalLabels, config.batchSize):\n",
    "                        loss, acc, auc, precision, recall = devStep(batchEval[0], batchEval[1])\n",
    "                        losses.append(loss)\n",
    "                        accs.append(acc)\n",
    "                        aucs.append(auc)\n",
    "                        precisions.append(precision)\n",
    "                        recalls.append(recall)\n",
    "                        \n",
    "                    time_str = datetime.datetime.now().isoformat()\n",
    "                    print(\"{}, step: {}, loss: {}, acc: {}, auc: {}, precision: {}, recall: {}\".format(time_str, currentStep, mean(losses), \n",
    "                                                                                                       mean(accs), mean(aucs), mean(precisions),\n",
    "                                                                                                       mean(recalls)))\n",
    "                    \n",
    "#                 if currentStep % config.training.checkpointEvery == 0:\n",
    "#                     # 保存模型的另一种方法，保存checkpoint文件\n",
    "#                     path = saver.save(sess, \"../model/Bi-LSTM/model/my-model\", global_step=currentStep)\n",
    "#                     print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "#         inputs = {\"inputX\": tf.saved_model.utils.build_tensor_info(lstm.inputX),\n",
    "#                   \"keepProb\": tf.saved_model.utils.build_tensor_info(lstm.dropoutKeepProb)}\n",
    "\n",
    "#         outputs = {\"binaryPreds\": tf.saved_model.utils.build_tensor_info(lstm.binaryPreds)}\n",
    "\n",
    "#         prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(inputs=inputs, outputs=outputs,\n",
    "#                                                                                       method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)\n",
    "#         legacy_init_op = tf.group(tf.tables_initializer(), name=\"legacy_init_op\")\n",
    "#         builder.add_meta_graph_and_variables(sess, [tf.saved_model.tag_constants.SERVING],\n",
    "#                                             signature_def_map={\"predict\": prediction_signature}, legacy_init_op=legacy_init_op)\n",
    "\n",
    "#          builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
